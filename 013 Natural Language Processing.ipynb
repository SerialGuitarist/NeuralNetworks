{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66836be9",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "A good NLP system typically performs many of these tasks inside\n",
    "* Analysis Tasks\n",
    "    * Syntactic\n",
    "        * Tokenization\n",
    "            * Dividing a body of text into the (indivisible) atomic units, such as words\n",
    "            * Especially important in languages such as Japanese where the words are not divided by spaces or punctuation marks such as apostrophes\n",
    "    * Semantic\n",
    "        * Sentence/Synopsis classification\n",
    "            * Spam detection\n",
    "            * News article classification into political, technology, etc etc\n",
    "            * Product review rating sentiment classification\n",
    "        * Named Entity Recognition (NER)\n",
    "            * Attempt to extracting names of entities (such as persons, locations, organizations, etc etc)\n",
    "    * Pragmatic\n",
    "        * Word Sense Disambiguation (WSD)\n",
    "            * The sentences \"the dog barked at the cows\" and \"the dog bit the tree bark\" has completely different meanings of the word bark\n",
    "        * Part-of-Speech (PoS) tagging\n",
    "            * Figuring out what purpose does the word serve in a given sentence, such as proper noun, common noun, phrasel verb, etc etc\n",
    "* Generation Tasks\n",
    "    * Language generation\n",
    "        * Predicting new text to follow after being classified in a large text body, such as a scifi story\n",
    "    * Question answering (QA)\n",
    "        * Chatbots on social media sites\n",
    "    * Machine translation (MT)\n",
    "        * Different languages have highly different morphological structures, and the the **word alignment problem**, where the word-to-word relationships can be one to many, one to one, many to one, or many to many makes this a very difficult task\n",
    "        \n",
    "        \n",
    "Deep learning allows for spatial and temporal filtering from the data, as all deep learning really does it to emphasize or de-emphasize specific bits of the complex data representation.\n",
    "\n",
    "Language isn't just sequential, but also recursive, which remains as one of the active fields of research to this day:\n",
    "* John says that Mary says that Bill told Richard he was on his way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66956569",
   "metadata": {},
   "source": [
    "# Deep Learning Approach To Natural Processing\n",
    "* Learns much richer features from raw data instead of using limited human-engineered features\n",
    "    * Made the tedious and expensive task of feature engineering to be obsolete\n",
    "* Does the feature learning and task learning simultaneously\n",
    "* The large number of parameters, the weights and biases, allow for understanding significantly more features than a human might have engineered\n",
    "* Considered black boxes due to the poor interpretability\n",
    "    * \"how\" and \"what\" features learnt by deep models are still open problems in the field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d633d",
   "metadata": {},
   "source": [
    "# Data Structures\n",
    "## One-Encoding\n",
    "A sparse encoding that's super inefficient\n",
    "## Bag of Words\n",
    "Every single one  of the words in the vocabulary are placed in a dictionary, and an index associated with them\n",
    "* Only keeps track of the words seen, and its frequency in the sentence\n",
    "* Loses the ordering of the word\n",
    "* Only really useful for simple tasks where the presence of certain words can affect the meaning greatly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690e887",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "Classifies or translates every single word into a vector with n-dimensions, and every component of the vector tells us how similar it is to other words. The vector of similar words would be pointing at a similar dimension\n",
    "\n",
    "The embedding itself is a layer of a model, and it actually learns the embedding of the word from its context\n",
    "\n",
    "That is, they optimize the representations they create given a certain\n",
    "criterion.  The  implicit,  default  criterion  is:  maximize  the  distinctiveness  of  the  vector\n",
    "representations, such that the confusability of any two vectors is kept to a minimum\n",
    "\n",
    "It's trained to optimize for a specific task, where the embedded representation is used later, such as sentiment classification after the embedding\n",
    "\n",
    "One downside is that it does not care about establishing relationship between words that share a similar context. word2vec tries to do just that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24f895e",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "Has 2 more or less equivalent implementations:\n",
    "* Predicting words from context\n",
    "* Predicting context from words\n",
    "\n",
    "Tries to maximize the similarity (or the dot product of) words that appear close together in a context and minimize words that do not\n",
    "\n",
    "$\\frac{v_c \\cdot v_w}{sum_i(v_{ci} \\cdot v_w)}$\n",
    "\n",
    "The numerator is calculating the similarity between the word and its context, while the denominator is calculating the similarity of all the other contexts and the target word. We maximize this ratio to rensure words that appear together in text have more similar vectors than words that do not.\n",
    "\n",
    "Because of the many possible contexts `ci`, computing this can be very slow, so we pick `ci` contexts at random in a process of negative sampling\n",
    "\n",
    "eg. If `cat` appears in the context of `food`, and the vector of `food` is more similar to the vector of `cat` than the vectors of several other randomly chosen words, such as `democracy`, `greed`, `Terry`, instead of all the words in the language, making it much faster to train\n",
    "\n",
    "1. Create a dictionary for the data, mapping every word to unique integers\n",
    "2. Using a random generator, and fixed context window size (context basically meaning the words surrounding the word we're targeting right now, and the context size meaning how many words total in each direction of the word to look at), collect valid context words in this window\n",
    "    * The `skipgrams()` function does this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9962a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(textFile,window_size):\n",
    "    couples=[]\n",
    "    labels=[]\n",
    "    sentences = getLines(textFile)\n",
    "    vocab = dict()\n",
    "    create_vocabulary(vocab, sentences)\n",
    "    vocab_size=len(vocab)\n",
    "    for s in sentences:\n",
    "        words=[]\n",
    "        for w in s.split(\" \"):\n",
    "            w=re.sub(\"[.,:;'\\\"!?()]+\",\"\",w.lower())\n",
    "            if w!='':\n",
    "                words.append(vocab[w])\n",
    "        c,l=skipgrams(words,vocab_size,window_size=window_size)\n",
    "        couples.extend(c)\n",
    "        labels.extend(l)\n",
    "    return vocab,couples,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f089d026",
   "metadata": {},
   "source": [
    "3. Sample arbitrary words in the dictionary that are out of the scope context of the input word, to generate the negative contexts\n",
    "    * eg. In the sentence `the restaurant has a terrible ambiance and the food is awful`, for the word `restaurant`, the words `the`, `has`, `a`, `terrible`, all fit in its context, but the words `terrible`, `ambience`, `and`, don't\n",
    "    * In this toy example, we also artifically increase the number of words in the vocabulary to a lot higher to avoid labeling valid context words as non-context words\n",
    "    * The `skipgrams()` function does this\n",
    "4. Generate batches to train hthe model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(target,context, labels, batch_size):\n",
    "    batch_target = np.zeros((batch_size, 1))\n",
    "    batch_context = np.zeros((batch_size, 1))\n",
    "    batch_labels = np.zeros((batch_size,1))\n",
    "    while True:\n",
    "        for i in range(batch_size):\n",
    "            index= random.randint(0,len(target)-1)\n",
    "            batch_target[i] = target[index]\n",
    "            batch_context[i]=context[index]\n",
    "            batch_labels[i] = labels[index]\n",
    "        yield [batch_target,batch_context], [batch_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa93019",
   "metadata": {},
   "source": [
    "5. Use a combination of two Embedding layers, one for the source words, and one for the context words, feeding into a dense layer, to which the output labels are fed a 0 for a negative context, vice versa. The network learns to decide whether the chosen context words are valid contexts for the target words\n",
    "\n",
    "You can also use pre-trained word2vec models trained from better training data, such as the Stanford University wrd2vec model with 400k vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85371a0",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "Embedding can extend past word level to sentences, paragraphs and even entire documents, collectively referred to as documents\n",
    "\n",
    "Useful for:\n",
    "* Matching questions with answers\n",
    "* Document retrieval\n",
    "    * Similar documents to an input document\n",
    "    * eg. Given a restaurant review about the fish, you want to find similar complaints about seafood\n",
    "    \n",
    "The general approach is to give each document a unique ID, and average the embedding of every word embedding in the document, along with the embedding of the paragraph ID\n",
    "\n",
    "We use a sliding window of a pre-specified size (eg. 3), and use that to create n-grams from the document. Then we use every n-gram, the document ID, and a target word, which is the first word beyond the current n-gram, to train the model to predict this target word. The document ID becomes associated with many n-grams inside itself during training, forming a memory and encoding the missing information from the local n-grams to predict the next word\n",
    "\n",
    "After training, the document ID becomes associated with vectors that describe the aggregation of this missing information for predicting next words, for all sequences; the approximation of the topic of a document\n",
    "\n",
    "eg. For the document with ID `127`, the content is 'My first visit to Hiro was a delight!'\n",
    "\n",
    "The n-grams and their respective targets become :\n",
    "* my first visit - to\n",
    "* first visit to - Hiro\n",
    "* visit to Hiro - was\n",
    "* to Hiro was - a\n",
    "* Hiro was a - delight\n",
    "* was a delight (we can choose to generate an `end of sentence` pseudotoken for this)\n",
    "\n",
    "The dataset is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31226d75",
   "metadata": {},
   "source": [
    "`Took an hour to get our food only 4 tables in restaurant my food was Luke warm,\n",
    "Our server was running around like he was totally overwhelmed.\n",
    "There is not a deal good enough that would drag me into that establishment again.\n",
    "Hard to judge whether these sides were good because we were grossed out by the melted\n",
    "styrofoam and didn't want to eat it for fear of getting sick.\n",
    "On a positive note, our server was very attentive and provided great service.\n",
    "Frozen pucks of disgust, with some of the worst people behind the register.\n",
    "The only thing I did like was the prime rib and dessert section.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15f1d4",
   "metadata": {},
   "source": [
    "The following code reads the file with one document per line into integer-valued arrays for teh document identifiers, context arrays of word n-grams, and target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92375422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
