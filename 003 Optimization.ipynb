{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74b6657c",
   "metadata": {},
   "source": [
    "### Full Code Up Until This Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2f00d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "| || || |_: 1.0986104\n",
      "acc: 0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):        \n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # gradient on values to pass further back\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# Rectified Linear Unit Activation Function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        #create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # calculate jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # calculate sample wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    #inheriting from the base Loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred will come from the neural network\n",
    "        # y_true will come from the training set\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # means scalar class values have been passed\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # one hot encoded values have been passed\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Backwards pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        # number of labels in every sample\n",
    "        # we'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # if labels are sparse, turn them into one-hot vector\n",
    "        # in case the shape is as [3, 0, 2] etc etc per sample\n",
    "        # we turn them into one-hot vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        # with a larger number of batches, it's all summed together\n",
    "        # in the dot product, and some will be given more importance\n",
    "        # than others when we don't normaize\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # creates activation and loss function objects inside\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # set the output\n",
    "        self.output = self.activation.output\n",
    "        # calculate and return the loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #if labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "            \n",
    "        # copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# create dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 3 inputs and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "# peeking the output of the first few samples\n",
    "print(loss_activation.output[:5])\n",
    "\n",
    "# loss value\n",
    "print(\"| || || |_:\", loss)\n",
    "\n",
    "# calculating accuracy of output of activation 2 and targets\n",
    "# calculate values along first axis because of batches\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis = 1)\n",
    "    \n",
    "accuracy = np.mean(predictions == y)\n",
    "print(\"acc:\", accuracy)\n",
    "\n",
    "# backward pass\n",
    "loss_activation.backward(loss_activation.output, y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)\n",
    "\n",
    "#gradients\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f585120",
   "metadata": {},
   "source": [
    "Simplest and the most rudimentary optimization method is to subtract a fraction of the gradient from each weight and bias. This is the **Stochastic Gradient Descent (SGD)**. Most optimizers are actually just a variant of SGD.\n",
    "\n",
    "### Terminology\n",
    "**Stochastic Gradient Descent**: Optimizers that fits a single sample at a time\n",
    "\n",
    "**Batch Gradient Descent**: Optimizers that fit a whole dataset at once\n",
    "\n",
    "**Mini-batch Gradient Descnet**: Optimizers that fit slices of a dataset, ie batches, at once\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058e985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    def __init__(self, learning_rate = 1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e37b0",
   "metadata": {},
   "source": [
    "And for the optimizer object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8788ac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer_SGD()\n",
    "\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be10fd",
   "metadata": {},
   "source": [
    "This is everything needed to train the model\n",
    "\n",
    "Now, what remains is to repeatedly loop back and forth until reaching some stopping point\n",
    "\n",
    "Each full forward, and backward pass thrugh all of the training data is the **epoch**. Most models train on the dataset for multiple epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec8794c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333, loss: 1.099\n",
      "epoch: 100, acc: 0.373, loss: 1.093\n",
      "epoch: 200, acc: 0.383, loss: 1.078\n",
      "epoch: 300, acc: 0.403, loss: 1.076\n",
      "epoch: 400, acc: 0.400, loss: 1.075\n",
      "epoch: 500, acc: 0.397, loss: 1.074\n",
      "epoch: 600, acc: 0.393, loss: 1.072\n",
      "epoch: 700, acc: 0.380, loss: 1.070\n",
      "epoch: 800, acc: 0.403, loss: 1.067\n",
      "epoch: 900, acc: 0.407, loss: 1.064\n",
      "epoch: 1000, acc: 0.417, loss: 1.060\n",
      "epoch: 1100, acc: 0.443, loss: 1.055\n",
      "epoch: 1200, acc: 0.400, loss: 1.063\n",
      "epoch: 1300, acc: 0.383, loss: 1.063\n",
      "epoch: 1400, acc: 0.383, loss: 1.062\n",
      "epoch: 1500, acc: 0.400, loss: 1.061\n",
      "epoch: 1600, acc: 0.407, loss: 1.061\n",
      "epoch: 1700, acc: 0.400, loss: 1.060\n",
      "epoch: 1800, acc: 0.420, loss: 1.059\n",
      "epoch: 1900, acc: 0.407, loss: 1.052\n",
      "epoch: 2000, acc: 0.403, loss: 1.060\n",
      "epoch: 2100, acc: 0.413, loss: 1.048\n",
      "epoch: 2200, acc: 0.443, loss: 1.052\n",
      "epoch: 2300, acc: 0.400, loss: 1.068\n",
      "epoch: 2400, acc: 0.443, loss: 1.052\n",
      "epoch: 2500, acc: 0.400, loss: 1.090\n",
      "epoch: 2600, acc: 0.447, loss: 1.044\n",
      "epoch: 2700, acc: 0.403, loss: 1.067\n",
      "epoch: 2800, acc: 0.403, loss: 1.034\n",
      "epoch: 2900, acc: 0.403, loss: 1.025\n",
      "epoch: 3000, acc: 0.460, loss: 1.016\n",
      "epoch: 3100, acc: 0.477, loss: 1.013\n",
      "epoch: 3200, acc: 0.457, loss: 1.000\n",
      "epoch: 3300, acc: 0.480, loss: 0.997\n",
      "epoch: 3400, acc: 0.480, loss: 0.996\n",
      "epoch: 3500, acc: 0.443, loss: 1.001\n",
      "epoch: 3600, acc: 0.537, loss: 0.982\n",
      "epoch: 3700, acc: 0.473, loss: 0.966\n",
      "epoch: 3800, acc: 0.453, loss: 0.987\n",
      "epoch: 3900, acc: 0.477, loss: 0.965\n",
      "epoch: 4000, acc: 0.503, loss: 0.957\n",
      "epoch: 4100, acc: 0.513, loss: 0.941\n",
      "epoch: 4200, acc: 0.553, loss: 0.926\n",
      "epoch: 4300, acc: 0.520, loss: 0.924\n",
      "epoch: 4400, acc: 0.557, loss: 0.904\n",
      "epoch: 4500, acc: 0.523, loss: 0.902\n",
      "epoch: 4600, acc: 0.590, loss: 0.865\n",
      "epoch: 4700, acc: 0.550, loss: 0.868\n",
      "epoch: 4800, acc: 0.590, loss: 0.831\n",
      "epoch: 4900, acc: 0.553, loss: 0.843\n",
      "epoch: 5000, acc: 0.610, loss: 0.818\n",
      "epoch: 5100, acc: 0.570, loss: 0.845\n",
      "epoch: 5200, acc: 0.620, loss: 0.819\n",
      "epoch: 5300, acc: 0.613, loss: 0.879\n",
      "epoch: 5400, acc: 0.620, loss: 0.767\n",
      "epoch: 5500, acc: 0.620, loss: 0.843\n",
      "epoch: 5600, acc: 0.613, loss: 0.786\n",
      "epoch: 5700, acc: 0.623, loss: 0.747\n",
      "epoch: 5800, acc: 0.637, loss: 0.729\n",
      "epoch: 5900, acc: 0.630, loss: 0.759\n",
      "epoch: 6000, acc: 0.727, loss: 0.675\n",
      "epoch: 6100, acc: 0.580, loss: 0.924\n",
      "epoch: 6200, acc: 0.727, loss: 0.642\n",
      "epoch: 6300, acc: 0.560, loss: 0.860\n",
      "epoch: 6400, acc: 0.700, loss: 0.637\n",
      "epoch: 6500, acc: 0.637, loss: 0.771\n",
      "epoch: 6600, acc: 0.713, loss: 0.630\n",
      "epoch: 6700, acc: 0.690, loss: 0.624\n",
      "epoch: 6800, acc: 0.683, loss: 0.624\n",
      "epoch: 6900, acc: 0.680, loss: 0.625\n",
      "epoch: 7000, acc: 0.677, loss: 0.624\n",
      "epoch: 7100, acc: 0.693, loss: 0.608\n",
      "epoch: 7200, acc: 0.657, loss: 0.808\n",
      "epoch: 7300, acc: 0.693, loss: 0.603\n",
      "epoch: 7400, acc: 0.713, loss: 0.597\n",
      "epoch: 7500, acc: 0.720, loss: 0.587\n",
      "epoch: 7600, acc: 0.713, loss: 0.591\n",
      "epoch: 7700, acc: 0.720, loss: 0.588\n",
      "epoch: 7800, acc: 0.720, loss: 0.591\n",
      "epoch: 7900, acc: 0.723, loss: 0.588\n",
      "epoch: 8000, acc: 0.700, loss: 0.600\n",
      "epoch: 8100, acc: 0.720, loss: 0.582\n",
      "epoch: 8200, acc: 0.713, loss: 0.580\n",
      "epoch: 8300, acc: 0.713, loss: 0.591\n",
      "epoch: 8400, acc: 0.713, loss: 0.578\n",
      "epoch: 8500, acc: 0.500, loss: 1.689\n",
      "epoch: 8600, acc: 0.723, loss: 0.576\n",
      "epoch: 8700, acc: 0.710, loss: 0.589\n",
      "epoch: 8800, acc: 0.723, loss: 0.572\n",
      "epoch: 8900, acc: 0.713, loss: 0.584\n",
      "epoch: 9000, acc: 0.727, loss: 0.569\n",
      "epoch: 9100, acc: 0.733, loss: 0.575\n",
      "epoch: 9200, acc: 0.633, loss: 0.923\n",
      "epoch: 9300, acc: 0.730, loss: 0.563\n",
      "epoch: 9400, acc: 0.737, loss: 0.551\n",
      "epoch: 9500, acc: 0.727, loss: 0.562\n",
      "epoch: 9600, acc: 0.723, loss: 0.564\n",
      "epoch: 9700, acc: 0.710, loss: 0.569\n",
      "epoch: 9800, acc: 0.737, loss: 0.556\n",
      "epoch: 9900, acc: 0.720, loss: 0.575\n",
      "epoch: 10000, acc: 0.737, loss: 0.556\n",
      "epoch: 10100, acc: 0.710, loss: 0.640\n",
      "epoch: 10200, acc: 0.740, loss: 0.550\n",
      "epoch: 10300, acc: 0.730, loss: 0.578\n",
      "epoch: 10400, acc: 0.743, loss: 0.546\n",
      "epoch: 10500, acc: 0.740, loss: 0.546\n",
      "epoch: 10600, acc: 0.673, loss: 0.718\n",
      "epoch: 10700, acc: 0.743, loss: 0.541\n",
      "epoch: 10800, acc: 0.740, loss: 0.574\n",
      "epoch: 10900, acc: 0.753, loss: 0.535\n",
      "epoch: 11000, acc: 0.743, loss: 0.537\n",
      "epoch: 11100, acc: 0.740, loss: 0.587\n",
      "epoch: 11200, acc: 0.763, loss: 0.529\n",
      "epoch: 11300, acc: 0.747, loss: 0.527\n",
      "epoch: 11400, acc: 0.767, loss: 0.514\n",
      "epoch: 11500, acc: 0.763, loss: 0.515\n",
      "epoch: 11600, acc: 0.797, loss: 0.479\n",
      "epoch: 11700, acc: 0.770, loss: 0.499\n",
      "epoch: 11800, acc: 0.770, loss: 0.489\n",
      "epoch: 11900, acc: 0.770, loss: 0.490\n",
      "epoch: 12000, acc: 0.780, loss: 0.484\n",
      "epoch: 12100, acc: 0.780, loss: 0.482\n",
      "epoch: 12200, acc: 0.793, loss: 0.471\n",
      "epoch: 12300, acc: 0.783, loss: 0.473\n",
      "epoch: 12400, acc: 0.787, loss: 0.466\n",
      "epoch: 12500, acc: 0.797, loss: 0.463\n",
      "epoch: 12600, acc: 0.803, loss: 0.462\n",
      "epoch: 12700, acc: 0.800, loss: 0.458\n",
      "epoch: 12800, acc: 0.803, loss: 0.458\n",
      "epoch: 12900, acc: 0.800, loss: 0.456\n",
      "epoch: 13000, acc: 0.800, loss: 0.456\n",
      "epoch: 13100, acc: 0.800, loss: 0.454\n",
      "epoch: 13200, acc: 0.767, loss: 0.485\n",
      "epoch: 13300, acc: 0.800, loss: 0.452\n",
      "epoch: 13400, acc: 0.787, loss: 0.458\n",
      "epoch: 13500, acc: 0.787, loss: 0.468\n",
      "epoch: 13600, acc: 0.803, loss: 0.447\n",
      "epoch: 13700, acc: 0.633, loss: 0.764\n",
      "epoch: 13800, acc: 0.803, loss: 0.444\n",
      "epoch: 13900, acc: 0.807, loss: 0.445\n",
      "epoch: 14000, acc: 0.803, loss: 0.441\n",
      "epoch: 14100, acc: 0.807, loss: 0.440\n",
      "epoch: 14200, acc: 0.817, loss: 0.442\n",
      "epoch: 14300, acc: 0.830, loss: 0.433\n",
      "epoch: 14400, acc: 0.807, loss: 0.445\n",
      "epoch: 14500, acc: 0.817, loss: 0.439\n",
      "epoch: 14600, acc: 0.817, loss: 0.439\n",
      "epoch: 14700, acc: 0.823, loss: 0.443\n",
      "epoch: 14800, acc: 0.813, loss: 0.438\n",
      "epoch: 14900, acc: 0.813, loss: 0.438\n",
      "epoch: 15000, acc: 0.823, loss: 0.430\n",
      "epoch: 15100, acc: 0.830, loss: 0.426\n",
      "epoch: 15200, acc: 0.827, loss: 0.429\n",
      "epoch: 15300, acc: 0.857, loss: 0.379\n",
      "epoch: 15400, acc: 0.830, loss: 0.429\n",
      "epoch: 15500, acc: 0.830, loss: 0.430\n",
      "epoch: 15600, acc: 0.833, loss: 0.423\n",
      "epoch: 15700, acc: 0.833, loss: 0.416\n",
      "epoch: 15800, acc: 0.837, loss: 0.413\n",
      "epoch: 15900, acc: 0.833, loss: 0.420\n",
      "epoch: 16000, acc: 0.837, loss: 0.418\n",
      "epoch: 16100, acc: 0.843, loss: 0.405\n",
      "epoch: 16200, acc: 0.837, loss: 0.412\n",
      "epoch: 16300, acc: 0.827, loss: 0.424\n",
      "epoch: 16400, acc: 0.827, loss: 0.427\n",
      "epoch: 16500, acc: 0.840, loss: 0.417\n",
      "epoch: 16600, acc: 0.833, loss: 0.409\n",
      "epoch: 16700, acc: 0.837, loss: 0.415\n",
      "epoch: 16800, acc: 0.843, loss: 0.402\n",
      "epoch: 16900, acc: 0.850, loss: 0.379\n",
      "epoch: 17000, acc: 0.847, loss: 0.395\n",
      "epoch: 17100, acc: 0.830, loss: 0.423\n",
      "epoch: 17200, acc: 0.830, loss: 0.419\n",
      "epoch: 17300, acc: 0.837, loss: 0.415\n",
      "epoch: 17400, acc: 0.843, loss: 0.400\n",
      "epoch: 17500, acc: 0.830, loss: 0.425\n",
      "epoch: 17600, acc: 0.830, loss: 0.427\n",
      "epoch: 17700, acc: 0.827, loss: 0.433\n",
      "epoch: 17800, acc: 0.823, loss: 0.444\n",
      "epoch: 17900, acc: 0.863, loss: 0.390\n",
      "epoch: 18000, acc: 0.853, loss: 0.394\n",
      "epoch: 18100, acc: 0.883, loss: 0.342\n",
      "epoch: 18200, acc: 0.873, loss: 0.354\n",
      "epoch: 18300, acc: 0.827, loss: 0.438\n",
      "epoch: 18400, acc: 0.827, loss: 0.446\n",
      "epoch: 18500, acc: 0.853, loss: 0.406\n",
      "epoch: 18600, acc: 0.850, loss: 0.378\n",
      "epoch: 18700, acc: 0.793, loss: 0.506\n",
      "epoch: 18800, acc: 0.850, loss: 0.377\n",
      "epoch: 18900, acc: 0.850, loss: 0.372\n",
      "epoch: 19000, acc: 0.847, loss: 0.403\n",
      "epoch: 19100, acc: 0.847, loss: 0.372\n",
      "epoch: 19200, acc: 0.847, loss: 0.374\n",
      "epoch: 19300, acc: 0.857, loss: 0.364\n",
      "epoch: 19400, acc: 0.847, loss: 0.372\n",
      "epoch: 19500, acc: 0.853, loss: 0.365\n",
      "epoch: 19600, acc: 0.767, loss: 0.613\n",
      "epoch: 19700, acc: 0.847, loss: 0.366\n",
      "epoch: 19800, acc: 0.850, loss: 0.367\n",
      "epoch: 19900, acc: 0.890, loss: 0.298\n",
      "epoch: 20000, acc: 0.847, loss: 0.366\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_SGD(0.85)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(20001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88b12db",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "The only hyper parameter for our model aside from the architecture, the learning rate is controlld to avoid a local minimum\n",
    "\n",
    "#### Momentum\n",
    "Basically inertia for our optimizer, high inertia, and low learning rate models can generally slowly creep towards a global minimum\n",
    "\n",
    "#### Gradient Explosion\n",
    "A learning rate that's too high can result in the adjustments being in the right direction but overshooting by a large amount, resulting in an unstable model, with loss jumping around. Gradient explosion is where the parameter update causes the output to be increasing, until the float can't store this variable, causing an overflow\n",
    "\n",
    "Because larger models can take weeks or more to train, it is very important to watch out for these sorts of things. Generally you want to be able to change the learning rate during the training.\n",
    "\n",
    "In most cases, start with larger learning rate and decrease over time; **learning rate decay**\n",
    "\n",
    "### Learning Rate Decay\n",
    "The crux here is to figure out the **Decay Rate**. Possible ways include:\n",
    "* Decrease learning rate in response to the loss across epochs, eg the loss has been plateauing or jumping over large deltas  \n",
    "* Simly track your loss over time and manually decrease the learning rate when deemed appropriate\n",
    "* Decay per step, ie **1/t decaying** or **exponential decaying**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c97d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1.0, decay = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2072a306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.310, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.400, loss: 1.079, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.383, loss: 1.070, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.393, loss: 1.068, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.380, loss: 1.067, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.380, loss: 1.065, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.380, loss: 1.063, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.387, loss: 1.060, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.417, loss: 1.051, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.463, loss: 1.033, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.433, loss: 1.028, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.447, loss: 1.015, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.450, loss: 1.007, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.473, loss: 0.999, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.457, loss: 0.999, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.470, loss: 0.984, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.533, loss: 0.985, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.467, loss: 1.003, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.490, loss: 0.971, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.533, loss: 0.991, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.477, loss: 0.963, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.517, loss: 0.963, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.523, loss: 0.940, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.493, loss: 0.941, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.557, loss: 0.928, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.540, loss: 0.920, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.570, loss: 0.920, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.557, loss: 0.899, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.570, loss: 0.882, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.580, loss: 0.873, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.573, loss: 0.868, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.553, loss: 0.876, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.577, loss: 0.866, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.623, loss: 0.839, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.590, loss: 0.883, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.607, loss: 0.813, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.617, loss: 0.823, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.567, loss: 0.847, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.597, loss: 0.824, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.640, loss: 0.799, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.593, loss: 0.811, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.690, loss: 0.742, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.660, loss: 0.728, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.670, loss: 0.761, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.623, loss: 0.781, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.553, loss: 0.970, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.643, loss: 0.773, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.633, loss: 0.744, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.663, loss: 0.767, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.630, loss: 0.739, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.730, loss: 0.692, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.673, loss: 0.712, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.703, loss: 0.650, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.690, loss: 0.704, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.733, loss: 0.641, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.700, loss: 0.683, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.730, loss: 0.643, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.687, loss: 0.676, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.637, loss: 0.824, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.687, loss: 0.675, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.633, loss: 0.901, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.697, loss: 0.655, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.687, loss: 0.655, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.550, loss: 1.131, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.683, loss: 0.653, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.693, loss: 0.643, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.587, loss: 1.063, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.690, loss: 0.622, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.717, loss: 0.646, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.737, loss: 0.611, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.740, loss: 0.605, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.737, loss: 0.617, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.540, loss: 0.989, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.593, loss: 0.877, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.757, loss: 0.546, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.750, loss: 0.572, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.750, loss: 0.602, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.750, loss: 0.565, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.747, loss: 0.569, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.753, loss: 0.556, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.753, loss: 0.592, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.757, loss: 0.587, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.750, loss: 0.585, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.657, loss: 0.712, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.740, loss: 0.614, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.743, loss: 0.584, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.750, loss: 0.586, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.770, loss: 0.560, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.587, loss: 0.931, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.777, loss: 0.539, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.770, loss: 0.532, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.777, loss: 0.529, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.573, loss: 1.040, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.777, loss: 0.509, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.780, loss: 0.521, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.777, loss: 0.499, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.783, loss: 0.514, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.777, loss: 0.495, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.780, loss: 0.498, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.790, loss: 0.490, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.790, loss: 0.494, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_SGD(decay = 1e-3)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4efafbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.423, loss: 1.099, lr: 0.9901970492127933\n",
      "epoch: 200, acc: 0.410, loss: 1.099, lr: 0.9804882831650161\n",
      "epoch: 300, acc: 0.420, loss: 1.098, lr: 0.9709680551509855\n",
      "epoch: 400, acc: 0.397, loss: 1.072, lr: 0.9616309260505818\n",
      "epoch: 500, acc: 0.457, loss: 1.067, lr: 0.9524716639679969\n",
      "epoch: 600, acc: 0.417, loss: 1.065, lr: 0.9434852344560807\n",
      "epoch: 700, acc: 0.407, loss: 1.059, lr: 0.9346667912889054\n",
      "epoch: 800, acc: 0.427, loss: 1.059, lr: 0.9260116677470135\n",
      "epoch: 900, acc: 0.420, loss: 1.059, lr: 0.9175153683824203\n",
      "epoch: 1000, acc: 0.410, loss: 1.058, lr: 0.9091735612328392\n",
      "epoch: 1100, acc: 0.400, loss: 1.056, lr: 0.9009820704567978\n",
      "epoch: 1200, acc: 0.403, loss: 1.047, lr: 0.892936869363336\n",
      "epoch: 1300, acc: 0.447, loss: 1.029, lr: 0.8850340738118416\n",
      "epoch: 1400, acc: 0.413, loss: 1.014, lr: 0.8772699359592947\n",
      "epoch: 1500, acc: 0.427, loss: 0.988, lr: 0.8696408383337683\n",
      "epoch: 1600, acc: 0.517, loss: 0.910, lr: 0.8621432882145013\n",
      "epoch: 1700, acc: 0.607, loss: 0.830, lr: 0.8547739123001966\n",
      "epoch: 1800, acc: 0.560, loss: 0.858, lr: 0.8475294516484448\n",
      "epoch: 1900, acc: 0.750, loss: 0.631, lr: 0.8404067568703253\n",
      "epoch: 2000, acc: 0.793, loss: 0.522, lr: 0.8334027835652972\n",
      "epoch: 2100, acc: 0.813, loss: 0.501, lr: 0.8265145879824779\n",
      "epoch: 2200, acc: 0.810, loss: 0.409, lr: 0.8197393228953193\n",
      "epoch: 2300, acc: 0.873, loss: 0.334, lr: 0.8130742336775347\n",
      "epoch: 2400, acc: 0.767, loss: 0.543, lr: 0.8065166545689169\n",
      "epoch: 2500, acc: 0.857, loss: 0.354, lr: 0.8000640051204096\n",
      "epoch: 2600, acc: 0.860, loss: 0.332, lr: 0.7937137868084768\n",
      "epoch: 2700, acc: 0.880, loss: 0.282, lr: 0.7874635798094338\n",
      "epoch: 2800, acc: 0.897, loss: 0.236, lr: 0.7813110399249941\n",
      "epoch: 2900, acc: 0.903, loss: 0.226, lr: 0.7752538956508256\n",
      "epoch: 3000, acc: 0.743, loss: 0.785, lr: 0.7692899453804138\n",
      "epoch: 3100, acc: 0.853, loss: 0.342, lr: 0.7634170547370028\n",
      "epoch: 3200, acc: 0.900, loss: 0.215, lr: 0.7576331540268202\n",
      "epoch: 3300, acc: 0.913, loss: 0.208, lr: 0.7519362358072035\n",
      "epoch: 3400, acc: 0.910, loss: 0.181, lr: 0.7463243525636241\n",
      "epoch: 3500, acc: 0.927, loss: 0.176, lr: 0.7407956144899621\n",
      "epoch: 3600, acc: 0.840, loss: 0.390, lr: 0.735348187366718\n",
      "epoch: 3700, acc: 0.917, loss: 0.220, lr: 0.7299802905321557\n",
      "epoch: 3800, acc: 0.873, loss: 0.306, lr: 0.7246901949416624\n",
      "epoch: 3900, acc: 0.917, loss: 0.196, lr: 0.7194762213108857\n",
      "epoch: 4000, acc: 0.910, loss: 0.231, lr: 0.7143367383384527\n",
      "epoch: 4100, acc: 0.873, loss: 0.278, lr: 0.7092701610043266\n",
      "epoch: 4200, acc: 0.907, loss: 0.200, lr: 0.7042749489400663\n",
      "epoch: 4300, acc: 0.877, loss: 0.286, lr: 0.6993496048674733\n",
      "epoch: 4400, acc: 0.923, loss: 0.207, lr: 0.6944926731022988\n",
      "epoch: 4500, acc: 0.930, loss: 0.167, lr: 0.6897027381198704\n",
      "epoch: 4600, acc: 0.923, loss: 0.186, lr: 0.6849784231796698\n",
      "epoch: 4700, acc: 0.903, loss: 0.189, lr: 0.6803183890060548\n",
      "epoch: 4800, acc: 0.907, loss: 0.187, lr: 0.6757213325224677\n",
      "epoch: 4900, acc: 0.920, loss: 0.159, lr: 0.6711859856366199\n",
      "epoch: 5000, acc: 0.937, loss: 0.158, lr: 0.6667111140742716\n",
      "epoch: 5100, acc: 0.583, loss: 1.576, lr: 0.6622955162593549\n",
      "epoch: 5200, acc: 0.900, loss: 0.216, lr: 0.6579380222383051\n",
      "epoch: 5300, acc: 0.927, loss: 0.157, lr: 0.6536374926465782\n",
      "epoch: 5400, acc: 0.917, loss: 0.193, lr: 0.649392817715436\n",
      "epoch: 5500, acc: 0.927, loss: 0.142, lr: 0.6452029163171817\n",
      "epoch: 5600, acc: 0.900, loss: 0.211, lr: 0.6410667350471184\n",
      "epoch: 5700, acc: 0.930, loss: 0.133, lr: 0.6369832473405949\n",
      "epoch: 5800, acc: 0.913, loss: 0.153, lr: 0.6329514526235838\n",
      "epoch: 5900, acc: 0.933, loss: 0.157, lr: 0.6289703754953141\n",
      "epoch: 6000, acc: 0.917, loss: 0.186, lr: 0.6250390649415589\n",
      "epoch: 6100, acc: 0.933, loss: 0.146, lr: 0.6211565935772407\n",
      "epoch: 6200, acc: 0.943, loss: 0.122, lr: 0.6173220569170937\n",
      "epoch: 6300, acc: 0.933, loss: 0.149, lr: 0.6135345726731701\n",
      "epoch: 6400, acc: 0.917, loss: 0.204, lr: 0.6097932800780536\n",
      "epoch: 6500, acc: 0.943, loss: 0.121, lr: 0.6060973392326807\n",
      "epoch: 6600, acc: 0.893, loss: 0.223, lr: 0.6024459304777396\n",
      "epoch: 6700, acc: 0.927, loss: 0.157, lr: 0.5988382537876519\n",
      "epoch: 6800, acc: 0.937, loss: 0.133, lr: 0.5952735281862016\n",
      "epoch: 6900, acc: 0.947, loss: 0.123, lr: 0.5917509911829102\n",
      "epoch: 7000, acc: 0.930, loss: 0.137, lr: 0.5882698982293076\n",
      "epoch: 7100, acc: 0.960, loss: 0.110, lr: 0.5848295221942803\n",
      "epoch: 7200, acc: 0.957, loss: 0.112, lr: 0.5814291528577243\n",
      "epoch: 7300, acc: 0.953, loss: 0.112, lr: 0.5780680964217585\n",
      "epoch: 7400, acc: 0.950, loss: 0.097, lr: 0.5747456750387954\n",
      "epoch: 7500, acc: 0.963, loss: 0.095, lr: 0.5714612263557918\n",
      "epoch: 7600, acc: 0.960, loss: 0.090, lr: 0.5682141030740383\n",
      "epoch: 7700, acc: 0.957, loss: 0.100, lr: 0.5650036725238714\n",
      "epoch: 7800, acc: 0.930, loss: 0.160, lr: 0.5618293162537221\n",
      "epoch: 7900, acc: 0.950, loss: 0.124, lr: 0.5586904296329404\n",
      "epoch: 8000, acc: 0.960, loss: 0.091, lr: 0.5555864214678593\n",
      "epoch: 8100, acc: 0.957, loss: 0.086, lr: 0.5525167136305873\n",
      "epoch: 8200, acc: 0.953, loss: 0.105, lr: 0.5494807407000385\n",
      "epoch: 8300, acc: 0.960, loss: 0.100, lr: 0.5464779496147331\n",
      "epoch: 8400, acc: 0.933, loss: 0.178, lr: 0.5435077993369205\n",
      "epoch: 8500, acc: 0.967, loss: 0.097, lr: 0.5405697605275961\n",
      "epoch: 8600, acc: 0.957, loss: 0.093, lr: 0.5376633152320017\n",
      "epoch: 8700, acc: 0.890, loss: 0.253, lr: 0.5347879565752179\n",
      "epoch: 8800, acc: 0.917, loss: 0.182, lr: 0.5319431884674717\n",
      "epoch: 8900, acc: 0.933, loss: 0.175, lr: 0.5291285253188\n",
      "epoch: 9000, acc: 0.940, loss: 0.159, lr: 0.5263434917627243\n",
      "epoch: 9100, acc: 0.873, loss: 0.275, lr: 0.5235876223886068\n",
      "epoch: 9200, acc: 0.923, loss: 0.179, lr: 0.5208604614823689\n",
      "epoch: 9300, acc: 0.943, loss: 0.149, lr: 0.5181615627752734\n",
      "epoch: 9400, acc: 0.937, loss: 0.137, lr: 0.5154904892004742\n",
      "epoch: 9500, acc: 0.940, loss: 0.155, lr: 0.5128468126570593\n",
      "epoch: 9600, acc: 0.950, loss: 0.112, lr: 0.5102301137813153\n",
      "epoch: 9700, acc: 0.940, loss: 0.127, lr: 0.5076399817249606\n",
      "epoch: 9800, acc: 0.937, loss: 0.131, lr: 0.5050760139400979\n",
      "epoch: 9900, acc: 0.953, loss: 0.108, lr: 0.5025378159706518\n",
      "epoch: 10000, acc: 0.947, loss: 0.119, lr: 0.5000250012500626\n",
      "epoch: 10100, acc: 0.950, loss: 0.107, lr: 0.49753719090502013\n",
      "epoch: 10200, acc: 0.927, loss: 0.185, lr: 0.495074013565028\n",
      "epoch: 10300, acc: 0.927, loss: 0.154, lr: 0.49263510517759496\n",
      "epoch: 10400, acc: 0.933, loss: 0.152, lr: 0.4902201088288641\n",
      "epoch: 10500, acc: 0.897, loss: 0.180, lr: 0.4878286745694912\n",
      "epoch: 10600, acc: 0.920, loss: 0.169, lr: 0.4854604592455945\n",
      "epoch: 10700, acc: 0.887, loss: 0.255, lr: 0.4831151263346055\n",
      "epoch: 10800, acc: 0.947, loss: 0.124, lr: 0.480792345785855\n",
      "epoch: 10900, acc: 0.957, loss: 0.107, lr: 0.4784917938657352\n",
      "epoch: 11000, acc: 0.950, loss: 0.099, lr: 0.4762131530072861\n",
      "epoch: 11100, acc: 0.963, loss: 0.086, lr: 0.4739561116640599\n",
      "epoch: 11200, acc: 0.967, loss: 0.089, lr: 0.4717203641681211\n",
      "epoch: 11300, acc: 0.957, loss: 0.094, lr: 0.46950561059204654\n",
      "epoch: 11400, acc: 0.640, loss: 2.480, lr: 0.4673115566147951\n",
      "epoch: 11500, acc: 0.930, loss: 0.165, lr: 0.4651379133913205\n",
      "epoch: 11600, acc: 0.950, loss: 0.096, lr: 0.46298439742580666\n",
      "epoch: 11700, acc: 0.953, loss: 0.088, lr: 0.46085073044840774\n",
      "epoch: 11800, acc: 0.933, loss: 0.143, lr: 0.45873663929538055\n",
      "epoch: 11900, acc: 0.880, loss: 0.252, lr: 0.456641855792502\n",
      "epoch: 12000, acc: 0.837, loss: 0.345, lr: 0.4545661166416655\n",
      "epoch: 12100, acc: 0.907, loss: 0.195, lr: 0.452509163310557\n",
      "epoch: 12200, acc: 0.920, loss: 0.163, lr: 0.45047074192531195\n",
      "epoch: 12300, acc: 0.913, loss: 0.182, lr: 0.4484506031660613\n",
      "epoch: 12400, acc: 0.943, loss: 0.131, lr: 0.44644850216527526\n",
      "epoch: 12500, acc: 0.917, loss: 0.181, lr: 0.4444641984088181\n",
      "epoch: 12600, acc: 0.933, loss: 0.149, lr: 0.44249745563963006\n",
      "epoch: 12700, acc: 0.947, loss: 0.121, lr: 0.4405480417639544\n",
      "epoch: 12800, acc: 0.967, loss: 0.096, lr: 0.43861572876003335\n",
      "epoch: 12900, acc: 0.927, loss: 0.171, lr: 0.43670029258919596\n",
      "epoch: 13000, acc: 0.917, loss: 0.257, lr: 0.4348015131092656\n",
      "epoch: 13100, acc: 0.953, loss: 0.097, lr: 0.43291917399021607\n",
      "epoch: 13200, acc: 0.897, loss: 0.185, lr: 0.43105306263201\n",
      "epoch: 13300, acc: 0.903, loss: 0.170, lr: 0.42920297008455294\n",
      "epoch: 13400, acc: 0.910, loss: 0.161, lr: 0.42736869096969954\n",
      "epoch: 13500, acc: 0.910, loss: 0.158, lr: 0.42555002340525133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13600, acc: 0.913, loss: 0.151, lr: 0.42374676893088686\n",
      "epoch: 13700, acc: 0.897, loss: 0.193, lr: 0.4219587324359677\n",
      "epoch: 13800, acc: 0.917, loss: 0.160, lr: 0.4201857220891634\n",
      "epoch: 13900, acc: 0.920, loss: 0.142, lr: 0.41842754926984393\n",
      "epoch: 14000, acc: 0.923, loss: 0.155, lr: 0.4166840285011875\n",
      "epoch: 14100, acc: 0.897, loss: 0.203, lr: 0.4149549773849537\n",
      "epoch: 14200, acc: 0.903, loss: 0.235, lr: 0.41324021653787346\n",
      "epoch: 14300, acc: 0.897, loss: 0.200, lr: 0.4115395695296103\n",
      "epoch: 14400, acc: 0.933, loss: 0.124, lr: 0.4098528628222468\n",
      "epoch: 14500, acc: 0.920, loss: 0.130, lr: 0.4081799257112535\n",
      "epoch: 14600, acc: 0.923, loss: 0.131, lr: 0.40652059026789705\n",
      "epoch: 14700, acc: 0.923, loss: 0.128, lr: 0.40487469128304787\n",
      "epoch: 14800, acc: 0.927, loss: 0.125, lr: 0.4032420662123473\n",
      "epoch: 14900, acc: 0.927, loss: 0.125, lr: 0.4016225551226957\n",
      "epoch: 15000, acc: 0.923, loss: 0.128, lr: 0.40001600064002557\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, and 64 output values\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# ReLU activation for dense2\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# third dense layer with 64 input features, and 3 output values\n",
    "dense3 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_SGD(decay = 1e-4)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(15001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # forward pass through activation\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # forward pass through third dense layer\n",
    "    dense3.forward(activation2.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense3.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234ffabf",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent With Momentum\n",
    "Our model can get stuck in a local minimum, bouncing back and forth, so the momentum addscauses the previous gradient update to influence the next.\n",
    "\n",
    "We set a parameter between 0 and 1, representing the fraction of the previous paremeter update to retain. A portion of the previous update, combined with the portion of the current gradient, together will be sed to update the parameters.\n",
    "\n",
    "The momentum fraction setting too high can cause the model to stop learning, as the direction of the updates won't be able to follow the global gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60351ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1.0, decay = 0., momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.momentum:\n",
    "            \n",
    "            #if layer does not contain momentum arrays\n",
    "            # create them and fill with zeroes\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                \n",
    "                # if there is no momentum array for weights\n",
    "                # the array doesn't exist for the biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "            # build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        # otherwise vanillae SGD\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "        \n",
    "        # update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83b936fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.293, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.453, loss: 1.037, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.473, loss: 0.932, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.583, loss: 0.776, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.763, loss: 0.572, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.790, loss: 0.504, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.810, loss: 0.470, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.823, loss: 0.443, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.820, loss: 0.423, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.843, loss: 0.396, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.857, loss: 0.373, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.867, loss: 0.357, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.870, loss: 0.346, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.870, loss: 0.339, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.867, loss: 0.331, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.877, loss: 0.317, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.870, loss: 0.309, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.870, loss: 0.304, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.877, loss: 0.297, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.873, loss: 0.288, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.880, loss: 0.283, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.897, loss: 0.279, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.890, loss: 0.277, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.893, loss: 0.274, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.897, loss: 0.273, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.893, loss: 0.271, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.893, loss: 0.270, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.887, loss: 0.268, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.887, loss: 0.267, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.887, loss: 0.266, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.887, loss: 0.265, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.887, loss: 0.265, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.890, loss: 0.263, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.883, loss: 0.261, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.883, loss: 0.260, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.883, loss: 0.259, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.883, loss: 0.258, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.887, loss: 0.258, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.887, loss: 0.257, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.887, loss: 0.256, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.887, loss: 0.256, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.887, loss: 0.255, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.887, loss: 0.255, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.887, loss: 0.254, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.890, loss: 0.254, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.887, loss: 0.254, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.893, loss: 0.253, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.887, loss: 0.253, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.890, loss: 0.253, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.890, loss: 0.252, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.890, loss: 0.252, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.890, loss: 0.252, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.890, loss: 0.251, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.890, loss: 0.251, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.890, loss: 0.251, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.890, loss: 0.251, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.890, loss: 0.250, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.890, loss: 0.250, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.890, loss: 0.250, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.890, loss: 0.250, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.890, loss: 0.250, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.890, loss: 0.249, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.890, loss: 0.249, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.890, loss: 0.249, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.890, loss: 0.249, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.890, loss: 0.248, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.890, loss: 0.248, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.890, loss: 0.248, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.887, loss: 0.248, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.890, loss: 0.248, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.887, loss: 0.248, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.887, loss: 0.247, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.887, loss: 0.247, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.887, loss: 0.247, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.887, loss: 0.247, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.887, loss: 0.247, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.887, loss: 0.247, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.887, loss: 0.247, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.887, loss: 0.247, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.887, loss: 0.247, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.887, loss: 0.246, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.890, loss: 0.246, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.890, loss: 0.246, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.887, loss: 0.246, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.890, loss: 0.246, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.887, loss: 0.246, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.887, loss: 0.246, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.890, loss: 0.246, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.887, loss: 0.246, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.887, loss: 0.246, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.890, loss: 0.246, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.890, loss: 0.246, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.890, loss: 0.245, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.890, loss: 0.245, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.890, loss: 0.245, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.890, loss: 0.245, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.890, loss: 0.245, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.890, loss: 0.245, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.890, loss: 0.245, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.890, loss: 0.245, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.890, loss: 0.245, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_SGD(decay = 1e-3, momentum = 0.9)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca9e86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1.0, decay = 0., momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.momentum:\n",
    "            \n",
    "            #if layer does not contain momentum arrays\n",
    "            # create them and fill with zeroes\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                \n",
    "                # if there is no momentum array for weights\n",
    "                # the array doesn't exist for the biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "            # build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        # otherwise vanillae SGD\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "        \n",
    "        # update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "48b3f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.323, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.373, loss: 1.099, lr: 0.9901970492127933\n",
      "epoch: 200, acc: 0.377, loss: 1.098, lr: 0.9804882831650161\n",
      "epoch: 300, acc: 0.377, loss: 1.083, lr: 0.9709680551509855\n",
      "epoch: 400, acc: 0.363, loss: 1.081, lr: 0.9616309260505818\n",
      "epoch: 500, acc: 0.380, loss: 1.080, lr: 0.9524716639679969\n",
      "epoch: 600, acc: 0.363, loss: 1.077, lr: 0.9434852344560807\n",
      "epoch: 700, acc: 0.377, loss: 1.068, lr: 0.9346667912889054\n",
      "epoch: 800, acc: 0.417, loss: 1.051, lr: 0.9260116677470135\n",
      "epoch: 900, acc: 0.557, loss: 0.964, lr: 0.9175153683824203\n",
      "epoch: 1000, acc: 0.447, loss: 1.063, lr: 0.9091735612328392\n",
      "epoch: 1100, acc: 0.700, loss: 0.729, lr: 0.9009820704567978\n",
      "epoch: 1200, acc: 0.720, loss: 0.677, lr: 0.892936869363336\n",
      "epoch: 1300, acc: 0.657, loss: 0.754, lr: 0.8850340738118416\n",
      "epoch: 1400, acc: 0.783, loss: 0.544, lr: 0.8772699359592947\n",
      "epoch: 1500, acc: 0.707, loss: 0.698, lr: 0.8696408383337683\n",
      "epoch: 1600, acc: 0.790, loss: 0.504, lr: 0.8621432882145013\n",
      "epoch: 1700, acc: 0.900, loss: 0.303, lr: 0.8547739123001966\n",
      "epoch: 1800, acc: 0.927, loss: 0.215, lr: 0.8475294516484448\n",
      "epoch: 1900, acc: 0.807, loss: 0.529, lr: 0.8404067568703253\n",
      "epoch: 2000, acc: 0.960, loss: 0.132, lr: 0.8334027835652972\n",
      "epoch: 2100, acc: 0.933, loss: 0.175, lr: 0.8265145879824779\n",
      "epoch: 2200, acc: 0.957, loss: 0.119, lr: 0.8197393228953193\n",
      "epoch: 2300, acc: 0.957, loss: 0.102, lr: 0.8130742336775347\n",
      "epoch: 2400, acc: 0.960, loss: 0.102, lr: 0.8065166545689169\n",
      "epoch: 2500, acc: 0.867, loss: 0.384, lr: 0.8000640051204096\n",
      "epoch: 2600, acc: 0.957, loss: 0.101, lr: 0.7937137868084768\n",
      "epoch: 2700, acc: 0.943, loss: 0.159, lr: 0.7874635798094338\n",
      "epoch: 2800, acc: 0.957, loss: 0.124, lr: 0.7813110399249941\n",
      "epoch: 2900, acc: 0.967, loss: 0.089, lr: 0.7752538956508256\n",
      "epoch: 3000, acc: 0.970, loss: 0.078, lr: 0.7692899453804138\n",
      "epoch: 3100, acc: 0.823, loss: 0.845, lr: 0.7634170547370028\n",
      "epoch: 3200, acc: 0.967, loss: 0.096, lr: 0.7576331540268202\n",
      "epoch: 3300, acc: 0.950, loss: 0.113, lr: 0.7519362358072035\n",
      "epoch: 3400, acc: 0.973, loss: 0.094, lr: 0.7463243525636241\n",
      "epoch: 3500, acc: 0.980, loss: 0.067, lr: 0.7407956144899621\n",
      "epoch: 3600, acc: 0.973, loss: 0.079, lr: 0.735348187366718\n",
      "epoch: 3700, acc: 0.983, loss: 0.054, lr: 0.7299802905321557\n",
      "epoch: 3800, acc: 0.923, loss: 0.186, lr: 0.7246901949416624\n",
      "epoch: 3900, acc: 0.957, loss: 0.112, lr: 0.7194762213108857\n",
      "epoch: 4000, acc: 0.803, loss: 1.078, lr: 0.7143367383384527\n",
      "epoch: 4100, acc: 0.973, loss: 0.082, lr: 0.7092701610043266\n",
      "epoch: 4200, acc: 0.977, loss: 0.076, lr: 0.7042749489400663\n",
      "epoch: 4300, acc: 0.980, loss: 0.071, lr: 0.6993496048674733\n",
      "epoch: 4400, acc: 0.983, loss: 0.063, lr: 0.6944926731022988\n",
      "epoch: 4500, acc: 0.960, loss: 0.089, lr: 0.6897027381198704\n",
      "epoch: 4600, acc: 0.977, loss: 0.078, lr: 0.6849784231796698\n",
      "epoch: 4700, acc: 0.980, loss: 0.069, lr: 0.6803183890060548\n",
      "epoch: 4800, acc: 0.977, loss: 0.061, lr: 0.6757213325224677\n",
      "epoch: 4900, acc: 0.983, loss: 0.048, lr: 0.6711859856366199\n",
      "epoch: 5000, acc: 0.920, loss: 0.206, lr: 0.6667111140742716\n",
      "epoch: 5100, acc: 0.983, loss: 0.064, lr: 0.6622955162593549\n",
      "epoch: 5200, acc: 0.987, loss: 0.055, lr: 0.6579380222383051\n",
      "epoch: 5300, acc: 0.983, loss: 0.064, lr: 0.6536374926465782\n",
      "epoch: 5400, acc: 0.987, loss: 0.051, lr: 0.649392817715436\n",
      "epoch: 5500, acc: 0.923, loss: 0.195, lr: 0.6452029163171817\n",
      "epoch: 5600, acc: 0.983, loss: 0.053, lr: 0.6410667350471184\n",
      "epoch: 5700, acc: 0.990, loss: 0.045, lr: 0.6369832473405949\n",
      "epoch: 5800, acc: 0.990, loss: 0.042, lr: 0.6329514526235838\n",
      "epoch: 5900, acc: 0.977, loss: 0.066, lr: 0.6289703754953141\n",
      "epoch: 6000, acc: 0.963, loss: 0.103, lr: 0.6250390649415589\n",
      "epoch: 6100, acc: 0.983, loss: 0.039, lr: 0.6211565935772407\n",
      "epoch: 6200, acc: 0.970, loss: 0.088, lr: 0.6173220569170937\n",
      "epoch: 6300, acc: 0.983, loss: 0.041, lr: 0.6135345726731701\n",
      "epoch: 6400, acc: 0.990, loss: 0.032, lr: 0.6097932800780536\n",
      "epoch: 6500, acc: 0.990, loss: 0.038, lr: 0.6060973392326807\n",
      "epoch: 6600, acc: 0.987, loss: 0.040, lr: 0.6024459304777396\n",
      "epoch: 6700, acc: 0.890, loss: 0.269, lr: 0.5988382537876519\n",
      "epoch: 6800, acc: 0.977, loss: 0.068, lr: 0.5952735281862016\n",
      "epoch: 6900, acc: 0.970, loss: 0.062, lr: 0.5917509911829102\n",
      "epoch: 7000, acc: 0.970, loss: 0.061, lr: 0.5882698982293076\n",
      "epoch: 7100, acc: 0.990, loss: 0.035, lr: 0.5848295221942803\n",
      "epoch: 7200, acc: 0.990, loss: 0.037, lr: 0.5814291528577243\n",
      "epoch: 7300, acc: 0.993, loss: 0.040, lr: 0.5780680964217585\n",
      "epoch: 7400, acc: 0.993, loss: 0.036, lr: 0.5747456750387954\n",
      "epoch: 7500, acc: 0.983, loss: 0.044, lr: 0.5714612263557918\n",
      "epoch: 7600, acc: 0.993, loss: 0.030, lr: 0.5682141030740383\n",
      "epoch: 7700, acc: 0.993, loss: 0.028, lr: 0.5650036725238714\n",
      "epoch: 7800, acc: 0.990, loss: 0.025, lr: 0.5618293162537221\n",
      "epoch: 7900, acc: 0.993, loss: 0.025, lr: 0.5586904296329404\n",
      "epoch: 8000, acc: 0.993, loss: 0.023, lr: 0.5555864214678593\n",
      "epoch: 8100, acc: 0.993, loss: 0.031, lr: 0.5525167136305873\n",
      "epoch: 8200, acc: 0.990, loss: 0.023, lr: 0.5494807407000385\n",
      "epoch: 8300, acc: 0.990, loss: 0.033, lr: 0.5464779496147331\n",
      "epoch: 8400, acc: 0.993, loss: 0.026, lr: 0.5435077993369205\n",
      "epoch: 8500, acc: 0.993, loss: 0.025, lr: 0.5405697605275961\n",
      "epoch: 8600, acc: 0.993, loss: 0.025, lr: 0.5376633152320017\n",
      "epoch: 8700, acc: 0.990, loss: 0.026, lr: 0.5347879565752179\n",
      "epoch: 8800, acc: 0.993, loss: 0.020, lr: 0.5319431884674717\n",
      "epoch: 8900, acc: 0.990, loss: 0.023, lr: 0.5291285253188\n",
      "epoch: 9000, acc: 0.993, loss: 0.021, lr: 0.5263434917627243\n",
      "epoch: 9100, acc: 0.993, loss: 0.020, lr: 0.5235876223886068\n",
      "epoch: 9200, acc: 0.993, loss: 0.020, lr: 0.5208604614823689\n",
      "epoch: 9300, acc: 0.993, loss: 0.019, lr: 0.5181615627752734\n",
      "epoch: 9400, acc: 0.993, loss: 0.018, lr: 0.5154904892004742\n",
      "epoch: 9500, acc: 0.993, loss: 0.018, lr: 0.5128468126570593\n",
      "epoch: 9600, acc: 0.993, loss: 0.019, lr: 0.5102301137813153\n",
      "epoch: 9700, acc: 0.993, loss: 0.021, lr: 0.5076399817249606\n",
      "epoch: 9800, acc: 0.993, loss: 0.019, lr: 0.5050760139400979\n",
      "epoch: 9900, acc: 0.987, loss: 0.024, lr: 0.5025378159706518\n",
      "epoch: 10000, acc: 0.993, loss: 0.019, lr: 0.5000250012500626\n",
      "epoch: 10100, acc: 0.993, loss: 0.023, lr: 0.49753719090502013\n",
      "epoch: 10200, acc: 0.993, loss: 0.019, lr: 0.495074013565028\n",
      "epoch: 10300, acc: 0.993, loss: 0.017, lr: 0.49263510517759496\n",
      "epoch: 10400, acc: 0.993, loss: 0.018, lr: 0.4902201088288641\n",
      "epoch: 10500, acc: 0.990, loss: 0.021, lr: 0.4878286745694912\n",
      "epoch: 10600, acc: 0.990, loss: 0.019, lr: 0.4854604592455945\n",
      "epoch: 10700, acc: 0.993, loss: 0.019, lr: 0.4831151263346055\n",
      "epoch: 10800, acc: 0.993, loss: 0.018, lr: 0.480792345785855\n",
      "epoch: 10900, acc: 0.990, loss: 0.019, lr: 0.4784917938657352\n",
      "epoch: 11000, acc: 0.993, loss: 0.018, lr: 0.4762131530072861\n",
      "epoch: 11100, acc: 0.993, loss: 0.017, lr: 0.4739561116640599\n",
      "epoch: 11200, acc: 0.993, loss: 0.017, lr: 0.4717203641681211\n",
      "epoch: 11300, acc: 0.993, loss: 0.017, lr: 0.46950561059204654\n",
      "epoch: 11400, acc: 0.993, loss: 0.017, lr: 0.4673115566147951\n",
      "epoch: 11500, acc: 0.993, loss: 0.017, lr: 0.4651379133913205\n",
      "epoch: 11600, acc: 0.993, loss: 0.017, lr: 0.46298439742580666\n",
      "epoch: 11700, acc: 0.993, loss: 0.017, lr: 0.46085073044840774\n",
      "epoch: 11800, acc: 0.993, loss: 0.018, lr: 0.45873663929538055\n",
      "epoch: 11900, acc: 0.990, loss: 0.018, lr: 0.456641855792502\n",
      "epoch: 12000, acc: 0.993, loss: 0.017, lr: 0.4545661166416655\n",
      "epoch: 12100, acc: 0.993, loss: 0.018, lr: 0.452509163310557\n",
      "epoch: 12200, acc: 0.993, loss: 0.016, lr: 0.45047074192531195\n",
      "epoch: 12300, acc: 0.993, loss: 0.017, lr: 0.4484506031660613\n",
      "epoch: 12400, acc: 0.993, loss: 0.017, lr: 0.44644850216527526\n",
      "epoch: 12500, acc: 0.993, loss: 0.016, lr: 0.4444641984088181\n",
      "epoch: 12600, acc: 0.993, loss: 0.016, lr: 0.44249745563963006\n",
      "epoch: 12700, acc: 0.993, loss: 0.017, lr: 0.4405480417639544\n",
      "epoch: 12800, acc: 0.993, loss: 0.016, lr: 0.43861572876003335\n",
      "epoch: 12900, acc: 0.993, loss: 0.017, lr: 0.43670029258919596\n",
      "epoch: 13000, acc: 0.993, loss: 0.015, lr: 0.4348015131092656\n",
      "epoch: 13100, acc: 0.993, loss: 0.015, lr: 0.43291917399021607\n",
      "epoch: 13200, acc: 0.993, loss: 0.015, lr: 0.43105306263201\n",
      "epoch: 13300, acc: 0.993, loss: 0.016, lr: 0.42920297008455294\n",
      "epoch: 13400, acc: 0.993, loss: 0.015, lr: 0.42736869096969954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13500, acc: 0.993, loss: 0.016, lr: 0.42555002340525133\n",
      "epoch: 13600, acc: 0.993, loss: 0.015, lr: 0.42374676893088686\n",
      "epoch: 13700, acc: 0.993, loss: 0.015, lr: 0.4219587324359677\n",
      "epoch: 13800, acc: 0.993, loss: 0.016, lr: 0.4201857220891634\n",
      "epoch: 13900, acc: 0.993, loss: 0.014, lr: 0.41842754926984393\n",
      "epoch: 14000, acc: 0.993, loss: 0.015, lr: 0.4166840285011875\n",
      "epoch: 14100, acc: 0.993, loss: 0.014, lr: 0.4149549773849537\n",
      "epoch: 14200, acc: 0.993, loss: 0.014, lr: 0.41324021653787346\n",
      "epoch: 14300, acc: 0.993, loss: 0.015, lr: 0.4115395695296103\n",
      "epoch: 14400, acc: 0.993, loss: 0.015, lr: 0.4098528628222468\n",
      "epoch: 14500, acc: 0.993, loss: 0.014, lr: 0.4081799257112535\n",
      "epoch: 14600, acc: 0.993, loss: 0.014, lr: 0.40652059026789705\n",
      "epoch: 14700, acc: 0.993, loss: 0.014, lr: 0.40487469128304787\n",
      "epoch: 14800, acc: 0.993, loss: 0.014, lr: 0.4032420662123473\n",
      "epoch: 14900, acc: 0.993, loss: 0.014, lr: 0.4016225551226957\n",
      "epoch: 15000, acc: 0.993, loss: 0.014, lr: 0.40001600064002557\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, and 64 output values\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# ReLU activation for dense2\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# third dense layer with 64 input features, and 3 output values\n",
    "dense3 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_SGD(decay = 1e-4, momentum = 0.5)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(15001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # forward pass through activation\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    # forward pass through third dense layer\n",
    "    dense3.forward(activation2.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense3.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense3.backward(loss_activation.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700659f8",
   "metadata": {},
   "source": [
    "This is clearly the best so far. The SGD optimizer with momentum is usually one of the 2 main choices for an optimizer in practice next to the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0c540",
   "metadata": {},
   "source": [
    "### AdaGrad (Adaptive Gradient)\n",
    "Idea is to normalize the updates being made to the features. During training, some weights can change dramatically, while others don't change much, and generally it is better for weights not to change too much, compared to others.\n",
    "\n",
    "We keep track of the sum of the squares of the updates, and keep the learning rate for that parameter inversely proportional to it, as shown here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba54c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache += parm_gradient ** 2\n",
    "parm_updates = learning_rate * parm_gradient / (sqrt(cache) + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef51741",
   "metadata": {},
   "source": [
    "The denominator ever growing larger can cause the learning rate to come to a virtual halt, making this optimizer used in niche applications.\n",
    "\n",
    "The eps or **epsilon** here is a pre-training hyperparamter to prevent division by 0; usually a super small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2866090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1., decay = 0., epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7c911cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.313, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.453, loss: 1.257, lr: 0.9901970492127933\n",
      "epoch: 200, acc: 0.597, loss: 0.988, lr: 0.9804882831650161\n",
      "epoch: 300, acc: 0.657, loss: 0.901, lr: 0.9709680551509855\n",
      "epoch: 400, acc: 0.657, loss: 0.851, lr: 0.9616309260505818\n",
      "epoch: 500, acc: 0.713, loss: 0.790, lr: 0.9524716639679969\n",
      "epoch: 600, acc: 0.737, loss: 0.753, lr: 0.9434852344560807\n",
      "epoch: 700, acc: 0.743, loss: 0.724, lr: 0.9346667912889054\n",
      "epoch: 800, acc: 0.753, loss: 0.700, lr: 0.9260116677470135\n",
      "epoch: 900, acc: 0.763, loss: 0.680, lr: 0.9175153683824203\n",
      "epoch: 1000, acc: 0.773, loss: 0.663, lr: 0.9091735612328392\n",
      "epoch: 1100, acc: 0.783, loss: 0.647, lr: 0.9009820704567978\n",
      "epoch: 1200, acc: 0.793, loss: 0.633, lr: 0.892936869363336\n",
      "epoch: 1300, acc: 0.797, loss: 0.620, lr: 0.8850340738118416\n",
      "epoch: 1400, acc: 0.797, loss: 0.609, lr: 0.8772699359592947\n",
      "epoch: 1500, acc: 0.797, loss: 0.599, lr: 0.8696408383337683\n",
      "epoch: 1600, acc: 0.800, loss: 0.590, lr: 0.8621432882145013\n",
      "epoch: 1700, acc: 0.810, loss: 0.581, lr: 0.8547739123001966\n",
      "epoch: 1800, acc: 0.813, loss: 0.574, lr: 0.8475294516484448\n",
      "epoch: 1900, acc: 0.810, loss: 0.568, lr: 0.8404067568703253\n",
      "epoch: 2000, acc: 0.803, loss: 0.561, lr: 0.8334027835652972\n",
      "epoch: 2100, acc: 0.803, loss: 0.555, lr: 0.8265145879824779\n",
      "epoch: 2200, acc: 0.803, loss: 0.550, lr: 0.8197393228953193\n",
      "epoch: 2300, acc: 0.803, loss: 0.545, lr: 0.8130742336775347\n",
      "epoch: 2400, acc: 0.807, loss: 0.541, lr: 0.8065166545689169\n",
      "epoch: 2500, acc: 0.807, loss: 0.536, lr: 0.8000640051204096\n",
      "epoch: 2600, acc: 0.803, loss: 0.532, lr: 0.7937137868084768\n",
      "epoch: 2700, acc: 0.807, loss: 0.527, lr: 0.7874635798094338\n",
      "epoch: 2800, acc: 0.807, loss: 0.523, lr: 0.7813110399249941\n",
      "epoch: 2900, acc: 0.813, loss: 0.519, lr: 0.7752538956508256\n",
      "epoch: 3000, acc: 0.813, loss: 0.515, lr: 0.7692899453804138\n",
      "epoch: 3100, acc: 0.823, loss: 0.511, lr: 0.7634170547370028\n",
      "epoch: 3200, acc: 0.827, loss: 0.507, lr: 0.7576331540268202\n",
      "epoch: 3300, acc: 0.827, loss: 0.503, lr: 0.7519362358072035\n",
      "epoch: 3400, acc: 0.827, loss: 0.500, lr: 0.7463243525636241\n",
      "epoch: 3500, acc: 0.827, loss: 0.497, lr: 0.7407956144899621\n",
      "epoch: 3600, acc: 0.833, loss: 0.493, lr: 0.735348187366718\n",
      "epoch: 3700, acc: 0.830, loss: 0.490, lr: 0.7299802905321557\n",
      "epoch: 3800, acc: 0.833, loss: 0.487, lr: 0.7246901949416624\n",
      "epoch: 3900, acc: 0.833, loss: 0.484, lr: 0.7194762213108857\n",
      "epoch: 4000, acc: 0.837, loss: 0.481, lr: 0.7143367383384527\n",
      "epoch: 4100, acc: 0.837, loss: 0.478, lr: 0.7092701610043266\n",
      "epoch: 4200, acc: 0.837, loss: 0.475, lr: 0.7042749489400663\n",
      "epoch: 4300, acc: 0.837, loss: 0.472, lr: 0.6993496048674733\n",
      "epoch: 4400, acc: 0.833, loss: 0.470, lr: 0.6944926731022988\n",
      "epoch: 4500, acc: 0.837, loss: 0.467, lr: 0.6897027381198704\n",
      "epoch: 4600, acc: 0.837, loss: 0.464, lr: 0.6849784231796698\n",
      "epoch: 4700, acc: 0.840, loss: 0.462, lr: 0.6803183890060548\n",
      "epoch: 4800, acc: 0.837, loss: 0.459, lr: 0.6757213325224677\n",
      "epoch: 4900, acc: 0.840, loss: 0.457, lr: 0.6711859856366199\n",
      "epoch: 5000, acc: 0.840, loss: 0.454, lr: 0.6667111140742716\n",
      "epoch: 5100, acc: 0.850, loss: 0.452, lr: 0.6622955162593549\n",
      "epoch: 5200, acc: 0.847, loss: 0.450, lr: 0.6579380222383051\n",
      "epoch: 5300, acc: 0.847, loss: 0.447, lr: 0.6536374926465782\n",
      "epoch: 5400, acc: 0.850, loss: 0.445, lr: 0.649392817715436\n",
      "epoch: 5500, acc: 0.850, loss: 0.443, lr: 0.6452029163171817\n",
      "epoch: 5600, acc: 0.843, loss: 0.440, lr: 0.6410667350471184\n",
      "epoch: 5700, acc: 0.843, loss: 0.438, lr: 0.6369832473405949\n",
      "epoch: 5800, acc: 0.850, loss: 0.435, lr: 0.6329514526235838\n",
      "epoch: 5900, acc: 0.850, loss: 0.433, lr: 0.6289703754953141\n",
      "epoch: 6000, acc: 0.847, loss: 0.431, lr: 0.6250390649415589\n",
      "epoch: 6100, acc: 0.847, loss: 0.428, lr: 0.6211565935772407\n",
      "epoch: 6200, acc: 0.847, loss: 0.426, lr: 0.6173220569170937\n",
      "epoch: 6300, acc: 0.857, loss: 0.424, lr: 0.6135345726731701\n",
      "epoch: 6400, acc: 0.857, loss: 0.422, lr: 0.6097932800780536\n",
      "epoch: 6500, acc: 0.857, loss: 0.421, lr: 0.6060973392326807\n",
      "epoch: 6600, acc: 0.850, loss: 0.419, lr: 0.6024459304777396\n",
      "epoch: 6700, acc: 0.850, loss: 0.416, lr: 0.5988382537876519\n",
      "epoch: 6800, acc: 0.853, loss: 0.414, lr: 0.5952735281862016\n",
      "epoch: 6900, acc: 0.857, loss: 0.413, lr: 0.5917509911829102\n",
      "epoch: 7000, acc: 0.853, loss: 0.411, lr: 0.5882698982293076\n",
      "epoch: 7100, acc: 0.857, loss: 0.409, lr: 0.5848295221942803\n",
      "epoch: 7200, acc: 0.857, loss: 0.408, lr: 0.5814291528577243\n",
      "epoch: 7300, acc: 0.857, loss: 0.406, lr: 0.5780680964217585\n",
      "epoch: 7400, acc: 0.857, loss: 0.405, lr: 0.5747456750387954\n",
      "epoch: 7500, acc: 0.857, loss: 0.403, lr: 0.5714612263557918\n",
      "epoch: 7600, acc: 0.857, loss: 0.402, lr: 0.5682141030740383\n",
      "epoch: 7700, acc: 0.857, loss: 0.400, lr: 0.5650036725238714\n",
      "epoch: 7800, acc: 0.860, loss: 0.399, lr: 0.5618293162537221\n",
      "epoch: 7900, acc: 0.860, loss: 0.397, lr: 0.5586904296329404\n",
      "epoch: 8000, acc: 0.860, loss: 0.396, lr: 0.5555864214678593\n",
      "epoch: 8100, acc: 0.860, loss: 0.395, lr: 0.5525167136305873\n",
      "epoch: 8200, acc: 0.860, loss: 0.394, lr: 0.5494807407000385\n",
      "epoch: 8300, acc: 0.857, loss: 0.392, lr: 0.5464779496147331\n",
      "epoch: 8400, acc: 0.857, loss: 0.391, lr: 0.5435077993369205\n",
      "epoch: 8500, acc: 0.857, loss: 0.390, lr: 0.5405697605275961\n",
      "epoch: 8600, acc: 0.853, loss: 0.389, lr: 0.5376633152320017\n",
      "epoch: 8700, acc: 0.857, loss: 0.388, lr: 0.5347879565752179\n",
      "epoch: 8800, acc: 0.847, loss: 0.387, lr: 0.5319431884674717\n",
      "epoch: 8900, acc: 0.853, loss: 0.385, lr: 0.5291285253188\n",
      "epoch: 9000, acc: 0.847, loss: 0.384, lr: 0.5263434917627243\n",
      "epoch: 9100, acc: 0.847, loss: 0.383, lr: 0.5235876223886068\n",
      "epoch: 9200, acc: 0.850, loss: 0.382, lr: 0.5208604614823689\n",
      "epoch: 9300, acc: 0.850, loss: 0.381, lr: 0.5181615627752734\n",
      "epoch: 9400, acc: 0.850, loss: 0.380, lr: 0.5154904892004742\n",
      "epoch: 9500, acc: 0.850, loss: 0.379, lr: 0.5128468126570593\n",
      "epoch: 9600, acc: 0.850, loss: 0.377, lr: 0.5102301137813153\n",
      "epoch: 9700, acc: 0.850, loss: 0.377, lr: 0.5076399817249606\n",
      "epoch: 9800, acc: 0.853, loss: 0.375, lr: 0.5050760139400979\n",
      "epoch: 9900, acc: 0.853, loss: 0.374, lr: 0.5025378159706518\n",
      "epoch: 10000, acc: 0.853, loss: 0.373, lr: 0.5000250012500626\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adagrad(decay = 1e-4)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4fd94",
   "metadata": {},
   "source": [
    "### RMSProp (Root Mean Square Propagation)\n",
    "Calclates an adaptive learning rate per parameter, like AdaGrad. The cache here is calculated as:\n",
    "\n",
    "    cache += rho * cache + (1 - rho) * gradient ** 2\n",
    "\n",
    "Basically adds a momentum like director for the updates, but also adds per-parameter adaptive learning rate, so the learning rate changes are smoother. Uses a moving average to keep track.\n",
    "\n",
    "The new parameter, rho is the cache memory decay rate. Because so much of the previous update is carried over, a learning rate of 1 is far too large and causes instability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "193255b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                    (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                    (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "96ad25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.350, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.407, loss: 1.445, lr: 0.019803940984255867\n",
      "epoch: 200, acc: 0.423, loss: 1.154, lr: 0.019609765663300322\n",
      "epoch: 300, acc: 0.450, loss: 1.047, lr: 0.01941936110301971\n",
      "epoch: 400, acc: 0.487, loss: 0.996, lr: 0.019232618521011637\n",
      "epoch: 500, acc: 0.500, loss: 0.963, lr: 0.01904943327935994\n",
      "epoch: 600, acc: 0.510, loss: 0.941, lr: 0.018869704689121615\n",
      "epoch: 700, acc: 0.527, loss: 0.923, lr: 0.018693335825778108\n",
      "epoch: 800, acc: 0.537, loss: 0.910, lr: 0.01852023335494027\n",
      "epoch: 900, acc: 0.530, loss: 0.894, lr: 0.018350307367648408\n",
      "epoch: 1000, acc: 0.533, loss: 0.885, lr: 0.018183471224656786\n",
      "epoch: 1100, acc: 0.543, loss: 0.873, lr: 0.018019641409135957\n",
      "epoch: 1200, acc: 0.547, loss: 0.862, lr: 0.01785873738726672\n",
      "epoch: 1300, acc: 0.550, loss: 0.851, lr: 0.017700681476236834\n",
      "epoch: 1400, acc: 0.553, loss: 0.843, lr: 0.017545398719185895\n",
      "epoch: 1500, acc: 0.557, loss: 0.835, lr: 0.017392816766675364\n",
      "epoch: 1600, acc: 0.560, loss: 0.825, lr: 0.017242865764290027\n",
      "epoch: 1700, acc: 0.563, loss: 0.816, lr: 0.017095478246003933\n",
      "epoch: 1800, acc: 0.557, loss: 0.806, lr: 0.016950589032968897\n",
      "epoch: 1900, acc: 0.570, loss: 0.797, lr: 0.016808135137406505\n",
      "epoch: 2000, acc: 0.587, loss: 0.789, lr: 0.016668055671305942\n",
      "epoch: 2100, acc: 0.590, loss: 0.780, lr: 0.016530291759649558\n",
      "epoch: 2200, acc: 0.597, loss: 0.774, lr: 0.016394786457906384\n",
      "epoch: 2300, acc: 0.617, loss: 0.762, lr: 0.016261484673550694\n",
      "epoch: 2400, acc: 0.627, loss: 0.751, lr: 0.01613033309137834\n",
      "epoch: 2500, acc: 0.640, loss: 0.741, lr: 0.016001280102408193\n",
      "epoch: 2600, acc: 0.640, loss: 0.733, lr: 0.015874275736169535\n",
      "epoch: 2700, acc: 0.643, loss: 0.726, lr: 0.015749271596188677\n",
      "epoch: 2800, acc: 0.643, loss: 0.719, lr: 0.01562622079849988\n",
      "epoch: 2900, acc: 0.647, loss: 0.711, lr: 0.015505077913016512\n",
      "epoch: 3000, acc: 0.660, loss: 0.703, lr: 0.015385798907608278\n",
      "epoch: 3100, acc: 0.663, loss: 0.695, lr: 0.015268341094740057\n",
      "epoch: 3200, acc: 0.683, loss: 0.687, lr: 0.015152663080536404\n",
      "epoch: 3300, acc: 0.683, loss: 0.680, lr: 0.01503872471614407\n",
      "epoch: 3400, acc: 0.693, loss: 0.673, lr: 0.014926487051272481\n",
      "epoch: 3500, acc: 0.687, loss: 0.667, lr: 0.014815912289799242\n",
      "epoch: 3600, acc: 0.690, loss: 0.660, lr: 0.014706963747334361\n",
      "epoch: 3700, acc: 0.707, loss: 0.652, lr: 0.014599605810643115\n",
      "epoch: 3800, acc: 0.707, loss: 0.647, lr: 0.014493803898833249\n",
      "epoch: 3900, acc: 0.717, loss: 0.641, lr: 0.014389524426217715\n",
      "epoch: 4000, acc: 0.727, loss: 0.633, lr: 0.014286734766769053\n",
      "epoch: 4100, acc: 0.730, loss: 0.628, lr: 0.014185403220086532\n",
      "epoch: 4200, acc: 0.727, loss: 0.625, lr: 0.014085498978801325\n",
      "epoch: 4300, acc: 0.727, loss: 0.620, lr: 0.013986992097349467\n",
      "epoch: 4400, acc: 0.730, loss: 0.615, lr: 0.013889853462045977\n",
      "epoch: 4500, acc: 0.733, loss: 0.610, lr: 0.013794054762397407\n",
      "epoch: 4600, acc: 0.743, loss: 0.604, lr: 0.013699568463593397\n",
      "epoch: 4700, acc: 0.743, loss: 0.601, lr: 0.013606367780121096\n",
      "epoch: 4800, acc: 0.747, loss: 0.597, lr: 0.013514426650449354\n",
      "epoch: 4900, acc: 0.743, loss: 0.595, lr: 0.013423719712732398\n",
      "epoch: 5000, acc: 0.737, loss: 0.588, lr: 0.013334222281485434\n",
      "epoch: 5100, acc: 0.737, loss: 0.582, lr: 0.013245910325187099\n",
      "epoch: 5200, acc: 0.747, loss: 0.579, lr: 0.013158760444766103\n",
      "epoch: 5300, acc: 0.750, loss: 0.575, lr: 0.013072749852931565\n",
      "epoch: 5400, acc: 0.743, loss: 0.577, lr: 0.01298785635430872\n",
      "epoch: 5500, acc: 0.743, loss: 0.564, lr: 0.012904058326343636\n",
      "epoch: 5600, acc: 0.743, loss: 0.563, lr: 0.012821334700942369\n",
      "epoch: 5700, acc: 0.740, loss: 0.561, lr: 0.012739664946811897\n",
      "epoch: 5800, acc: 0.760, loss: 0.547, lr: 0.012659029052471675\n",
      "epoch: 5900, acc: 0.740, loss: 0.553, lr: 0.012579407509906283\n",
      "epoch: 6000, acc: 0.743, loss: 0.549, lr: 0.01250078129883118\n",
      "epoch: 6100, acc: 0.740, loss: 0.547, lr: 0.012423131871544814\n",
      "epoch: 6200, acc: 0.737, loss: 0.551, lr: 0.012346441138341876\n",
      "epoch: 6300, acc: 0.750, loss: 0.538, lr: 0.012270691453463402\n",
      "epoch: 6400, acc: 0.767, loss: 0.538, lr: 0.012195865601561072\n",
      "epoch: 6500, acc: 0.757, loss: 0.529, lr: 0.012121946784653614\n",
      "epoch: 6600, acc: 0.760, loss: 0.527, lr: 0.012048918609554793\n",
      "epoch: 6700, acc: 0.763, loss: 0.524, lr: 0.011976765075753038\n",
      "epoch: 6800, acc: 0.770, loss: 0.520, lr: 0.011905470563724032\n",
      "epoch: 6900, acc: 0.763, loss: 0.517, lr: 0.011835019823658205\n",
      "epoch: 7000, acc: 0.753, loss: 0.519, lr: 0.011765397964586153\n",
      "epoch: 7100, acc: 0.777, loss: 0.508, lr: 0.011696590443885607\n",
      "epoch: 7200, acc: 0.763, loss: 0.501, lr: 0.011628583057154487\n",
      "epoch: 7300, acc: 0.780, loss: 0.498, lr: 0.01156136192843517\n",
      "epoch: 7400, acc: 0.773, loss: 0.494, lr: 0.011494913500775908\n",
      "epoch: 7500, acc: 0.773, loss: 0.501, lr: 0.011429224527115835\n",
      "epoch: 7600, acc: 0.773, loss: 0.503, lr: 0.011364282061480767\n",
      "epoch: 7700, acc: 0.780, loss: 0.481, lr: 0.011300073450477429\n",
      "epoch: 7800, acc: 0.777, loss: 0.479, lr: 0.011236586325074443\n",
      "epoch: 7900, acc: 0.787, loss: 0.484, lr: 0.011173808592658808\n",
      "epoch: 8000, acc: 0.807, loss: 0.471, lr: 0.011111728429357186\n",
      "epoch: 8100, acc: 0.803, loss: 0.468, lr: 0.011050334272611746\n",
      "epoch: 8200, acc: 0.830, loss: 0.467, lr: 0.01098961481400077\n",
      "epoch: 8300, acc: 0.810, loss: 0.457, lr: 0.010929558992294662\n",
      "epoch: 8400, acc: 0.793, loss: 0.456, lr: 0.01087015598673841\n",
      "epoch: 8500, acc: 0.813, loss: 0.447, lr: 0.010811395210551923\n",
      "epoch: 8600, acc: 0.803, loss: 0.447, lr: 0.010753266304640035\n",
      "epoch: 8700, acc: 0.807, loss: 0.445, lr: 0.010695759131504359\n",
      "epoch: 8800, acc: 0.803, loss: 0.447, lr: 0.010638863769349433\n",
      "epoch: 8900, acc: 0.837, loss: 0.438, lr: 0.010582570506376\n",
      "epoch: 9000, acc: 0.823, loss: 0.431, lr: 0.010526869835254487\n",
      "epoch: 9100, acc: 0.807, loss: 0.444, lr: 0.010471752447772136\n",
      "epoch: 9200, acc: 0.817, loss: 0.428, lr: 0.010417209229647378\n",
      "epoch: 9300, acc: 0.817, loss: 0.422, lr: 0.010363231255505468\n",
      "epoch: 9400, acc: 0.833, loss: 0.434, lr: 0.010309809784009485\n",
      "epoch: 9500, acc: 0.837, loss: 0.415, lr: 0.010256936253141186\n",
      "epoch: 9600, acc: 0.837, loss: 0.412, lr: 0.010204602275626307\n",
      "epoch: 9700, acc: 0.847, loss: 0.407, lr: 0.010152799634499212\n",
      "epoch: 9800, acc: 0.830, loss: 0.406, lr: 0.010101520278801958\n",
      "epoch: 9900, acc: 0.830, loss: 0.406, lr: 0.010050756319413037\n",
      "epoch: 10000, acc: 0.853, loss: 0.403, lr: 0.010000500025001252\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_RMSprop(learning_rate=0.02, decay=1e-4, rho=0.999)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8806bfaa",
   "metadata": {},
   "source": [
    "### Adam (Adative Momentum)\n",
    "Currently most widely used optimizer. Built atop RMSProp, with momentum from SGD. Insteadof just applying the current gradients, we apply momentums like SGD optimizers, then apply per-weight adaptive learning rate with cache as does RMSProp.\n",
    "\n",
    "Also adds a bias correction mechanism to the cache and momentum, compensating for the initial zeroed values before they warm up with the initial steps. This is done by dividing the momentum and the cache by (1-beta^step)\n",
    "\n",
    "    1 - 0.9 ** 1 = 1 - 0.9 = 0.1\n",
    "    \n",
    "As training goes on, step count rises\n",
    "\n",
    "    1 - 0.9 ** inf = 1 - 0 = 1\n",
    "    \n",
    "The *beta 1* for momentum starts at 0.9, and *beta 2* for cache starts at 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "360ccbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    \n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                layer.weight_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                                layer.bias_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start wit h1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        # get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            weight_momentums_corrected / \\\n",
    "                            (np.sqrt(weight_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            bias_momentums_corrected / \\\n",
    "                            (np.sqrt(bias_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d5d3f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.313, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.623, loss: 0.893, lr: 0.01998021958261321\n",
      "epoch: 200, acc: 0.740, loss: 0.646, lr: 0.019960279044701046\n",
      "epoch: 300, acc: 0.763, loss: 0.551, lr: 0.019940378268975763\n",
      "epoch: 400, acc: 0.767, loss: 0.499, lr: 0.01992051713662487\n",
      "epoch: 500, acc: 0.790, loss: 0.455, lr: 0.01990069552930875\n",
      "epoch: 600, acc: 0.817, loss: 0.398, lr: 0.019880913329158343\n",
      "epoch: 700, acc: 0.833, loss: 0.369, lr: 0.019861170418772778\n",
      "epoch: 800, acc: 0.847, loss: 0.340, lr: 0.019841466681217078\n",
      "epoch: 900, acc: 0.857, loss: 0.324, lr: 0.01982180200001982\n",
      "epoch: 1000, acc: 0.873, loss: 0.306, lr: 0.019802176259170884\n",
      "epoch: 1100, acc: 0.880, loss: 0.293, lr: 0.01978258934311912\n",
      "epoch: 1200, acc: 0.883, loss: 0.283, lr: 0.01976304113677013\n",
      "epoch: 1300, acc: 0.887, loss: 0.273, lr: 0.019743531525483964\n",
      "epoch: 1400, acc: 0.897, loss: 0.264, lr: 0.01972406039507293\n",
      "epoch: 1500, acc: 0.900, loss: 0.250, lr: 0.019704627631799327\n",
      "epoch: 1600, acc: 0.920, loss: 0.235, lr: 0.019685233122373254\n",
      "epoch: 1700, acc: 0.923, loss: 0.223, lr: 0.019665876753950384\n",
      "epoch: 1800, acc: 0.927, loss: 0.213, lr: 0.01964655841412981\n",
      "epoch: 1900, acc: 0.930, loss: 0.203, lr: 0.019627277990951823\n",
      "epoch: 2000, acc: 0.930, loss: 0.197, lr: 0.019608035372895814\n",
      "epoch: 2100, acc: 0.920, loss: 0.194, lr: 0.01958883044887805\n",
      "epoch: 2200, acc: 0.930, loss: 0.187, lr: 0.019569663108249594\n",
      "epoch: 2300, acc: 0.933, loss: 0.182, lr: 0.01955053324079414\n",
      "epoch: 2400, acc: 0.940, loss: 0.178, lr: 0.019531440736725945\n",
      "epoch: 2500, acc: 0.933, loss: 0.174, lr: 0.019512385486687673\n",
      "epoch: 2600, acc: 0.940, loss: 0.171, lr: 0.019493367381748363\n",
      "epoch: 2700, acc: 0.947, loss: 0.168, lr: 0.019474386313401298\n",
      "epoch: 2800, acc: 0.947, loss: 0.165, lr: 0.019455442173562\n",
      "epoch: 2900, acc: 0.937, loss: 0.162, lr: 0.019436534854566128\n",
      "epoch: 3000, acc: 0.943, loss: 0.161, lr: 0.01941766424916747\n",
      "epoch: 3100, acc: 0.937, loss: 0.157, lr: 0.019398830250535893\n",
      "epoch: 3200, acc: 0.937, loss: 0.155, lr: 0.019380032752255354\n",
      "epoch: 3300, acc: 0.937, loss: 0.152, lr: 0.01936127164832186\n",
      "epoch: 3400, acc: 0.943, loss: 0.150, lr: 0.01934254683314152\n",
      "epoch: 3500, acc: 0.947, loss: 0.149, lr: 0.019323858201528515\n",
      "epoch: 3600, acc: 0.947, loss: 0.146, lr: 0.019305205648703173\n",
      "epoch: 3700, acc: 0.943, loss: 0.143, lr: 0.01928658907028997\n",
      "epoch: 3800, acc: 0.947, loss: 0.139, lr: 0.01926800836231563\n",
      "epoch: 3900, acc: 0.950, loss: 0.137, lr: 0.019249463421207133\n",
      "epoch: 4000, acc: 0.950, loss: 0.135, lr: 0.019230954143789846\n",
      "epoch: 4100, acc: 0.947, loss: 0.133, lr: 0.019212480427285565\n",
      "epoch: 4200, acc: 0.947, loss: 0.133, lr: 0.019194042169310647\n",
      "epoch: 4300, acc: 0.950, loss: 0.130, lr: 0.019175639267874092\n",
      "epoch: 4400, acc: 0.947, loss: 0.129, lr: 0.019157271621375684\n",
      "epoch: 4500, acc: 0.950, loss: 0.128, lr: 0.0191389391286041\n",
      "epoch: 4600, acc: 0.950, loss: 0.127, lr: 0.019120641688735073\n",
      "epoch: 4700, acc: 0.957, loss: 0.127, lr: 0.019102379201329525\n",
      "epoch: 4800, acc: 0.953, loss: 0.126, lr: 0.01908415156633174\n",
      "epoch: 4900, acc: 0.953, loss: 0.125, lr: 0.01906595868406753\n",
      "epoch: 5000, acc: 0.950, loss: 0.123, lr: 0.01904780045524243\n",
      "epoch: 5100, acc: 0.953, loss: 0.123, lr: 0.019029676780939874\n",
      "epoch: 5200, acc: 0.953, loss: 0.122, lr: 0.019011587562619416\n",
      "epoch: 5300, acc: 0.953, loss: 0.121, lr: 0.01899353270211493\n",
      "epoch: 5400, acc: 0.953, loss: 0.121, lr: 0.018975512101632844\n",
      "epoch: 5500, acc: 0.953, loss: 0.120, lr: 0.018957525663750367\n",
      "epoch: 5600, acc: 0.953, loss: 0.119, lr: 0.018939573291413745\n",
      "epoch: 5700, acc: 0.953, loss: 0.119, lr: 0.018921654887936498\n",
      "epoch: 5800, acc: 0.957, loss: 0.119, lr: 0.018903770356997706\n",
      "epoch: 5900, acc: 0.953, loss: 0.118, lr: 0.018885919602640248\n",
      "epoch: 6000, acc: 0.950, loss: 0.119, lr: 0.018868102529269144\n",
      "epoch: 6100, acc: 0.960, loss: 0.118, lr: 0.018850319041649778\n",
      "epoch: 6200, acc: 0.953, loss: 0.117, lr: 0.018832569044906263\n",
      "epoch: 6300, acc: 0.957, loss: 0.116, lr: 0.018814852444519702\n",
      "epoch: 6400, acc: 0.953, loss: 0.115, lr: 0.018797169146326564\n",
      "epoch: 6500, acc: 0.953, loss: 0.115, lr: 0.01877951905651696\n",
      "epoch: 6600, acc: 0.953, loss: 0.115, lr: 0.018761902081633034\n",
      "epoch: 6700, acc: 0.953, loss: 0.114, lr: 0.018744318128567278\n",
      "epoch: 6800, acc: 0.953, loss: 0.114, lr: 0.018726767104560903\n",
      "epoch: 6900, acc: 0.963, loss: 0.115, lr: 0.018709248917202218\n",
      "epoch: 7000, acc: 0.953, loss: 0.113, lr: 0.018691763474424996\n",
      "epoch: 7100, acc: 0.953, loss: 0.113, lr: 0.018674310684506857\n",
      "epoch: 7200, acc: 0.953, loss: 0.113, lr: 0.01865689045606769\n",
      "epoch: 7300, acc: 0.953, loss: 0.113, lr: 0.01863950269806802\n",
      "epoch: 7400, acc: 0.953, loss: 0.112, lr: 0.018622147319807447\n",
      "epoch: 7500, acc: 0.960, loss: 0.112, lr: 0.018604824230923075\n",
      "epoch: 7600, acc: 0.953, loss: 0.111, lr: 0.01858753334138793\n",
      "epoch: 7700, acc: 0.953, loss: 0.111, lr: 0.018570274561509396\n",
      "epoch: 7800, acc: 0.953, loss: 0.111, lr: 0.018553047801927663\n",
      "epoch: 7900, acc: 0.960, loss: 0.107, lr: 0.018535852973614212\n",
      "epoch: 8000, acc: 0.823, loss: 0.688, lr: 0.01851868998787026\n",
      "epoch: 8100, acc: 0.957, loss: 0.106, lr: 0.018501558756325222\n",
      "epoch: 8200, acc: 0.960, loss: 0.106, lr: 0.01848445919093522\n",
      "epoch: 8300, acc: 0.960, loss: 0.105, lr: 0.018467391203981567\n",
      "epoch: 8400, acc: 0.960, loss: 0.105, lr: 0.018450354708069265\n",
      "epoch: 8500, acc: 0.960, loss: 0.104, lr: 0.018433349616125496\n",
      "epoch: 8600, acc: 0.960, loss: 0.104, lr: 0.018416375841398172\n",
      "epoch: 8700, acc: 0.960, loss: 0.104, lr: 0.01839943329745444\n",
      "epoch: 8800, acc: 0.960, loss: 0.104, lr: 0.01838252189817921\n",
      "epoch: 8900, acc: 0.960, loss: 0.104, lr: 0.018365641557773718\n",
      "epoch: 9000, acc: 0.960, loss: 0.103, lr: 0.018348792190754044\n",
      "epoch: 9100, acc: 0.960, loss: 0.103, lr: 0.0183319737119497\n",
      "epoch: 9200, acc: 0.960, loss: 0.103, lr: 0.018315186036502167\n",
      "epoch: 9300, acc: 0.960, loss: 0.103, lr: 0.018298429079863496\n",
      "epoch: 9400, acc: 0.960, loss: 0.103, lr: 0.018281702757794862\n",
      "epoch: 9500, acc: 0.960, loss: 0.103, lr: 0.018265006986365174\n",
      "epoch: 9600, acc: 0.960, loss: 0.102, lr: 0.018248341681949654\n",
      "epoch: 9700, acc: 0.960, loss: 0.102, lr: 0.018231706761228456\n",
      "epoch: 9800, acc: 0.960, loss: 0.102, lr: 0.018215102141185255\n",
      "epoch: 9900, acc: 0.960, loss: 0.102, lr: 0.018198527739105907\n",
      "epoch: 10000, acc: 0.960, loss: 0.102, lr: 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=1e-5)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4633ac35",
   "metadata": {},
   "source": [
    "Slightly Different hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f8ed6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.333, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.657, loss: 0.706, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.770, loss: 0.529, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.800, loss: 0.463, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.800, loss: 0.436, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.813, loss: 0.380, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.817, loss: 0.359, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.840, loss: 0.345, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.833, loss: 0.334, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.823, loss: 0.322, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.840, loss: 0.318, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.850, loss: 0.310, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.853, loss: 0.303, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.860, loss: 0.303, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.850, loss: 0.293, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.873, loss: 0.289, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.870, loss: 0.281, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.857, loss: 0.290, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.870, loss: 0.273, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.860, loss: 0.268, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.867, loss: 0.263, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.883, loss: 0.264, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.870, loss: 0.262, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.877, loss: 0.246, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.880, loss: 0.240, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.883, loss: 0.237, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.890, loss: 0.234, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.887, loss: 0.231, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.893, loss: 0.228, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.897, loss: 0.225, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.900, loss: 0.223, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.900, loss: 0.221, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.900, loss: 0.219, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.900, loss: 0.217, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.900, loss: 0.223, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.910, loss: 0.214, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.897, loss: 0.218, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.903, loss: 0.209, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.910, loss: 0.209, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.903, loss: 0.206, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.903, loss: 0.208, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.910, loss: 0.203, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.907, loss: 0.206, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.900, loss: 0.214, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.907, loss: 0.200, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.913, loss: 0.201, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.910, loss: 0.203, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.910, loss: 0.197, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.907, loss: 0.194, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.913, loss: 0.196, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.913, loss: 0.190, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.907, loss: 0.198, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.917, loss: 0.188, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.917, loss: 0.185, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.910, loss: 0.192, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.917, loss: 0.180, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.913, loss: 0.183, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.913, loss: 0.184, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.923, loss: 0.177, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.917, loss: 0.177, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.913, loss: 0.180, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.927, loss: 0.175, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.930, loss: 0.176, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.907, loss: 0.189, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.920, loss: 0.181, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.923, loss: 0.178, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.927, loss: 0.174, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.930, loss: 0.172, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.930, loss: 0.170, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.930, loss: 0.169, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.930, loss: 0.169, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.930, loss: 0.168, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.933, loss: 0.168, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.933, loss: 0.167, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.933, loss: 0.167, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.933, loss: 0.166, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.920, loss: 0.167, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.933, loss: 0.165, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.913, loss: 0.170, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.913, loss: 0.168, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.933, loss: 0.164, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.937, loss: 0.165, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.937, loss: 0.164, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.930, loss: 0.167, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.913, loss: 0.175, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.937, loss: 0.163, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.930, loss: 0.166, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.930, loss: 0.163, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.923, loss: 0.163, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.937, loss: 0.163, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.933, loss: 0.161, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.930, loss: 0.167, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.913, loss: 0.170, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.937, loss: 0.161, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.933, loss: 0.162, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.937, loss: 0.164, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.913, loss: 0.165, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.927, loss: 0.161, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.923, loss: 0.161, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.920, loss: 0.160, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.910, loss: 0.165, lr: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a26f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
