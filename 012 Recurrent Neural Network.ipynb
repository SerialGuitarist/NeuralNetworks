{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd0ba1b",
   "metadata": {},
   "source": [
    "Traditional multilayer perceptrons reboot their understanding of langauges every time it's called upon, unlike a human who has memory\n",
    "\n",
    "A Recurrent Neural Networks (RNNs) implement a similar mechanism, making neural networks Turing complete\n",
    "\n",
    "#### Applications\n",
    "* Speech recognition\n",
    "* Machine translation\n",
    "* Music composition\n",
    "* Handwriting recognition\n",
    "* Grammar learning\n",
    "\n",
    "For some classes of data, the order in which the observations are recieved is important:\n",
    "1. \"I'm sorry... it's not you, it's me\"\n",
    "2. \"It's not me, it's you... I'm sorry\"\n",
    "\n",
    "In a normal feed forward network, the hidden layers provide a useful intermediate representation, which each layer transforming the input data into another representation to make it easier to solve the problem. The **hidden state** is the representation being stored in the hidden layers.\n",
    "\n",
    "When considering the next time-step in the sequence, we want to leverage any information we've already extracted from the sequence, by calculating the next hidden state as a combination of the previous hidden state and latest input\n",
    "\n",
    "One method of combining is as follows\n",
    "1. The representation of the prevous item in the sequence is calculated as $w_{ih}x^{(t=1)}_{hidden}$\n",
    "2. The representation of the next item in the sequence is calculated as $w_{ih}x^{(t=2)}_{hidden}$\n",
    "3. The representation of the prevous time step is combined as $x^{(t)}_{hidden}=tanh(w_{hh}x^{(t-1)}_{hidden}+w_{ih}x^{(t)}_{input})$\n",
    "4. Repeat as long as necessary for an arbirarily long sequence of observations\n",
    "\n",
    "Notice the different set of weights used for the previous state and the current state\n",
    "\n",
    "Other methods of combining include:\n",
    "* Gated recurrent units\n",
    "* Long short-term memory units\n",
    "\n",
    "By remembering the previous hidden state, we can backpropagate to the earlier timesteps, \"backpropagation through time.\" If training a vanilla neural network is optimization over functions, training recurrent nets is optimization over programs\n",
    "\n",
    "# Common Structures\n",
    "The ability to handle an arbitrary length input and outputs allows the RNN to tackle a broad range of tasks\n",
    "## One to many\n",
    "Generating an arbitrary length sequence from a fixed length input\n",
    "* Image captioning\n",
    "The prediction from each time step is fed in as input to the enxt timestep\n",
    "\n",
    "## Many to one\n",
    "* Sentiment from a text\n",
    "Generally used to do classification on a sequence of data\n",
    "\n",
    "## Many to many (same)\n",
    "Predicting a lable for each observation in a sequence; **dense classification**\n",
    "* Detect named entiies (person, organization, location) in sentences\n",
    "* Predicting the current activity in a frame of a video\n",
    "\n",
    "## Many to many (different)\n",
    "Translates a sequence of inputs to a different but related sequence of outputs\n",
    "\n",
    "Both the inputs and the outputs are abritrary length sequence\n",
    "\n",
    "* Machine translation\n",
    "\n",
    "## Bidirectionality\n",
    "A weakness in an ordinary RNN is being able to use only the observations previously seen. For example, a model for recognizing named entity recognition:\n",
    "\n",
    "* \"I can't believe that Teddy Roosevelt was your great grandfather!\"\n",
    "* \"I can't believe that Teddy bear is made out of chocolate!\"\n",
    "\n",
    "In this case, it's very difficult to tell if Teddy is the president or the bear if you're only going from the front to back. The model needs to be able to see what comes after the current token, to determine if it's the start of a name\n",
    "\n",
    "A **bidirectional** recurrent network processes the sequence left-to-right and right-to-left in parallel then combines these two representation such that at any point in the sequence, the model has knowledge of the tokens which can before AND after it\n",
    "\n",
    "# Limitations\n",
    "The layer's weights are SHARED across time-steps, so when a RNN makes a mistake, the error signal needs to back many time-steps\n",
    "\n",
    "Because the signal is being multiplied by the same weight many time over, if it's something like 0.5, after 10 time steps, the error signal has changed by a factor of $0.5^{10} = 0.00098$, drastically reducing the magnitude of the error signal: **vanishing gradient problem** A large weight can have a similar problem, but can be managed through gradient clipping.\n",
    "\n",
    "This makes training with an RNN rather slow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbbbee5",
   "metadata": {},
   "source": [
    "# Computation\n",
    "Accepts an input vector x, and outputs vector y, but influenced by the entire history of inputs\n",
    "\n",
    "A forward pass of a RNN layer might look like this\n",
    "\n",
    "* Calculating the new history based on the previous history\n",
    "\n",
    "$a^{(t)} = b + Wh^{(t-1)} + Ux^{(t)}$\n",
    "\n",
    "$h^{(t)} = tanh(a^{(t)})$\n",
    "\n",
    "* Calculating the output from the layer\n",
    "\n",
    "$ o^{(t)} = c + Vh(t) $\n",
    "\n",
    "* Calculating the classification from that\n",
    "\n",
    "$ \\hat{y}^{(t)} = softmax(o^{(t)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518bfe9",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Very small dataset of sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc90e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 unique words found\n",
      "{'earlier': 0, 'or': 1, 'not': 2, 'sad': 3, 'all': 4, 'at': 5, 'happy': 6, 'very': 7, 'now': 8, 'was': 9, 'this': 10, 'is': 11, 'and': 12, 'right': 13, 'i': 14, 'good': 15, 'bad': 16, 'am': 17}\n",
      "{0: 'earlier', 1: 'or', 2: 'not', 3: 'sad', 4: 'all', 5: 'at', 6: 'happy', 7: 'very', 8: 'now', 9: 'was', 10: 'this', 11: 'is', 12: 'and', 13: 'right', 14: 'i', 15: 'good', 16: 'bad', 17: 'am'}\n"
     ]
    }
   ],
   "source": [
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "\n",
    "# Create the vocabulary\n",
    "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
    "vocab_size = len(vocab)\n",
    "print('%d unique words found' % vocab_size) # 18 unique words found\n",
    "\n",
    "# Assign indices to each word.\n",
    "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
    "idx_to_word = { i: w for i, w in enumerate(vocab) }\n",
    "\n",
    "print(word_to_idx)\n",
    "print(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e6d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def createInputs(text):\n",
    "    # turns the text input into a one hot array\n",
    "    inputs = []\n",
    "    \n",
    "    for word in text.split(' '):\n",
    "        working = np.zeros((vocab_size, 1))\n",
    "        working[word_to_idx[word]] = 1\n",
    "        inputs.append(working)\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d47ece95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Recurrent:\n",
    "    # ...\n",
    "    def forward(self, inputs):\n",
    "        # remember input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # update the hidden state\n",
    "        self.h = np.tanh(np.dot(self.history, self.weights_hh) +\n",
    "                         np.dot(inputs, self.weights_xh) +\n",
    "                         self.bias_h)\n",
    "        \n",
    "        # output vector\n",
    "        self.output = np.dot(self.history, self.weights_hy) + self.bias_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705ddc4",
   "metadata": {},
   "source": [
    "The tanh function is user here because it squishes the output between \\[-1; 1\\]\n",
    "\n",
    "But multiple RNN layers can be layered one after another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ab8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_1.forward(X)\n",
    "recurrent_2.forward(recurrent_1.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8717e5d",
   "metadata": {},
   "source": [
    "Neither layers know or care that the other is recurrent layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b80b3bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size = 64):\n",
    "        \n",
    "        # weights\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.001\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.001\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.001\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7fc37e",
   "metadata": {},
   "source": [
    "In action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "387181b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49999525],\n",
       "       [0.50000475]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(xs):\n",
    "    # Applies the Softmax Function to the input array.\n",
    "    return np.exp(xs) / sum(np.exp(xs))\n",
    "\n",
    "# initialization of the model\n",
    "model = RNN(vocab_size, 2)\n",
    "\n",
    "inputs = createInputs(\"i am very good\")\n",
    "out, h = model.forward(inputs)\n",
    "probs = softmax(out)\n",
    "\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f898e1c5",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The layer needs to remember every history for the time steps, and every input for the time steps\n",
    "\n",
    "Because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time step\n",
    "\n",
    "#### Loss\n",
    "But first, a very quick implementation of categorical cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c64241c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c05b9f37",
   "metadata": {},
   "source": [
    "#### Remembering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c376d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size = 64):\n",
    "        \n",
    "        # weights\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.001\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.001\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.001\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "        \n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = {0: h}\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    def backward(self, dvalues, learn_rate = 2e-2):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071366a",
   "metadata": {},
   "source": [
    "#### Gradient\n",
    "Since the derivative of the gradient, $\\frac{\\partial L}{\\partial y_i}$, comes out as \n",
    "\n",
    "* $p_i$ when $i \\neq c$\n",
    "* $p_i - 1$ when $i = c$\n",
    "\n",
    "Where $p$ is the probability of a given index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb1a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping over each training example\n",
    "for x, y in train_data.items():\n",
    "    inputs = createInputs(x)\n",
    "    target = int(y)\n",
    "    \n",
    "    # forward\n",
    "    out, _ = model.forward(inputs)\n",
    "    probs = softmax(out)\n",
    "    \n",
    "    # build dL/dy\n",
    "    dL_dy = probs\n",
    "    dL_dy[target] -= 1\n",
    "    \n",
    "    # backward\n",
    "    model.backward(dL_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3529d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size = 64):\n",
    "        \n",
    "        # weights\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.001\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.001\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.001\n",
    "\n",
    "        # Biases\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "        \n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = {0: h}\n",
    "\n",
    "        # Perform each step of the RNN\n",
    "        for i, x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "            self.last_hs[i + 1] = h\n",
    "\n",
    "        # Compute the output\n",
    "        y = self.Why @ h + self.by\n",
    "\n",
    "        return y, h\n",
    "    \n",
    "    def backward(self, dvalues, learn_rate = 2e-2):\n",
    "        \n",
    "        n = len(self.last_inputs)\n",
    "        \n",
    "        d_Why = dvalues @ self.last_hs[n].T\n",
    "        d_by = dvalues\n",
    "        \n",
    "        # Because Wxh affects every h_t, which all affect y\n",
    "        # we need to backpropagate through all timesteps\n",
    "        # and take the sum of all of the gradients we get\n",
    "        \n",
    "        # initializing each of the needed gradients as zero\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh  = np.zeros(self.bh.shape)\n",
    "        \n",
    "        # calculate dL/dh for the last h\n",
    "        dh = self.Why.T @ dvalues\n",
    "        \n",
    "        # backpropagate through time\n",
    "        for t in reversed(range(n)):\n",
    "            # intermediate value\n",
    "            # dL/dsum = dL/dh * dh/dsum\n",
    "            # dvalue * (1 - h^2)\n",
    "            temp = dh * ((1 - self.last_hs[t + 1] ** 2))\n",
    "            \n",
    "            # dL/dbh = dL/h * dh/dsum * dsum/dbh\n",
    "            d_bh += temp\n",
    "            \n",
    "            # dL/dWhh = dL/h * dh/dsum * dsum/dWhh\n",
    "            d_Whh += temp @ self.last_hs[t].T\n",
    "            \n",
    "            # dL/dWxh = dL/h * dh/dsum * dsum/x\n",
    "            d_Wxh += temp @ self.last_inputs[t].T\n",
    "            \n",
    "            # next dL/dh = dL/dh * dh/dsum * dsum/dh\n",
    "            d_h = self.Whh @ temp\n",
    "            \n",
    "        # clip to prevent exploding gradients\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out = d)\n",
    "        \n",
    "        # update the weights and biases using gradient descent\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a2989a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "model = RNN(vocab_size, 2)\n",
    "\n",
    "def processData(data, backprop = True):\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "    \n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "    \n",
    "    for x, y in items:\n",
    "        inputs = createInputs(x)\n",
    "        target = int(y)\n",
    "        \n",
    "        # forward\n",
    "        out, _ = model.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "        \n",
    "        # calculate loss/accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        num_correct += int(np.argmax(probs) == target)\n",
    "        \n",
    "        if backprop:\n",
    "            dL_dy = probs\n",
    "            dL_dy[target] -= 1\n",
    "        \n",
    "            model.backward(dL_dy)\n",
    "        \n",
    "    return loss / len(data), num_correct / len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7f766",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f2650a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 100\n",
      "Train:\tLoss 0.589 | Accuracy: 0.672\n",
      "Test:\tLoss 0.609 | Accuracy: 0.800\n",
      "--- Epoch 200\n",
      "Train:\tLoss 0.423 | Accuracy: 0.845\n",
      "Test:\tLoss 0.480 | Accuracy: 0.800\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.268 | Accuracy: 0.914\n",
      "Test:\tLoss 0.259 | Accuracy: 0.950\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.013 | Accuracy: 1.000\n",
      "Test:\tLoss 0.014 | Accuracy: 1.000\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.005 | Accuracy: 1.000\n",
      "Test:\tLoss 0.005 | Accuracy: 1.000\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.357 | Accuracy: 0.862\n",
      "Test:\tLoss 0.706 | Accuracy: 0.550\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.009 | Accuracy: 1.000\n",
      "Test:\tLoss 0.033 | Accuracy: 1.000\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.005 | Accuracy: 1.000\n",
      "Test:\tLoss 0.034 | Accuracy: 1.000\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.043 | Accuracy: 0.950\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.054 | Accuracy: 0.950\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    train_loss, train_acc = processData(train_data)\n",
    "    \n",
    "    if epoch % 100 == 99:\n",
    "        '''\n",
    "        print(f'====Epoch: {epoch}, ' +\n",
    "              f'loss: {train_loss:.3f}, ' +\n",
    "              f'accuracy: {train_acc:.3f}')\n",
    "        '''\n",
    "        print('--- Epoch %d' % (epoch + 1))\n",
    "        print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "        test_loss, test_acc = processData(test_data, backprop = False)\n",
    "        print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ec51a6",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTM) Network\n",
    "A better performing variant of the RNN, that calculates its history differently, and has appealing backpropagation dynamics, but uses a lot more computational resources\n",
    "\n",
    "Solves the problem with standard RNNs, which fail to learn when there are 5 to 10 discrete time steps of lag in the input\n",
    "\n",
    "If a sequence is long enough, the model will have difficulties carrying information from earlier time steps to later ones\n",
    "\n",
    "Solves the problems of vanishing gradients and exploding gradients\n",
    "\n",
    "The hidden layers are fully self connected, contains memory cells and corresponding gate units\n",
    "\n",
    "An LSTM layer consists of a set of recurrently connected blocks; **memory blocks**, that can be thought of as differentiable versions of memory chips of a computer. Each one contains one or more recurrently connected memory cells, and three multiplicative units:\n",
    "\n",
    "* the input\n",
    "* the output\n",
    "* the forget gates\n",
    "\n",
    "These units provide continuous analogues of write, read, and rest operations for the cells\n",
    "\n",
    "The cells can only be interacted with via the gates\n",
    "\n",
    "The gates can learn which data in a sequence is important and to keep or to throw away, so it passes relevant information down the long chain of sequences to make predictions\n",
    "\n",
    "Learning rate and network size are the most crucial tunable hyperparameters, meaning they can be tuned independently. So the learning rate can be calibrated first using a small network, saving lot of experimentation time\n",
    "\n",
    "## Eg\n",
    "When trying to decipher the sentiment from the following review:\n",
    "\n",
    "\"Amazing! This box of cereal gave me a perfectly balances breakfast, as all things should be. I only ate half of it, but will definitely be buying again.\"\n",
    "\n",
    "A human, after reading the review, will prolly remember the main points and key words such as\n",
    "\n",
    "* Amazing\n",
    "* perfectly balanced breakfast\n",
    "* will definitely be buying again\n",
    "\n",
    "And forgetting a lot of words you don't care about like \"this\", \"gave\", \"all\", etc etc\n",
    "\n",
    "This is what a LSTM does\n",
    "\n",
    "## Applications\n",
    "Can be used for any sequential task, in which a hierarchical decomposition may exist, but unknown to us\n",
    "\n",
    "Effective at capturing long-term temporal dependencies without suffering from the optimization problems of simple recurrent networks\n",
    "\n",
    "State of the art for many difficult problems\n",
    "\n",
    "* language modeling\n",
    "* speech recognition\n",
    "* acoustic modeling of speech\n",
    "* speech synthesis\n",
    "* machine translation\n",
    "* handwriting recognition and generation\n",
    "* protein secondary structure prediction\n",
    "* audio analysis\n",
    "\n",
    "## Bidirectional LSTMs\n",
    "Present the training sequence fowards and backwards to two separate recurrent nets, both of which are connected to the same output layer\n",
    "\n",
    "So the BRNN has a complete knowledge of all the points before and after it. Also nulls the need to find a time-window or target delay size because the net is free to use as much or as little of this context as necessary\n",
    "\n",
    "Even problems such as speech recognition, for which a BRNN seems to violate causality, extremely helpful because humans make sense of sounds and words in light of the context revealed in the future\n",
    "\n",
    "The output of a current timestep is generated from the combination of both layers' hidden vectors\n",
    "\n",
    "## seq2seq LSTMs || RNN Encoder-Decoders\n",
    "One LSTM reads the input sequence, one timestep at a time, creating a large fixed-dimensional vector representation\n",
    "\n",
    "Another LSTM extracts the output sequence from that vector\n",
    "\n",
    "The LSTM's ability to successfully learn from data with long range temporal dependencies makes it an excellent choice for this application, because of the large time lag between the inputs and their corresponding outputs\n",
    "\n",
    "The encoder can be replaced by a CNN, first trained for image classification task, then the last hidden layer feeding into an RNN decoder that generates sentences\n",
    "\n",
    "## Inner Workings\n",
    "Tries to \"remember\" all the past knowledge that the network has seen so far and to \"forget\" irrelevant data. Done through the introduction of different activation function layers called \"gates\" with different purposes\n",
    "\n",
    "Each LSTM recurrent unit maintains a vector called **Internal Cell State**, the information that was chosen to be retained by the previous LSTM recurrent unit\n",
    "\n",
    "The gates are:\n",
    "* Forget Gate (f): determines to what extent to forget the previous data\n",
    "* Input Gate (i): determines the extent of information to be written onto the Internal Cell State\n",
    "* Input Modulation Gate (g): a sub-part of the input gate, generally assumed to be inside the input gate\n",
    "    * Modulates the information that the input gate writes onto the Internal State Cell by adding non-lineraity to the information, and making the information **Zero-Mean**\n",
    "    * Done so to reduce the learning time\n",
    "    * The zero-mean input has faster convergence\n",
    "* Output Gate (o): determines what output (next hidden state) to generate from the current internal cell state\n",
    "\n",
    "Works generally similarly to a SRN, but the Internal Cell State is passed foward along with the hidden state\n",
    "\n",
    "### Workflow\n",
    "1. Take the current input, the previous hidden state, and the previous internal cell state\n",
    "2. Calculate the values of the four gates by\n",
    "    * For each gate:\n",
    "        * Calculate the parameterized vectors for the current input and the previous hidden state by element-wise multiplication with the concerned vector with the respective weights for each gate\n",
    "    * Apply the respective activation function for each gate element-wise on the parameterized vectors\n",
    "        * sigmoid(f)\n",
    "        * sigmoid(i)\n",
    "        * tanh(g)\n",
    "        * sigmoid(o)\n",
    "3. Calculate the current internal cell state by first calculating the element-wise multiplication vector of the input gate and the input modulation gate, then calculate the element-wise multiplication vector of the forget gate and the previous internal cell satte and then add the two vectors\n",
    "4. Calculate the current hidden state by first taking the element-wise hyperbolic tanget of the current internal cell state vector and then performing element-wise multiplication with the output gate\n",
    "\n",
    "The backpropagation is only different from the RNN because of the different mathematics, but follows the same flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bf667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
