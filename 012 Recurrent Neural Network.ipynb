{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e44d0ac0",
   "metadata": {},
   "source": [
    "Traditional multilayer perceptrons reboot their understanding of langauges every time it's called upon, unlike a human who has memory\n",
    "\n",
    "A Recurrent Neural Networks (RNNs) implement a similar mechanism\n",
    "\n",
    "#### Applications\n",
    "* Speech recognition\n",
    "* Machine translation\n",
    "* Music composition\n",
    "* Handwriting recognition\n",
    "* Grammar learning\n",
    "\n",
    "For some classes of data, the order in which the observations are recieved is important:\n",
    "1. \"I'm sorry... it's not you, it's me\"\n",
    "2. \"It's not me, it's you... I'm sorry\"\n",
    "\n",
    "In a normal feed forward network, the hidden layers provide a useful intermediate representation, which each layer transforming the input data into another representation to make it easier to solve the problem. The **hidden state** is the representation being stored in the hidden layers.\n",
    "\n",
    "When considering the next time-step in the sequence, we want to leverage any information we've already extracted from the sequence, by calculating the next hidden state as a combination of the previous hidden state and latest input\n",
    "\n",
    "One method of combining is as follows\n",
    "1. The representation of the prevous item in the sequence is calculated as $w_{ih}x^{(t=1)}_{hidden}$\n",
    "2. The representation of the next item in the sequence is calculated as $w_{ih}x^{(t=2)}_{hidden}$\n",
    "3. The representation of the prevous time step is combined as $x^{(t)}_{hidden}=tanh(w_{hh}x^{(t-1)}_{hidden}+w_{ih}x^{(t)}_{input})$\n",
    "4. Repeat as long as necessary for an arbirarily long sequence of observations\n",
    "\n",
    "Notice the different set of weights used for the previous state and the current state\n",
    "\n",
    "Other methods of combining include:\n",
    "* Gated recurrent units\n",
    "* Long short-term memory units\n",
    "\n",
    "By remembering the previous hidden state, we can backpropagate to the earlier timesteps, \"backpropagation through time\"\n",
    "\n",
    "# Common Structures\n",
    "The ability to handle an arbitrary length input and outputs allows the RNN to tackle a broad range of tasks\n",
    "## One to many\n",
    "Generating an arbitrary length sequence from a fixed length input\n",
    "* Image captioning\n",
    "The prediction from each time step is fed in as input to the enxt timestep\n",
    "\n",
    "## Many to one\n",
    "* Sentiment from a text\n",
    "Generally used to do classification on a sequence of data\n",
    "\n",
    "## Many to many (same)\n",
    "Predicting a lable for each observation in a sequence; **dense classification**\n",
    "* Detect named entiies (person, organization, location) in sentences\n",
    "* Predicting the current activity in a frame of a video\n",
    "\n",
    "## Many to many (different)\n",
    "Translates a sequence of inputs to a different but related sequence of outputs\n",
    "\n",
    "Both the inputs and the outputs are abritrary length sequence\n",
    "\n",
    "* Machine translation\n",
    "\n",
    "## Bidirectionality\n",
    "A weakness in an ordinary RNN is being able to use only the observations previously seen. For example, a model for recognizing named entity recognition:\n",
    "\n",
    "* \"I can't believe that Teddy Roosevelt was your great grandfather!\"\n",
    "* \"I can't believe that Teddy bear is made out of chocolate!\"\n",
    "\n",
    "In this case, it's very difficult to tell if Teddy is the president or the bear if you're only going from the front to back. The model needs to be able to see what comes after the current token, to determine if it's the start of a name\n",
    "\n",
    "A **bidirectional** recurrent network processes the sequence left-to-right and right-to-left in parallel then combines these two representation such that at any point in the sequence, the model has knowledge of the tokens which can before AND after it\n",
    "\n",
    "# Limitations\n",
    "The layer's weights are SHARED across time-steps, so when a RNN makes a mistake, the error signal needs to back many time-steps\n",
    "\n",
    "Because the signal is being multiplied by the same weight many time over, if it's something like 0.5, after 10 time steps, the error signal has changed by a factor of $0.5^{10} = 0.00098$, drastically reducing the magnitude of the error signal: **vanishing gradient problem** A large weight can have a similar problem, but can be managed through gradient clipping.\n",
    "\n",
    "This makes training with an RNN rather slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d35f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
