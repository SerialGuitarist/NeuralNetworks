{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c4650c",
   "metadata": {},
   "source": [
    "Neural Machine Translation, or NMT, can be directly trained on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning\n",
    "\n",
    "# Encoder-Decoder Model\n",
    "The early models were Multilayer Perceptrons, limiting the input and output into a fixed-length sequence. Recurrent neural networks have greatly improved upon this.\n",
    "\n",
    "* An encoder neural network reads and encodes a source sentence into a fixed-length \"context\" vector\n",
    "* A decoder, usually an RNN, then outputs a translation from the encoded vector\n",
    "* The encoder and decoder pair for a language pair is jointly trained to maximize the probability of a correct translation given a source sentence\n",
    "\n",
    "## Attention\n",
    "* Has problems with long sequences of text\n",
    "    * Because of the fixed length internal representations\n",
    "* An attention mechanism allows the model to learn where to place attention on the input sequence\n",
    "    * A more efficient approach than to use a fixed size representation to capture all the semantic details\n",
    "    * Read the whole sentence or paragraph, then produce the translated words one at a time, each time focusing on a different part of the input sentence to gather the semantic details to produce the next word\n",
    "    \n",
    "## Problems\n",
    "* Scaling to larger vocabularies of words\n",
    "* Slow speed of training the models\n",
    "* Inference speed\n",
    "\n",
    "## Long Short-Term Memory\n",
    "### Sequence to Sequence\n",
    "* Sequence prediction involves forecasting the next value in a real valued sequence or outputting a class label for an input sequence\n",
    "* The hardest type of sequence prediction problems is using a sequence as an input and requiring a sequence prediction output; seq2seq\n",
    "* The length of the input and output may vary\n",
    "* An effective approach is the Encoder-Decoder LSTM, designed specifically for seq2seq problems\n",
    "### Encoder-Decoder LSTM\n",
    "* The innovation here is the fixed-size internal representation in the heart of the model, where the inputs are read to and the outputs are read from\n",
    "    * Called **sequence embedding** for this\n",
    "* Reversing the input sequence in the training and testing sets introduces many short term dependencies that makes the optimization problems simpler, leading to better performance\n",
    "* Also used on images with a CNN used to extract features from input images, which was read by a decoder LSTM\n",
    "### Applications\n",
    "* Machine Translation\n",
    "* Learning to execute and calculate the outcome of small programs\n",
    "* Image captioning\n",
    "* Conversational modeling (generating answers to textual questions)\n",
    "* Movement classification (generating a sequence of commands from a sequence of gestures)\n",
    "### Implementation\n",
    "* First, the input sequence is shown to the network one encoded character at a time\n",
    "* One or more LSTM layers can be used to implement the encoder model\n",
    "* The output of the model is a fixed-size vector that represents the internal representation of the input sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753c2c3",
   "metadata": {},
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(..., input_shape = (...)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f510e2",
   "metadata": {},
   "source": [
    "* The decoder now transforms the learned internal representation of  the input sequence into the correct output sequence\n",
    "* One or more LSTM layers can be used to implement the decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da510803",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2a02944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "\n",
    "path = pathlib.Path('deu.txt')\n",
    "text = path.read_text(encoding = 'utf-8')\n",
    "lines = text.splitlines()\n",
    "pairs = [line.split('\\t')[:2] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1658543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.', 'Geh.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6a9677",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = [inp for targ, inp in pairs]\n",
    "targ = [targ for targ, inp in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d678a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ohne Zweifel findet sich auf dieser Welt zu jedem Mann genau die richtige Ehefrau und umgekehrt; wenn man jedoch in Betracht zieht, dass ein Mensch nur Gelegenheit hat, mit ein paar hundert anderen bekannt zu sein, von denen ihm nur ein Dutzend oder weniger nahesteht, darunter höchstens ein oder zwei Freunde, dann erahnt man eingedenk der Millionen Einwohner dieser Welt\\xa0leicht, dass seit Erschaffung ebenderselben wohl noch nie der richtige Mann der richtigen Frau begegnet ist.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57881edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40e38f",
   "metadata": {},
   "source": [
    "Creating the tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd4a2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b988608d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None,), (None,)), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943aac76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Wohin haben Sie Tom geschickt?'\n",
      " b'Was ist deine Lieblingssportart im Sommer?'\n",
      " b'D\\xc3\\xbcrfte ich dich um einen gro\\xc3\\x9fen Gefallen bitten?'\n",
      " b'Gerechterweise muss man sagen, dass er mit seinem begrenzten Personal und Material sein Bestes geleistet hat.'\n",
      " b'Wer sind diese M\\xc3\\xa4nner?'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'Where did you send Tom?' b\"What's your favorite summer sport?\"\n",
      " b'Could I ask you a big favor?'\n",
      " b'To do him justice, he did his best with his limited men and supplies.'\n",
      " b'Who are these men?'], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbd224b",
   "metadata": {},
   "source": [
    "### Text Prepocessing\n",
    "One of the goals of a saveable models should be to do all the text processing to happen inside, taking in a tf.strings and returning tf.strings output\n",
    "#### Standardization\n",
    "It is it important to standardize the input text because the model is dealing with multilingual text with a limited vocabulary\n",
    "\n",
    "Firstly, we split accented Unicode characters and replce them with the ASCII equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "093876c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc2\\xbfTodav\\xc3\\xada est\\xc3\\xa1 en casa?'\n",
      "b'\\xc2\\xbfTodavi\\xcc\\x81a esta\\xcc\\x81 en casa?'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_text as tf_text\n",
    "\n",
    "example_text = tf.constant('¿Todavía está en casa?')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff60a8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Gr\\xc3\\xbc\\xc3\\x9f Gott m\\xc3\\xb6gen!'\n",
      "b'Gru\\xcc\\x88\\xc3\\x9f Gott mo\\xcc\\x88gen!'\n"
     ]
    }
   ],
   "source": [
    "example_text = tf.constant('Grüß Gott mögen!')\n",
    "\n",
    "print(example_text.numpy())\n",
    "print(tf_text.normalize_utf8(example_text, 'NFKD').numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69361f04",
   "metadata": {},
   "source": [
    "Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b53f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_lower_and_split_punct(text):\n",
    "    # split accented characters\n",
    "    text = tf_text.normalize_utf8(text)\n",
    "    text = tf.strings.lower(text)\n",
    "    # keep space, a to z, and select punctuation\n",
    "    text = tf.strings.regex_replace(text, '[^ a-z.?!,ä,ö,ü,ß]', '')\n",
    "    #replacing umlauts\n",
    "    text = tf.strings.regex_replace(text, 'ä', 'a')\n",
    "    text = tf.strings.regex_replace(text, 'ö', 'o')\n",
    "    text = tf.strings.regex_replace(text, 'ü', 'u')\n",
    "    text = tf.strings.regex_replace(text, 'ß', 'ss')\n",
    "    # add spaces around punctuation\n",
    "    text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n",
    "    # strip whitespace\n",
    "    text = tf.strings.strip(text)\n",
    "    \n",
    "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6c6fe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grüß Gott mögen!\n",
      "[START] gruss gott mogen ! [END]\n"
     ]
    }
   ],
   "source": [
    "print(example_text.numpy().decode())\n",
    "print(tf_lower_and_split_punct(example_text).numpy().decode())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217fb70",
   "metadata": {},
   "source": [
    "#### Text Vectorization\n",
    "This layer will handle the vocabulary extraction and conversion of input text to sequence of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18460e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "\n",
    "input_text_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72562f3",
   "metadata": {},
   "source": [
    "The experimental.preprocessing layers generally have an adapt method, which reads one epoch of training data, and initializes the layer based on the data\n",
    "\n",
    "Here, it determines the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb116fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_processor.adapt(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30dca5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " ',',\n",
       " 'ich',\n",
       " 'tom',\n",
       " '?',\n",
       " 'nicht',\n",
       " 'ist',\n",
       " 'das',\n",
       " 'du',\n",
       " 'sie',\n",
       " 'zu',\n",
       " 'es',\n",
       " 'die',\n",
       " 'er',\n",
       " 'der',\n",
       " 'hat',\n",
       " 'in',\n",
       " 'dass',\n",
       " 'ein',\n",
       " 'wir',\n",
       " 'habe',\n",
       " 'was',\n",
       " 'mir',\n",
       " '!',\n",
       " 'sich',\n",
       " 'auf',\n",
       " 'mit',\n",
       " 'war',\n",
       " 'wie',\n",
       " 'mich',\n",
       " 'eine',\n",
       " 'den',\n",
       " 'und',\n",
       " 'maria',\n",
       " 'ihr',\n",
       " 'haben',\n",
       " 'an',\n",
       " 'sind',\n",
       " 'kann',\n",
       " 'einen',\n",
       " 'bin',\n",
       " 'so',\n",
       " 'noch',\n",
       " 'von',\n",
       " 'fur',\n",
       " 'sehr']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 50 words from the vocabulary\n",
    "input_text_processor.get_vocabulary()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e56a7",
   "metadata": {},
   "source": [
    "And for the English TextVectorization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a056cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size\n",
    ")\n",
    "\n",
    "output_text_processor.adapt(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa136da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " '[START]',\n",
       " '[END]',\n",
       " '.',\n",
       " 'tom',\n",
       " 'to',\n",
       " 'you',\n",
       " 'the',\n",
       " 'i',\n",
       " '?',\n",
       " 'a',\n",
       " 'is',\n",
       " 'that',\n",
       " 'in',\n",
       " 'do',\n",
       " 'he',\n",
       " 'of',\n",
       " 'it',\n",
       " ',',\n",
       " 'was',\n",
       " 'have',\n",
       " 'this',\n",
       " 'me',\n",
       " 'dont',\n",
       " 'for',\n",
       " 'my',\n",
       " 'what',\n",
       " 'are',\n",
       " 'mary',\n",
       " 'we',\n",
       " 'your',\n",
       " 'his',\n",
       " 'be',\n",
       " 'im',\n",
       " 'and',\n",
       " 'on',\n",
       " 'with',\n",
       " 'like',\n",
       " 'know',\n",
       " 'want',\n",
       " 'not',\n",
       " 'at',\n",
       " 'she',\n",
       " 'can',\n",
       " 'did',\n",
       " 'how',\n",
       " 'has',\n",
       " 'very',\n",
       " 'think']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text_processor.get_vocabulary()[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba3efe",
   "metadata": {},
   "source": [
    "Now a batch of string can be converted into a batch of token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6960bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
       "array([[   2,  618,   39,   13,    7, 1202,    8,    3,    0,    0],\n",
       "       [   2,   25,   10,  119,    1,   61,  644,    8,    3,    0],\n",
       "       [   2, 1464,    6,   54,   60,   43,  520,  338,  676,    8]])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tokens = input_text_processor(example_input_batch)\n",
    "example_tokens[:3, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942365d9",
   "metadata": {},
   "source": [
    "get_vocabulary can be used to convert it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a0f1d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START] wohin haben sie tom geschickt ? [END]            '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_tokens[0].numpy()]\n",
    "' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daf460b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "    def __init__(self):\n",
    "        # Keep a cache of every axis-name seen\n",
    "        self.shapes = {}\n",
    "\n",
    "    def __call__(self, tensor, names, broadcast=False):\n",
    "        if not tf.executing_eagerly():\n",
    "            return\n",
    "\n",
    "        if isinstance(names, str):\n",
    "            names = (names,)\n",
    "\n",
    "        shape = tf.shape(tensor)\n",
    "        rank = tf.rank(tensor)\n",
    "\n",
    "        if rank != len(names):\n",
    "            raise ValueError(f'Rank mismatch:\\n'\n",
    "                           f'    found {rank}: {shape.numpy()}\\n'\n",
    "                           f'    expected {len(names)}: {names}\\n')\n",
    "\n",
    "        for i, name in enumerate(names):\n",
    "            if isinstance(name, int):\n",
    "                old_dim = name\n",
    "            else:\n",
    "                old_dim = self.shapes.get(name, None)\n",
    "            new_dim = shape[i]\n",
    "\n",
    "            if (broadcast and new_dim == 1):\n",
    "                continue\n",
    "\n",
    "            if old_dim is None:\n",
    "                # If the axis name is new, add its length to the cache.\n",
    "                self.shapes[name] = new_dim\n",
    "                continue\n",
    "\n",
    "            if new_dim != old_dim:\n",
    "                raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                                 f\"    found: {new_dim}\\n\"\n",
    "                                 f\"    expected: {old_dim}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf07eb5",
   "metadata": {},
   "source": [
    "## The Encoder/Decoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acc0cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for the model\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f1a724",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "1. Takes a list of token IDs (from the `input_text_processor`)\n",
    "2. Looks up an embedding vector for each token (using a `layers.Embedding`)\n",
    "3. Processes the embeddings nto a new sequence (using a `layers.GRU`)\n",
    "4. Returns:\n",
    "    * The processed sequence (to be passed into the attention head)\n",
    "    * The internal state (to be used passed into the decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        \n",
    "        # the embedding layer converts tokens into vectors\n",
    "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                                   embedding_dim)\n",
    "        \n",
    "        # the GRU RNN layer processes those vectors sequentially\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       # return the sequence and state\n",
    "                                       return_sequence=True,\n",
    "                                       return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "    def call(self, tokens, state=None):\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(tokens, ('batch', 's'))\n",
    "        \n",
    "        # 2. embedding layer looking up the embedding for each token\n",
    "        vectors = self.embedding(tokens)\n",
    "        shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "        \n",
    "        # 3. the GRU processing the embedding sequence\n",
    "        #    output shape: (batch, s, enc_units)\n",
    "        #    state shape:  (batch, enc_units)\n",
    "        output, state = self.gru(vectors, initial_state=state)\n",
    "        shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(state,  ('batch', 'enc_units'))\n",
    "        \n",
    "        # 4. return the new sequence and its state\n",
    "        return output, state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
