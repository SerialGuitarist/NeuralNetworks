{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3362a20f",
   "metadata": {},
   "source": [
    "# Generating Shakespear\n",
    "The following is the building of a character-based model that learns English on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de7c13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a87621f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a73c53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72e1833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "852135ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/serialguitarist/.keras/datasets/shakespeare.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cea23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# read, then decode for py2 compatibility\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of the text as the number of characters\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57044f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# first 250 characters\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b52f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# number of unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acf60aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1b4efb",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Vectorizing\n",
    "`preprocessing.StringLookup` layer can convert each character into numeric ID, but needs the text to be split into tokens first, hence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c2962bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z'], [b'\\n', b' ', b'!', b'$']]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz', '\\n !$']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bbc3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the StringLookup layer\n",
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab),\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9d6fbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65], [1, 2, 3, 4]]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b71601",
   "metadata": {},
   "source": [
    "To recover human readable text, use `preprocessing.StringLookup(..., invert=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fba8b397",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(),\n",
    "    invert=True,\n",
    "    mask_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8760e6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z'], [b'\\n', b' ', b'!', b'$']]>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf078d5",
   "metadata": {},
   "source": [
    "`tf.strings.reduce_join` can be used to join the characters back into strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b79c2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz', b'\\n !$'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d38274b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db865d3b",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "We're training the model to predict, given a sequence of text, what's the most likely next character\n",
    "\n",
    "The text needs to be divided into example sequences. For each input sequence, the target is the same sequence, but shifted one character to the right from the text. So we're breaking the text into chunks of `seq_length+1`\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices` can convert the text vector into a stream of character indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b402ba77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddc1a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4098c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7062ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b35ef",
   "metadata": {},
   "source": [
    "`batch` method will easily convert these individual characters to sequences of desired length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe798e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80734c99",
   "metadata": {},
   "source": [
    "To join them back into strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe61c69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df442a",
   "metadata": {},
   "source": [
    "A function that takes said sequences, and shifts and creates the input, label pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b37ed9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    \n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "972f3507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Factori', 'actorio')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target('Factorio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d42afe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fcb55065",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target:  b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataset.take(1):\n",
    "    print('Input: ', text_from_ids(X).numpy())\n",
    "    print('Target: ', text_from_ids(y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594bfb57",
   "metadata": {},
   "source": [
    "## Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fcabfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffled the entire sequence in memory.\n",
    "# Instead, it maintains a buffer in which it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87bffd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[15 60 59 ... 59 54  2]\n",
      " [62  2 53 ... 14 31 17]\n",
      " [ 2 45 54 ...  2 62 44]\n",
      " ...\n",
      " [36 48 59 ... 50  2 40]\n",
      " [ 2 45 40 ...  7  2 58]\n",
      " [44  2 57 ... 64  2 47]], shape=(64, 100), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataset.take(1):\n",
    "#     print('Input: ', text_from_ids(X).numpy())\n",
    "#     print('Target: ', text_from_ids(y).numpy())\n",
    "    print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f62d8",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "The GRU layer can be replaced by an LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5fcf980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids_from_chars.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "47fd9f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfa20390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# embeddimg dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc20b6",
   "metadata": {},
   "source": [
    "The internals of the model is being managed here like this to make it much simpler to generate text later on\n",
    "\n",
    "But keras.Sequential CAN be used to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "77350eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2383c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # vocab size must match the `StringLookup` layers\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e371f8dc",
   "metadata": {},
   "source": [
    "## Running the Model\n",
    "First, check the shape of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7f7b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d214edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  16896     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  multiple                  3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  67650     \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a39363",
   "metadata": {},
   "source": [
    "### Getting Actual Predictions\n",
    "Need to look at the output distribution to get the indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f447fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 64, 15,  9, 61, 62, 43, 50, 20,  7,  2, 42, 35, 64, 44,  8, 54,\n",
       "       31, 21, 34,  0, 54, 35, 45, 63, 33, 57, 35, 43, 23, 45, 47, 65, 34,\n",
       "       27, 25, 49, 17, 25, 34,  8, 17, 10, 63, 32, 18, 44, 36, 41, 40, 48,\n",
       "       14, 40, 43, 57, 34, 34, 64, 11, 37,  8, 42, 60, 20, 31,  6, 27, 27,\n",
       "       19, 55, 37, 21, 15, 31, 17, 51, 36, 19, 57, 22, 37,  1, 11, 59, 37,\n",
       "       33, 18, 41, 17, 60, 34, 61, 62, 13, 24, 51, 16, 15, 58, 48])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7964e06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'rother; and then the prince my brother and\\nthe princess my sister called my father father; and\\nso we'\n",
      "\n",
      "Next char predictions:\n",
      " b\"GyB.vwdkG, cVye-oRHU[UNK]oVfxTrVdJfhzUNLjDLU-D3xSEeWbaiAadrUUy:X-cuGR'NNFpXHBRDlWFrIX\\n:tXTEbDuUvw?KlCBsi\"\n"
     ]
    }
   ],
   "source": [
    "# decoding the indices\n",
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next char predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90a4ec",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now it's a standard classification problem: Given the previous RNN state, and the input this time step, predict the class of the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9ca6e",
   "metadata": {},
   "source": [
    "### Optimizer and a Loss function\n",
    "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss works here because it is applied across the last dimension of the predictions\n",
    "\n",
    "Because the model returns logits, `from_logits` flag needs to be true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d1a2ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9314d279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Shape:  (64, 100, 66) # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.188983\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print('Prediction Shape: ', example_batch_predictions.shape, '# (batch_size, sequence_length, vocab_size)')\n",
    "print('Mean loss:        ', mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523b067b",
   "metadata": {},
   "source": [
    "A newly initialized model should be unsure of itself, giving everything more or less similar values. To make sure of this, we can check the loss raised to the power of e is more or less similar to the vocabulary size. Being much higher means the model is sure o fits wrong answers, and is badly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70249a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.95567"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "303f8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91833190",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "`tf.keras.classbacks.ModelCheckpoint` ensures checkpoints are saved during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9e4f5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where the choickpoints will be saved\n",
    "checkpoint_dir = './models/training_checkpoints'\n",
    "\n",
    "# name of the choickpoint file\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c33df1c",
   "metadata": {},
   "source": [
    "### Training itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a5b8774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 13s 70ms/step - loss: 1.1733\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 1.1339\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 13s 69ms/step - loss: 1.0906\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 13s 70ms/step - loss: 1.0457\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 13s 70ms/step - loss: 0.9972\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.9471\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 13s 69ms/step - loss: 0.8955\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.8417\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.7902\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.7397\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 13s 70ms/step - loss: 0.6939\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.6534\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.6165\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5849\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5587\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5374\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5181\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.5004\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.4877\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.4773\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 12s 66ms/step - loss: 0.4670\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 0.4610\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 13s 73ms/step - loss: 0.4492\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 13s 70ms/step - loss: 0.4475\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.4378 0s - loss: 0\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.4327\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 13s 71ms/step - loss: 0.4279\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.4283\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 13s 70ms/step - loss: 0.4294\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.4235\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c4263",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "The easiest method is to run it in a loop, and keep track of the internal state as you do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a9b4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "037c332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8168ec",
   "metadata": {},
   "source": [
    "Now we just run it in a loop to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "92bcc465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "By rote of him, the time is very neither.\n",
      "\n",
      "COMINIUS:\n",
      "O, teephess; gast thou, the news is spraw fond corks?\n",
      "\n",
      "MENENIUS:\n",
      "Come, contrad, to the garden for the mind hate the word:\n",
      "Go to the maid; my loving Montague,\n",
      "When now, to assive from hence hath sorrow\n",
      "Or eye, with known too light and princely gaze;\n",
      "Show 'tis shed post: he shall renown lady. But, as I said.\n",
      "\n",
      "Second Citizen:\n",
      "Construe till a Richard kill'd him;\n",
      "I call'd thee thus partly to bed.\n",
      "Sir, that he wounded hence,\n",
      "For Warwick and what wit innocent mine,\n",
      "With repetition of Hortensio.\n",
      "\n",
      "ROMEO:\n",
      "Take honour on him.\n",
      "\n",
      "First Servingman:\n",
      "Nay, come to me, Tyrrel, soon do your bless\n",
      "Unto my nothing. Women are miseries!\n",
      "O, well she's yet; the doint of York as bonn\n",
      "gently conscience, and tell them at are already, and he\n",
      "is even: while the term of thine.\n",
      "\n",
      "POLIXENES:\n",
      "I have let fitting for his wander: she\n",
      "welcome home: he would not say how true--\n",
      "Widow, tell me where setting a week. We have ask,\n",
      "When true gant tyrannous lambs that treason you \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.812549114227295\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0522e68",
   "metadata": {},
   "source": [
    "## Exporting the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "45056169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7fcb723d6520>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
      "\n",
      "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e7a7eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "How now, fair sire! Now the greater place of star,\n",
      "Having no other abed, and cannot woe wangs draw-\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3840ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
