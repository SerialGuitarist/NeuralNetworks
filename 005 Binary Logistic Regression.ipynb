{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da201a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1 = 0, weight_regularizer_l2 = 0, \n",
    "                 bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        #the lambda values of the regularization methods\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):        \n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on the weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += self.weights * 2 \\\n",
    "                            * self.weight_regularizer_l2\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on the weights\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += self.biases * 2 \\\n",
    "                            * self.bias_regularizer_l2\n",
    "        \n",
    "        # gradient on values to pass further back\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "class Layer_Dropout:\n",
    "    \n",
    "    # init\n",
    "    def __init__(self, rate):\n",
    "        # store keep rate, we invert it as the dropout rate instead\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # save input values\n",
    "        self.inputs = inputs\n",
    "        # generate and save scald mask for calculating p.d. later\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                                             size = inputs.shape) / \\\n",
    "                                             self.rate\n",
    "        # apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "# Rectified Linear Unit Activation Function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        #create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # calculate jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # calculate sample wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        \n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # calculate only when the factor is greater than 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * \\\n",
    "                                           layer.weights)\n",
    "        \n",
    "        # L1 regularization - bias\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.biases))\n",
    "        \n",
    "        # L1 regularization - bias\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * \\\n",
    "                                           layer.weights)\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    #inheriting from the base Loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred will come from the neural network\n",
    "        # y_true will come from the training set\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # means scalar class values have been passed\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # one hot encoded values have been passed\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Backwards pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        # number of labels in every sample\n",
    "        # we'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # if labels are sparse, turn them into one-hot vector\n",
    "        # in case the shape is as [3, 0, 2] etc etc per sample\n",
    "        # we turn them into one-hot vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        # with a larger number of batches, it's all summed together\n",
    "        # in the dot product, and some will be given more importance\n",
    "        # than others when we don't normaize\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # creates activation and loss function objects inside\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # set the output\n",
    "        self.output = self.activation.output\n",
    "        # calculate and return the loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #if labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "            \n",
    "        # copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1.0, decay = 0., momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.momentum:\n",
    "            \n",
    "            #if layer does not contain momentum arrays\n",
    "            # create them and fill with zeroes\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                \n",
    "                # if there is no momentum array for weights\n",
    "                # the array doesn't exist for the biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "            # build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        # otherwise vanillae SGD\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "        \n",
    "        # update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1., decay = 0., epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                    (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                    (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                layer.weight_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                                layer.bias_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start wit h1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        # get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            weight_momentums_corrected / \\\n",
    "                            (np.sqrt(weight_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            bias_momentums_corrected / \\\n",
    "                            (np.sqrt(bias_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49556d8c",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression\n",
    "Up until now we've been using the type of model where the output is 1 for one or more of the output classes, and the rest being 0. A Binary Logistic Regression is the same, but with only two classes, specifying stuff like cat vs dog, or cat vs not cat, or indoors vs outdoors, etc etc. The crux is sigmoid activation is used instead of softmax, and binary entropy-error is used rather than categorical cross-entropy for loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e53c1",
   "metadata": {},
   "source": [
    "### Sigmoid Activation Function\n",
    "Squishes the output from \\]-inf; +inf\\[ to \\]0; 1\\[\n",
    "\n",
    "Derivative boils down to sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a08318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid:\n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # save input and calculate/save utput\n",
    "        # of the function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # derivative\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf7d85",
   "metadata": {},
   "source": [
    "### Binary Cross-Entropy Loss\n",
    "Uses negative log like categorical cross-entropy, but calculates the logs of both correct and incorrect classes separately, rather than just the one correct class\n",
    "\n",
    "$L_{i,j} = (y_{i,j})(-log(\\hat{y}_{i,j})) + (1 - y_{i,j})(-log(1-\\hat{y}_{i,y}))$\n",
    "\n",
    "The derivative boils down to (where y hat is the predicted value):\n",
    "\n",
    "$-(\\frac{y_{i,j}}{\\hat{y}_{i,j}}-\\frac{1-y_{i,j}}{1-\\hat{y}_{i,j}})$\n",
    "\n",
    "Because the model works with batches, the error will be calculated as a mean of the errors of each atomic loss, we will have to chain together the mean function's derivative, which ends up being $\\frac{1}{J}$, where $J$ is the number of samples per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45313511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-004facb07402>:1: RuntimeWarning: divide by zero encountered in log\n",
      "  print(np.mean([5, 2, 4, np.log(0)]))\n"
     ]
    }
   ],
   "source": [
    "print(np.mean([5, 2, 4, np.log(0)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0aa85",
   "metadata": {},
   "source": [
    "Because of the above behaviour, we need to clip to predictions (the output of the sigmoid function) with a very small amount from both sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "031f89da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_BinaryCrossentropy(Loss):\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        # clip data to prevent division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # calculate smaple-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + \n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis = -1)\n",
    "        # axis of -1 means the last, or the innermost dimension\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        #number of outputs in every sample\n",
    "        # first sample is used to count them\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # clip data to prevent division by 0\n",
    "        # clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        \n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef37c348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.500, loss: 0.693, data_loss: 0.693, reg_loss: 0.000, lr: 0.001\n",
      "epoch: 100, acc: 0.630, loss: 0.675, data_loss: 0.673, reg_loss: 0.001, lr: 0.0009999505024501287\n",
      "epoch: 200, acc: 0.625, loss: 0.670, data_loss: 0.668, reg_loss: 0.002, lr: 0.0009999005098992651\n",
      "epoch: 300, acc: 0.645, loss: 0.666, data_loss: 0.663, reg_loss: 0.003, lr: 0.000999850522346909\n",
      "epoch: 400, acc: 0.650, loss: 0.661, data_loss: 0.657, reg_loss: 0.004, lr: 0.0009998005397923115\n",
      "epoch: 500, acc: 0.675, loss: 0.651, data_loss: 0.644, reg_loss: 0.007, lr: 0.0009997505622347225\n",
      "epoch: 600, acc: 0.720, loss: 0.637, data_loss: 0.626, reg_loss: 0.011, lr: 0.0009997005896733929\n",
      "epoch: 700, acc: 0.770, loss: 0.622, data_loss: 0.604, reg_loss: 0.018, lr: 0.0009996506221075735\n",
      "epoch: 800, acc: 0.775, loss: 0.605, data_loss: 0.579, reg_loss: 0.026, lr: 0.000999600659536515\n",
      "epoch: 900, acc: 0.775, loss: 0.590, data_loss: 0.556, reg_loss: 0.035, lr: 0.0009995507019594694\n",
      "epoch: 1000, acc: 0.785, loss: 0.579, data_loss: 0.536, reg_loss: 0.042, lr: 0.000999500749375687\n",
      "epoch: 1100, acc: 0.790, loss: 0.568, data_loss: 0.520, reg_loss: 0.049, lr: 0.0009994508017844195\n",
      "epoch: 1200, acc: 0.790, loss: 0.559, data_loss: 0.505, reg_loss: 0.054, lr: 0.0009994008591849186\n",
      "epoch: 1300, acc: 0.795, loss: 0.550, data_loss: 0.491, reg_loss: 0.059, lr: 0.0009993509215764362\n",
      "epoch: 1400, acc: 0.795, loss: 0.541, data_loss: 0.478, reg_loss: 0.063, lr: 0.0009993009889582235\n",
      "epoch: 1500, acc: 0.810, loss: 0.531, data_loss: 0.464, reg_loss: 0.067, lr: 0.0009992510613295335\n",
      "epoch: 1600, acc: 0.810, loss: 0.523, data_loss: 0.452, reg_loss: 0.071, lr: 0.0009992011386896176\n",
      "epoch: 1700, acc: 0.820, loss: 0.516, data_loss: 0.441, reg_loss: 0.075, lr: 0.0009991512210377285\n",
      "epoch: 1800, acc: 0.825, loss: 0.509, data_loss: 0.431, reg_loss: 0.078, lr: 0.0009991013083731183\n",
      "epoch: 1900, acc: 0.825, loss: 0.503, data_loss: 0.422, reg_loss: 0.081, lr: 0.0009990514006950402\n",
      "epoch: 2000, acc: 0.825, loss: 0.497, data_loss: 0.413, reg_loss: 0.083, lr: 0.0009990014980027463\n",
      "epoch: 2100, acc: 0.835, loss: 0.491, data_loss: 0.405, reg_loss: 0.086, lr: 0.0009989516002954898\n",
      "epoch: 2200, acc: 0.840, loss: 0.486, data_loss: 0.398, reg_loss: 0.088, lr: 0.000998901707572524\n",
      "epoch: 2300, acc: 0.845, loss: 0.481, data_loss: 0.392, reg_loss: 0.089, lr: 0.0009988518198331018\n",
      "epoch: 2400, acc: 0.850, loss: 0.476, data_loss: 0.386, reg_loss: 0.091, lr: 0.0009988019370764769\n",
      "epoch: 2500, acc: 0.855, loss: 0.472, data_loss: 0.380, reg_loss: 0.092, lr: 0.0009987520593019025\n",
      "epoch: 2600, acc: 0.865, loss: 0.468, data_loss: 0.375, reg_loss: 0.093, lr: 0.000998702186508632\n",
      "epoch: 2700, acc: 0.865, loss: 0.464, data_loss: 0.370, reg_loss: 0.094, lr: 0.00099865231869592\n",
      "epoch: 2800, acc: 0.865, loss: 0.460, data_loss: 0.365, reg_loss: 0.095, lr: 0.0009986024558630198\n",
      "epoch: 2900, acc: 0.870, loss: 0.457, data_loss: 0.361, reg_loss: 0.096, lr: 0.0009985525980091856\n",
      "epoch: 3000, acc: 0.870, loss: 0.454, data_loss: 0.357, reg_loss: 0.097, lr: 0.000998502745133672\n",
      "epoch: 3100, acc: 0.875, loss: 0.450, data_loss: 0.353, reg_loss: 0.097, lr: 0.0009984528972357331\n",
      "epoch: 3200, acc: 0.875, loss: 0.447, data_loss: 0.349, reg_loss: 0.098, lr: 0.0009984030543146237\n",
      "epoch: 3300, acc: 0.880, loss: 0.444, data_loss: 0.346, reg_loss: 0.098, lr: 0.0009983532163695982\n",
      "epoch: 3400, acc: 0.885, loss: 0.441, data_loss: 0.343, reg_loss: 0.098, lr: 0.000998303383399912\n",
      "epoch: 3500, acc: 0.890, loss: 0.438, data_loss: 0.340, reg_loss: 0.099, lr: 0.0009982535554048193\n",
      "epoch: 3600, acc: 0.890, loss: 0.435, data_loss: 0.337, reg_loss: 0.099, lr: 0.000998203732383576\n",
      "epoch: 3700, acc: 0.890, loss: 0.433, data_loss: 0.334, reg_loss: 0.099, lr: 0.0009981539143354365\n",
      "epoch: 3800, acc: 0.890, loss: 0.430, data_loss: 0.331, reg_loss: 0.099, lr: 0.0009981041012596574\n",
      "epoch: 3900, acc: 0.890, loss: 0.427, data_loss: 0.328, reg_loss: 0.099, lr: 0.0009980542931554933\n",
      "epoch: 4000, acc: 0.900, loss: 0.419, data_loss: 0.321, reg_loss: 0.098, lr: 0.0009980044900222008\n",
      "epoch: 4100, acc: 0.900, loss: 0.414, data_loss: 0.316, reg_loss: 0.098, lr: 0.0009979546918590348\n",
      "epoch: 4200, acc: 0.900, loss: 0.412, data_loss: 0.313, reg_loss: 0.099, lr: 0.0009979048986652524\n",
      "epoch: 4300, acc: 0.900, loss: 0.409, data_loss: 0.310, reg_loss: 0.099, lr: 0.000997855110440109\n",
      "epoch: 4400, acc: 0.900, loss: 0.407, data_loss: 0.307, reg_loss: 0.099, lr: 0.0009978053271828614\n",
      "epoch: 4500, acc: 0.905, loss: 0.404, data_loss: 0.305, reg_loss: 0.100, lr: 0.0009977555488927658\n",
      "epoch: 4600, acc: 0.905, loss: 0.402, data_loss: 0.303, reg_loss: 0.100, lr: 0.000997705775569079\n",
      "epoch: 4700, acc: 0.905, loss: 0.400, data_loss: 0.300, reg_loss: 0.100, lr: 0.0009976560072110577\n",
      "epoch: 4800, acc: 0.910, loss: 0.396, data_loss: 0.296, reg_loss: 0.100, lr: 0.0009976062438179587\n",
      "epoch: 4900, acc: 0.910, loss: 0.393, data_loss: 0.292, reg_loss: 0.101, lr: 0.0009975564853890394\n",
      "epoch: 5000, acc: 0.910, loss: 0.391, data_loss: 0.288, reg_loss: 0.103, lr: 0.000997506731923557\n",
      "epoch: 5100, acc: 0.910, loss: 0.389, data_loss: 0.285, reg_loss: 0.104, lr: 0.0009974569834207687\n",
      "epoch: 5200, acc: 0.915, loss: 0.386, data_loss: 0.282, reg_loss: 0.105, lr: 0.0009974072398799322\n",
      "epoch: 5300, acc: 0.915, loss: 0.384, data_loss: 0.278, reg_loss: 0.106, lr: 0.0009973575013003048\n",
      "epoch: 5400, acc: 0.915, loss: 0.382, data_loss: 0.276, reg_loss: 0.107, lr: 0.0009973077676811448\n",
      "epoch: 5500, acc: 0.915, loss: 0.381, data_loss: 0.273, reg_loss: 0.107, lr: 0.00099725803902171\n",
      "epoch: 5600, acc: 0.915, loss: 0.379, data_loss: 0.271, reg_loss: 0.108, lr: 0.0009972083153212581\n",
      "epoch: 5700, acc: 0.915, loss: 0.377, data_loss: 0.269, reg_loss: 0.108, lr: 0.000997158596579048\n",
      "epoch: 5800, acc: 0.915, loss: 0.375, data_loss: 0.267, reg_loss: 0.108, lr: 0.0009971088827943377\n",
      "epoch: 5900, acc: 0.915, loss: 0.373, data_loss: 0.265, reg_loss: 0.108, lr: 0.0009970591739663862\n",
      "epoch: 6000, acc: 0.915, loss: 0.372, data_loss: 0.264, reg_loss: 0.108, lr: 0.0009970094700944517\n",
      "epoch: 6100, acc: 0.915, loss: 0.370, data_loss: 0.262, reg_loss: 0.108, lr: 0.0009969597711777935\n",
      "epoch: 6200, acc: 0.915, loss: 0.368, data_loss: 0.261, reg_loss: 0.107, lr: 0.00099691007721567\n",
      "epoch: 6300, acc: 0.920, loss: 0.363, data_loss: 0.256, reg_loss: 0.108, lr: 0.000996860388207341\n",
      "epoch: 6400, acc: 0.925, loss: 0.360, data_loss: 0.253, reg_loss: 0.107, lr: 0.0009968107041520655\n",
      "epoch: 6500, acc: 0.925, loss: 0.356, data_loss: 0.249, reg_loss: 0.108, lr: 0.000996761025049103\n",
      "epoch: 6600, acc: 0.935, loss: 0.352, data_loss: 0.245, reg_loss: 0.108, lr: 0.000996711350897713\n",
      "epoch: 6700, acc: 0.935, loss: 0.350, data_loss: 0.243, reg_loss: 0.107, lr: 0.0009966616816971556\n",
      "epoch: 6800, acc: 0.935, loss: 0.348, data_loss: 0.241, reg_loss: 0.107, lr: 0.00099661201744669\n",
      "epoch: 6900, acc: 0.935, loss: 0.347, data_loss: 0.239, reg_loss: 0.107, lr: 0.0009965623581455767\n",
      "epoch: 7000, acc: 0.940, loss: 0.345, data_loss: 0.238, reg_loss: 0.107, lr: 0.000996512703793076\n",
      "epoch: 7100, acc: 0.935, loss: 0.343, data_loss: 0.236, reg_loss: 0.107, lr: 0.0009964630543884481\n",
      "epoch: 7200, acc: 0.940, loss: 0.341, data_loss: 0.234, reg_loss: 0.107, lr: 0.0009964134099309536\n",
      "epoch: 7300, acc: 0.935, loss: 0.340, data_loss: 0.233, reg_loss: 0.107, lr: 0.0009963637704198528\n",
      "epoch: 7400, acc: 0.940, loss: 0.338, data_loss: 0.231, reg_loss: 0.107, lr: 0.0009963141358544066\n",
      "epoch: 7500, acc: 0.940, loss: 0.337, data_loss: 0.230, reg_loss: 0.107, lr: 0.000996264506233876\n",
      "epoch: 7600, acc: 0.940, loss: 0.335, data_loss: 0.229, reg_loss: 0.106, lr: 0.0009962148815575223\n",
      "epoch: 7700, acc: 0.940, loss: 0.334, data_loss: 0.227, reg_loss: 0.106, lr: 0.000996165261824606\n",
      "epoch: 7800, acc: 0.940, loss: 0.332, data_loss: 0.226, reg_loss: 0.106, lr: 0.0009961156470343895\n",
      "epoch: 7900, acc: 0.940, loss: 0.331, data_loss: 0.225, reg_loss: 0.106, lr: 0.0009960660371861334\n",
      "epoch: 8000, acc: 0.940, loss: 0.329, data_loss: 0.224, reg_loss: 0.105, lr: 0.0009960164322790998\n",
      "epoch: 8100, acc: 0.940, loss: 0.326, data_loss: 0.222, reg_loss: 0.104, lr: 0.0009959668323125503\n",
      "epoch: 8200, acc: 0.940, loss: 0.324, data_loss: 0.220, reg_loss: 0.104, lr: 0.000995917237285747\n",
      "epoch: 8300, acc: 0.940, loss: 0.322, data_loss: 0.219, reg_loss: 0.103, lr: 0.000995867647197952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8400, acc: 0.940, loss: 0.321, data_loss: 0.218, reg_loss: 0.103, lr: 0.0009958180620484277\n",
      "epoch: 8500, acc: 0.940, loss: 0.319, data_loss: 0.216, reg_loss: 0.103, lr: 0.0009957684818364362\n",
      "epoch: 8600, acc: 0.940, loss: 0.318, data_loss: 0.215, reg_loss: 0.102, lr: 0.0009957189065612402\n",
      "epoch: 8700, acc: 0.945, loss: 0.316, data_loss: 0.214, reg_loss: 0.102, lr: 0.000995669336222102\n",
      "epoch: 8800, acc: 0.945, loss: 0.315, data_loss: 0.213, reg_loss: 0.102, lr: 0.000995619770818285\n",
      "epoch: 8900, acc: 0.945, loss: 0.314, data_loss: 0.212, reg_loss: 0.101, lr: 0.0009955702103490519\n",
      "epoch: 9000, acc: 0.945, loss: 0.312, data_loss: 0.211, reg_loss: 0.101, lr: 0.000995520654813666\n",
      "epoch: 9100, acc: 0.945, loss: 0.311, data_loss: 0.210, reg_loss: 0.101, lr: 0.0009954711042113903\n",
      "epoch: 9200, acc: 0.945, loss: 0.310, data_loss: 0.210, reg_loss: 0.100, lr: 0.0009954215585414883\n",
      "epoch: 9300, acc: 0.945, loss: 0.309, data_loss: 0.209, reg_loss: 0.100, lr: 0.000995372017803224\n",
      "epoch: 9400, acc: 0.945, loss: 0.307, data_loss: 0.208, reg_loss: 0.099, lr: 0.0009953224819958604\n",
      "epoch: 9500, acc: 0.945, loss: 0.306, data_loss: 0.207, reg_loss: 0.099, lr: 0.000995272951118662\n",
      "epoch: 9600, acc: 0.945, loss: 0.305, data_loss: 0.206, reg_loss: 0.098, lr: 0.0009952234251708924\n",
      "epoch: 9700, acc: 0.945, loss: 0.304, data_loss: 0.206, reg_loss: 0.098, lr: 0.000995173904151816\n",
      "epoch: 9800, acc: 0.945, loss: 0.303, data_loss: 0.205, reg_loss: 0.098, lr: 0.0009951243880606966\n",
      "epoch: 9900, acc: 0.945, loss: 0.301, data_loss: 0.204, reg_loss: 0.097, lr: 0.0009950748768967994\n",
      "epoch: 10000, acc: 0.945, loss: 0.300, data_loss: 0.204, reg_loss: 0.097, lr: 0.0009950253706593885\n",
      "validation, acc: 0.920, loss: 0.237\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 2)\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# dropout layer\n",
    "# dropout1 = Layer_Dropout(0.1) # ie. dropout rate of 90%\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "# sigmoid activation\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "# create loss function\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(decay=5e-7)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "    \n",
    "    # forward pass through second activation function\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    \n",
    "    # regularizatin penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2)\n",
    "    \n",
    "    # overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # calculate accuracy from output of activation2 and targets\n",
    "    # part in the brackets returns a binary mask - array consisting\n",
    "    # of True/False values; multiplying it by 1 changes it into array\n",
    "    # of 1s and 0s\n",
    "    prediction = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(prediction == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "# validate the model\n",
    "\n",
    "# creating dataset\n",
    "X_test, y_test = spiral_data(samples = 100, classes = 2)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward through second activation function\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "# calculate accuracy from output of activation2 and targets\n",
    "# part in the brackets returns a binary mask - array consisting\n",
    "# of True/False values; multiplying it by 1 changes it into array\n",
    "# of 1s and 0s\n",
    "prediction = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d2c86f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.500, loss: 0.693 (data_loss: 0.693, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100, acc: 0.630, loss: 0.674 (data_loss: 0.673, reg_loss: 0.001), lr: 0.0009999505024501287\n",
      "epoch: 200, acc: 0.625, loss: 0.669 (data_loss: 0.668, reg_loss: 0.001), lr: 0.0009999005098992651\n",
      "epoch: 300, acc: 0.645, loss: 0.665 (data_loss: 0.663, reg_loss: 0.002), lr: 0.000999850522346909\n",
      "epoch: 400, acc: 0.650, loss: 0.659 (data_loss: 0.657, reg_loss: 0.002), lr: 0.0009998005397923115\n",
      "epoch: 500, acc: 0.675, loss: 0.648 (data_loss: 0.644, reg_loss: 0.004), lr: 0.0009997505622347225\n",
      "epoch: 600, acc: 0.720, loss: 0.632 (data_loss: 0.626, reg_loss: 0.006), lr: 0.0009997005896733929\n",
      "epoch: 700, acc: 0.770, loss: 0.614 (data_loss: 0.604, reg_loss: 0.010), lr: 0.0009996506221075735\n",
      "epoch: 800, acc: 0.775, loss: 0.594 (data_loss: 0.579, reg_loss: 0.015), lr: 0.000999600659536515\n",
      "epoch: 900, acc: 0.775, loss: 0.575 (data_loss: 0.556, reg_loss: 0.020), lr: 0.0009995507019594694\n",
      "epoch: 1000, acc: 0.785, loss: 0.560 (data_loss: 0.536, reg_loss: 0.024), lr: 0.000999500749375687\n",
      "epoch: 1100, acc: 0.790, loss: 0.547 (data_loss: 0.520, reg_loss: 0.028), lr: 0.0009994508017844195\n",
      "epoch: 1200, acc: 0.790, loss: 0.536 (data_loss: 0.505, reg_loss: 0.031), lr: 0.0009994008591849186\n",
      "epoch: 1300, acc: 0.795, loss: 0.524 (data_loss: 0.491, reg_loss: 0.033), lr: 0.0009993509215764362\n",
      "epoch: 1400, acc: 0.795, loss: 0.513 (data_loss: 0.478, reg_loss: 0.036), lr: 0.0009993009889582235\n",
      "epoch: 1500, acc: 0.810, loss: 0.502 (data_loss: 0.464, reg_loss: 0.038), lr: 0.0009992510613295335\n",
      "epoch: 1600, acc: 0.810, loss: 0.493 (data_loss: 0.452, reg_loss: 0.040), lr: 0.0009992011386896176\n",
      "epoch: 1700, acc: 0.820, loss: 0.484 (data_loss: 0.441, reg_loss: 0.043), lr: 0.0009991512210377285\n",
      "epoch: 1800, acc: 0.825, loss: 0.475 (data_loss: 0.431, reg_loss: 0.044), lr: 0.0009991013083731183\n",
      "epoch: 1900, acc: 0.825, loss: 0.468 (data_loss: 0.422, reg_loss: 0.046), lr: 0.0009990514006950402\n",
      "epoch: 2000, acc: 0.825, loss: 0.461 (data_loss: 0.413, reg_loss: 0.048), lr: 0.0009990014980027463\n",
      "epoch: 2100, acc: 0.835, loss: 0.454 (data_loss: 0.405, reg_loss: 0.049), lr: 0.0009989516002954898\n",
      "epoch: 2200, acc: 0.840, loss: 0.448 (data_loss: 0.398, reg_loss: 0.050), lr: 0.000998901707572524\n",
      "epoch: 2300, acc: 0.845, loss: 0.443 (data_loss: 0.392, reg_loss: 0.051), lr: 0.0009988518198331018\n",
      "epoch: 2400, acc: 0.850, loss: 0.438 (data_loss: 0.386, reg_loss: 0.052), lr: 0.0009988019370764769\n",
      "epoch: 2500, acc: 0.855, loss: 0.433 (data_loss: 0.380, reg_loss: 0.053), lr: 0.0009987520593019025\n",
      "epoch: 2600, acc: 0.865, loss: 0.428 (data_loss: 0.375, reg_loss: 0.053), lr: 0.000998702186508632\n",
      "epoch: 2700, acc: 0.865, loss: 0.424 (data_loss: 0.370, reg_loss: 0.054), lr: 0.00099865231869592\n",
      "epoch: 2800, acc: 0.865, loss: 0.420 (data_loss: 0.365, reg_loss: 0.055), lr: 0.0009986024558630198\n",
      "epoch: 2900, acc: 0.870, loss: 0.416 (data_loss: 0.361, reg_loss: 0.055), lr: 0.0009985525980091856\n",
      "epoch: 3000, acc: 0.870, loss: 0.412 (data_loss: 0.357, reg_loss: 0.055), lr: 0.000998502745133672\n",
      "epoch: 3100, acc: 0.875, loss: 0.409 (data_loss: 0.353, reg_loss: 0.056), lr: 0.0009984528972357331\n",
      "epoch: 3200, acc: 0.875, loss: 0.405 (data_loss: 0.349, reg_loss: 0.056), lr: 0.0009984030543146237\n",
      "epoch: 3300, acc: 0.880, loss: 0.402 (data_loss: 0.346, reg_loss: 0.056), lr: 0.0009983532163695982\n",
      "epoch: 3400, acc: 0.885, loss: 0.399 (data_loss: 0.343, reg_loss: 0.057), lr: 0.000998303383399912\n",
      "epoch: 3500, acc: 0.890, loss: 0.396 (data_loss: 0.340, reg_loss: 0.057), lr: 0.0009982535554048193\n",
      "epoch: 3600, acc: 0.890, loss: 0.394 (data_loss: 0.337, reg_loss: 0.057), lr: 0.000998203732383576\n",
      "epoch: 3700, acc: 0.890, loss: 0.391 (data_loss: 0.334, reg_loss: 0.057), lr: 0.0009981539143354365\n",
      "epoch: 3800, acc: 0.890, loss: 0.388 (data_loss: 0.331, reg_loss: 0.057), lr: 0.0009981041012596574\n",
      "epoch: 3900, acc: 0.890, loss: 0.385 (data_loss: 0.328, reg_loss: 0.057), lr: 0.0009980542931554933\n",
      "epoch: 4000, acc: 0.900, loss: 0.378 (data_loss: 0.321, reg_loss: 0.057), lr: 0.0009980044900222008\n",
      "epoch: 4100, acc: 0.900, loss: 0.373 (data_loss: 0.316, reg_loss: 0.057), lr: 0.0009979546918590348\n",
      "epoch: 4200, acc: 0.900, loss: 0.370 (data_loss: 0.313, reg_loss: 0.057), lr: 0.0009979048986652524\n",
      "epoch: 4300, acc: 0.900, loss: 0.368 (data_loss: 0.310, reg_loss: 0.058), lr: 0.000997855110440109\n",
      "epoch: 4400, acc: 0.900, loss: 0.365 (data_loss: 0.307, reg_loss: 0.058), lr: 0.0009978053271828614\n",
      "epoch: 4500, acc: 0.905, loss: 0.363 (data_loss: 0.305, reg_loss: 0.058), lr: 0.0009977555488927658\n",
      "epoch: 4600, acc: 0.905, loss: 0.361 (data_loss: 0.303, reg_loss: 0.058), lr: 0.000997705775569079\n",
      "epoch: 4700, acc: 0.905, loss: 0.358 (data_loss: 0.300, reg_loss: 0.058), lr: 0.0009976560072110577\n",
      "epoch: 4800, acc: 0.910, loss: 0.354 (data_loss: 0.296, reg_loss: 0.058), lr: 0.0009976062438179587\n",
      "epoch: 4900, acc: 0.910, loss: 0.351 (data_loss: 0.292, reg_loss: 0.059), lr: 0.0009975564853890394\n",
      "epoch: 5000, acc: 0.910, loss: 0.348 (data_loss: 0.288, reg_loss: 0.060), lr: 0.000997506731923557\n",
      "epoch: 5100, acc: 0.910, loss: 0.345 (data_loss: 0.285, reg_loss: 0.060), lr: 0.0009974569834207687\n",
      "epoch: 5200, acc: 0.915, loss: 0.342 (data_loss: 0.282, reg_loss: 0.061), lr: 0.0009974072398799322\n",
      "epoch: 5300, acc: 0.915, loss: 0.340 (data_loss: 0.278, reg_loss: 0.061), lr: 0.0009973575013003048\n",
      "epoch: 5400, acc: 0.915, loss: 0.337 (data_loss: 0.276, reg_loss: 0.062), lr: 0.0009973077676811448\n",
      "epoch: 5500, acc: 0.915, loss: 0.335 (data_loss: 0.273, reg_loss: 0.062), lr: 0.00099725803902171\n",
      "epoch: 5600, acc: 0.915, loss: 0.333 (data_loss: 0.271, reg_loss: 0.062), lr: 0.0009972083153212581\n",
      "epoch: 5700, acc: 0.915, loss: 0.331 (data_loss: 0.269, reg_loss: 0.062), lr: 0.000997158596579048\n",
      "epoch: 5800, acc: 0.915, loss: 0.329 (data_loss: 0.267, reg_loss: 0.062), lr: 0.0009971088827943377\n",
      "epoch: 5900, acc: 0.915, loss: 0.328 (data_loss: 0.265, reg_loss: 0.062), lr: 0.0009970591739663862\n",
      "epoch: 6000, acc: 0.915, loss: 0.326 (data_loss: 0.264, reg_loss: 0.062), lr: 0.0009970094700944517\n",
      "epoch: 6100, acc: 0.915, loss: 0.324 (data_loss: 0.262, reg_loss: 0.062), lr: 0.0009969597711777935\n",
      "epoch: 6200, acc: 0.915, loss: 0.322 (data_loss: 0.261, reg_loss: 0.062), lr: 0.00099691007721567\n",
      "epoch: 6300, acc: 0.920, loss: 0.318 (data_loss: 0.256, reg_loss: 0.062), lr: 0.000996860388207341\n",
      "epoch: 6400, acc: 0.925, loss: 0.315 (data_loss: 0.253, reg_loss: 0.062), lr: 0.0009968107041520655\n",
      "epoch: 6500, acc: 0.925, loss: 0.310 (data_loss: 0.249, reg_loss: 0.062), lr: 0.000996761025049103\n",
      "epoch: 6600, acc: 0.935, loss: 0.307 (data_loss: 0.245, reg_loss: 0.062), lr: 0.000996711350897713\n",
      "epoch: 6700, acc: 0.935, loss: 0.304 (data_loss: 0.243, reg_loss: 0.062), lr: 0.0009966616816971556\n",
      "epoch: 6800, acc: 0.935, loss: 0.303 (data_loss: 0.241, reg_loss: 0.062), lr: 0.00099661201744669\n",
      "epoch: 6900, acc: 0.935, loss: 0.301 (data_loss: 0.239, reg_loss: 0.062), lr: 0.0009965623581455767\n",
      "epoch: 7000, acc: 0.940, loss: 0.299 (data_loss: 0.238, reg_loss: 0.062), lr: 0.000996512703793076\n",
      "epoch: 7100, acc: 0.935, loss: 0.298 (data_loss: 0.236, reg_loss: 0.062), lr: 0.0009964630543884481\n",
      "epoch: 7200, acc: 0.940, loss: 0.296 (data_loss: 0.234, reg_loss: 0.061), lr: 0.0009964134099309536\n",
      "epoch: 7300, acc: 0.935, loss: 0.294 (data_loss: 0.233, reg_loss: 0.061), lr: 0.0009963637704198528\n",
      "epoch: 7400, acc: 0.940, loss: 0.293 (data_loss: 0.231, reg_loss: 0.061), lr: 0.0009963141358544066\n",
      "epoch: 7500, acc: 0.940, loss: 0.291 (data_loss: 0.230, reg_loss: 0.061), lr: 0.000996264506233876\n",
      "epoch: 7600, acc: 0.940, loss: 0.290 (data_loss: 0.229, reg_loss: 0.061), lr: 0.0009962148815575223\n",
      "epoch: 7700, acc: 0.940, loss: 0.289 (data_loss: 0.227, reg_loss: 0.061), lr: 0.000996165261824606\n",
      "epoch: 7800, acc: 0.940, loss: 0.287 (data_loss: 0.226, reg_loss: 0.061), lr: 0.0009961156470343895\n",
      "epoch: 7900, acc: 0.940, loss: 0.286 (data_loss: 0.225, reg_loss: 0.061), lr: 0.0009960660371861334\n",
      "epoch: 8000, acc: 0.940, loss: 0.285 (data_loss: 0.224, reg_loss: 0.061), lr: 0.0009960164322790998\n",
      "epoch: 8100, acc: 0.940, loss: 0.282 (data_loss: 0.222, reg_loss: 0.060), lr: 0.0009959668323125503\n",
      "epoch: 8200, acc: 0.940, loss: 0.280 (data_loss: 0.220, reg_loss: 0.060), lr: 0.000995917237285747\n",
      "epoch: 8300, acc: 0.940, loss: 0.279 (data_loss: 0.219, reg_loss: 0.060), lr: 0.000995867647197952\n",
      "epoch: 8400, acc: 0.940, loss: 0.277 (data_loss: 0.218, reg_loss: 0.060), lr: 0.0009958180620484277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8500, acc: 0.940, loss: 0.276 (data_loss: 0.216, reg_loss: 0.060), lr: 0.0009957684818364362\n",
      "epoch: 8600, acc: 0.940, loss: 0.275 (data_loss: 0.215, reg_loss: 0.059), lr: 0.0009957189065612402\n",
      "epoch: 8700, acc: 0.945, loss: 0.273 (data_loss: 0.214, reg_loss: 0.059), lr: 0.000995669336222102\n",
      "epoch: 8800, acc: 0.945, loss: 0.272 (data_loss: 0.213, reg_loss: 0.059), lr: 0.000995619770818285\n",
      "epoch: 8900, acc: 0.945, loss: 0.271 (data_loss: 0.212, reg_loss: 0.059), lr: 0.0009955702103490519\n",
      "epoch: 9000, acc: 0.945, loss: 0.270 (data_loss: 0.211, reg_loss: 0.059), lr: 0.000995520654813666\n",
      "epoch: 9100, acc: 0.945, loss: 0.269 (data_loss: 0.210, reg_loss: 0.059), lr: 0.0009954711042113903\n",
      "epoch: 9200, acc: 0.945, loss: 0.268 (data_loss: 0.210, reg_loss: 0.058), lr: 0.0009954215585414883\n",
      "epoch: 9300, acc: 0.945, loss: 0.267 (data_loss: 0.209, reg_loss: 0.058), lr: 0.000995372017803224\n",
      "epoch: 9400, acc: 0.945, loss: 0.266 (data_loss: 0.208, reg_loss: 0.058), lr: 0.0009953224819958604\n",
      "epoch: 9500, acc: 0.945, loss: 0.265 (data_loss: 0.207, reg_loss: 0.058), lr: 0.000995272951118662\n",
      "epoch: 9600, acc: 0.945, loss: 0.264 (data_loss: 0.206, reg_loss: 0.057), lr: 0.0009952234251708924\n",
      "epoch: 9700, acc: 0.945, loss: 0.263 (data_loss: 0.206, reg_loss: 0.057), lr: 0.000995173904151816\n",
      "epoch: 9800, acc: 0.945, loss: 0.262 (data_loss: 0.205, reg_loss: 0.057), lr: 0.0009951243880606966\n",
      "epoch: 9900, acc: 0.945, loss: 0.261 (data_loss: 0.204, reg_loss: 0.057), lr: 0.0009950748768967994\n",
      "epoch: 10000, acc: 0.945, loss: 0.260 (data_loss: 0.204, reg_loss: 0.056), lr: 0.0009950253706593885\n",
      "validation, acc: 0.920, loss: 0.237\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                   np.sum(layer.weights *\n",
    "                                          layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                   np.sum(layer.biases *\n",
    "                                          layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Sigmoid activation:\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(decay=5e-7)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Part in the brackets returns a binary mask - array consisting\n",
    "    # of True/False values, multiplying it by 1 changes it into array\n",
    "    # of 1s and 0s\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions==y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, '+\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate the data loss\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Part in the brackets returns a binary mask - array consisting of\n",
    "# True/False values, multiplying it by 1 changes it into array\n",
    "# of 1s and 0s\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d6cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33dfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
