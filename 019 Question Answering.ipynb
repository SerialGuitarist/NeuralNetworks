{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1c249f6",
   "metadata": {},
   "source": [
    "Question asnwering involves memory: facts stored in memory, and question refers back to past information\n",
    "\n",
    "How do models for sequential processing memorize things?\n",
    "\n",
    "Two approaches:\n",
    "* Flat memory: RNN and LSTMs\n",
    "* Responsive memory: end-to-end memory networks\n",
    "\n",
    "Language is inherently sequential and contextual, and often addresses long range dependencies. Question Answering is an especially memory intensive task: answering a question on the basis of a preceding sequence of facts\n",
    "\n",
    "Given a large dataset of hand annotated questions linked to supporting facts (where the facts to answer a question can be found in only a single sentence), how can we retrieve the necessary information for answering the question? Since it is not known which specific fragment of which specific sentence contains the answer to the question, the model must store all of the sentences (the story)\n",
    "\n",
    "The Facebook BABI dataset contains stories like these:\n",
    "\n",
    "    1 Mary moved to the bathroom.\n",
    "    2 John went to the hallway.\n",
    "    3 Where is Mary?     bathroom    1\n",
    "    4 Daniel went back to the hallway.\n",
    "    5 Sandra moved to the garden.\n",
    "    6 Where is Daniel?     hallway    4\n",
    "    7 John moved to the office.\n",
    "    8 Sandra journeyed to the bathroom.\n",
    "    9 Where is Daniel?     hallway    4\n",
    "    10 Mary moved to the hallway.\n",
    "    11 Daniel travelled to the office.\n",
    "    12 Where is Daniel?     office    11\n",
    "    13 John went back to the garden.\n",
    "    14 John moved to the bedroom.\n",
    "    15 Where is Sandra?     bathroom    8\n",
    "\n",
    "Two conditions for the model to explore here is:\n",
    "* Only using a question and the supporting fact\n",
    "* Using all facts in a story, including non-relevant ones for answering the particular question\n",
    "\n",
    "## Data Vectorization\n",
    "We need three vectors:\n",
    "* A list holding all facts as vectors\n",
    "* A list of vectorized questions\n",
    "* A list of labels: word indices referring to the word that is the answer to a question\n",
    "\n",
    "First of all, we need to tokenize the stories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a7e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(training_data, test_data):\n",
    "    f = open(training_data, \"r\")\n",
    "    text = []\n",
    "    \n",
    "    for line in f:\n",
    "        m = re.match(\"^\\d+\\s([^\\.]+)[\\.].*\", line.rstrip())\n",
    "        if m:\n",
    "            text.append(m.group(1))\n",
    "        else:\n",
    "            m = re.match(\"^\\d+\\s([^\\?]+)[\\?]\\s\\t([^\\t]+)\",line.rstrip())\n",
    "            if m:\n",
    "                text.append(m.group(1) + ' ' + m.group(2))\n",
    "    f.close()\n",
    "    \n",
    "    f = open(test_data, \"r\")\n",
    "    for line in f:\n",
    "        m=re.match(\"^\\d+\\s([^\\.]+)[\\.].*\",line.rstrip())                    \n",
    "        if m:\n",
    "            text.append(m.group(1))\n",
    "        else:\n",
    "            m=re.match(\"^\\d+\\s([^\\?]+)[\\?].*\",line.rstrip())                \n",
    "            if m:\n",
    "                text.append(m.group(1))\n",
    "    f.close()\n",
    "    \n",
    "    vocabulary = set([word for word in text])\n",
    "    max_words = len(vocabulary)\n",
    "    \n",
    "    print(vocabulary)\n",
    "                      \n",
    "    tokenizer = Tokenizer(num_words=max_words, char_level=False, split=' ')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    return tokenizer, max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20ed51b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Daniel journeyed to the hallway', 'Sandra travelled to the kitchen', 'Sandra travelled to the bathroom', 'Where is Sandra office', 'Mary went to the kitchen', 'John journeyed to the hallway', 'John went back to the kitchen', 'Sandra went to the bedroom', 'Daniel went to the office', 'Daniel moved to the kitchen', 'Mary moved to the bedroom', 'John journeyed to the office', 'John went to the garden', 'Sandra went back to the hallway', 'John travelled to the bathroom', 'Sandra went back to the bedroom', 'John went to the office', 'John went to the bathroom', 'Daniel moved to the office', 'Where is Mary kitchen', 'Daniel travelled to the kitchen', 'Where is Sandra bathroom', 'Where is John bedroom', 'Mary went to the bedroom', 'Mary went back to the bedroom', 'John journeyed to the garden', 'Daniel journeyed to the kitchen', 'Daniel went back to the bedroom', 'Where is Daniel garden', 'Where is Daniel', 'Where is John', 'John went back to the office', 'Mary journeyed to the hallway', 'John went back to the bathroom', 'John moved to the garden', 'Mary moved to the garden', 'Sandra went to the office', 'Sandra moved to the garden', 'Sandra journeyed to the hallway', 'John went back to the garden', 'Where is Mary', 'Mary moved to the kitchen', 'Daniel went back to the garden', 'John went back to the bedroom', 'Mary travelled to the office', 'Daniel went back to the kitchen', 'Sandra moved to the bedroom', 'Daniel moved to the garden', 'Sandra travelled to the hallway', 'Where is Daniel bedroom', 'Where is John garden', 'Mary went back to the bathroom', 'Daniel journeyed to the office', 'Sandra went to the kitchen', 'John went to the hallway', 'Mary moved to the office', 'Mary journeyed to the kitchen', 'Daniel moved to the hallway', 'Sandra went back to the bathroom', 'Mary went back to the hallway', 'John went back to the hallway', 'Mary journeyed to the bedroom', 'Mary journeyed to the office', 'John moved to the kitchen', 'John moved to the bedroom', 'Sandra travelled to the office', 'Sandra moved to the office', 'Daniel travelled to the bedroom', 'Daniel travelled to the bathroom', 'Mary moved to the bathroom', 'Mary moved to the hallway', 'Sandra journeyed to the bedroom', 'Sandra travelled to the bedroom', 'Mary went back to the kitchen', 'John journeyed to the bathroom', 'Daniel went to the bathroom', 'Mary journeyed to the garden', 'Daniel journeyed to the bathroom', 'Sandra went to the bathroom', 'Where is John office', 'Daniel went to the hallway', 'Daniel journeyed to the garden', 'Where is Mary bathroom', 'Mary went back to the garden', 'Mary went to the garden', 'Daniel went back to the bathroom', 'John travelled to the bedroom', 'Mary travelled to the bedroom', 'Daniel went back to the hallway', 'John went to the kitchen', 'Mary travelled to the kitchen', 'Mary went back to the office', 'Sandra moved to the kitchen', 'Where is Mary garden', 'Daniel moved to the bathroom', 'John travelled to the hallway', 'John travelled to the kitchen', 'Daniel travelled to the garden', 'John went to the bedroom', 'Where is Sandra', 'Where is Mary office', 'John moved to the bathroom', 'Where is John bathroom', 'Sandra moved to the hallway', 'Sandra went back to the garden', 'Mary journeyed to the bathroom', 'Where is Sandra bedroom', 'Sandra went to the garden', 'Sandra went back to the office', 'John journeyed to the kitchen', 'Where is Daniel hallway', 'Sandra journeyed to the bathroom', 'Daniel travelled to the hallway', 'Where is Mary bedroom', 'Sandra travelled to the garden', 'Mary travelled to the hallway', 'Mary went to the office', 'Sandra journeyed to the kitchen', 'Where is Daniel office', 'John travelled to the garden', 'John travelled to the office', 'John moved to the office', 'Sandra went to the hallway', 'Daniel went to the garden', 'Sandra journeyed to the office', 'Sandra went back to the kitchen', 'Where is Sandra garden', 'Where is Daniel bathroom', 'Where is John kitchen', 'Where is Mary hallway', 'Sandra journeyed to the garden', 'John moved to the hallway', 'Where is Daniel kitchen', 'Mary went to the hallway', 'Daniel moved to the bedroom', 'Where is Sandra hallway', 'Where is John hallway', 'Mary went to the bathroom', 'Daniel went to the bedroom', 'John journeyed to the bedroom', 'Daniel travelled to the office', 'Daniel journeyed to the bedroom', 'Daniel went to the kitchen', 'Mary travelled to the garden', 'Mary travelled to the bathroom', 'Where is Sandra kitchen', 'Sandra moved to the bathroom', 'Daniel went back to the office'}\n"
     ]
    }
   ],
   "source": [
    "train = 'datasets/tasksv11/en/qa1_single-supporting-fact_train.txt'\n",
    "test = 'datasets/tasksv11/en/qa1_single-supporting-fact_test.txt'\n",
    "tokenizer, max_words = create_tokenizer(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14604ac8",
   "metadata": {},
   "source": [
    "The entire list of facts will be concatenated into one big string, and that will be tokenized\n",
    "\n",
    "A boolean flag will determine if the etnrie story will be kept or just the one holding the answer to the question\n",
    "\n",
    "So after the relevant facts for answering the question have been determined, it's vectorized, along with the questions and answers, and the results are appended to desgined output variables for the entire training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af93e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def vectorize(s, tokenizer):\n",
    "    vector = tokenizer.texts_to_sequences([s])\n",
    "    return vector[0]\n",
    "\n",
    "def process_stories(filename, tokenizer, max_story_len, vocab_size, use_context=False):\n",
    "    f = open(filename, 'r')\n",
    "    X = []\n",
    "    Q = []\n",
    "    y = []\n",
    "    n_questions = 0\n",
    "    \n",
    "    for line in f:\n",
    "        m = re.match(\"^(\\d+)\\s(.+)\\.\",line.rstrip())\n",
    "        if m:\n",
    "            if int(m.group(1)) == 1:\n",
    "                # if this is the first index\n",
    "                # then this is a new story\n",
    "                story = {}\n",
    "            else:\n",
    "                # otherwise, add the fact to the index\n",
    "                # of the number of the fact\n",
    "                story[int(m.group(1))] = m.group(2)\n",
    "        else:\n",
    "            # else, read a question\n",
    "            m = re.match(\"^\\d+\\s(.+)\\?\\s\\t([^\\t]+)\\t(.+)\", line.rstrip())\n",
    "            if m:\n",
    "                question = m.group(1)\n",
    "                answer = m.group(2)\n",
    "                answer_ids = [int(x) for x in m.group(3).split(\" \")]\n",
    "                if use_context == False:\n",
    "                    facts=' '.join([story[id] for id in answer_ids])\n",
    "                    vectorized_fact = vectorize(facts, tokenizer)\n",
    "                else:\n",
    "                    vectorized_fact = vectorize(' '.join(story.values()), tokenizer)\n",
    "                vectorized_question = vectorize(question, tokenizer)\n",
    "                vectorized_answer = vectorize(answer, tokenizer)\n",
    "                X.append(vectorized_fact)\n",
    "                Q.append(vectorized_question)\n",
    "                answer = np.zeros(vocab_size)\n",
    "                answer[vectorized_answer[0]] = 1\n",
    "                y.append(answer)\n",
    "    f.close()\n",
    "    \n",
    "    X = pad_sequences(X, maxlen = max_story_len)\n",
    "    Q = pad_sequences(Q, maxlen = max_query_len)\n",
    "    \n",
    "    return np.array(X), np.array(Q), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "676cc86c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-51e2ed7ecd91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_stories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_story_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-d87dec6a467a>\u001b[0m in \u001b[0;36mprocess_stories\u001b[0;34m(filename, tokenizer, max_story_len, vocab_size, use_context)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0manswer_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_context\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mfacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0mvectorized_fact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d87dec6a467a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0manswer_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_context\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mfacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mID\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mID\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manswer_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0mvectorized_fact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfacts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "process_stories(train, tokenizer, max_story_len = 100, vocab_size = max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57108b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
