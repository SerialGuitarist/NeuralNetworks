{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1cdf66d",
   "metadata": {},
   "source": [
    "### Partial Derivative of a Single Neuron\n",
    "A single neuron with 3 inputs can be written in function form as \n",
    "\n",
    "ReLU(sum( mul(x_0, w_0) , mul(x_1, w_1), mul(x_2, w_2), b))\n",
    "\n",
    "The partial derivative of this output with respect to s a specific weight (let's say w_0) will be calculated through the chain rule\n",
    "\n",
    "The ReLU function, which outputs z, if z is more than 0, has the derivative 1 if z is more than 0, and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7d1400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drelu_dz: 1.0\n",
      "1.0 1.0 1.0 1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "### Foward Pass\n",
    "x = [1.0, -2.0, 3.0] # input values from the 3 neurons in the preceding layer\n",
    "w = [-3.0, -1.0, 2.0] # weights\n",
    "b = 1.0 # bias\n",
    "\n",
    "# multiplying inputs by weights\n",
    "xw0 = x[0] * w[0]\n",
    "xw1 = x[1] * w[1]\n",
    "xw2 = x[2] * w[2]\n",
    "\n",
    "# adding the weighted inputs and a bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activatin function\n",
    "relu = max(z, 0)\n",
    "\n",
    "### Backwards pass\n",
    "\n",
    "# the derivate from the next layer\n",
    "dvalue = 1.0\n",
    "\n",
    "# derivative of the ReLU and the chain rule\n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "print(\"drelu_dz:\", drelu_dz)\n",
    "\n",
    "# partial derivatives of the multiplication\n",
    "# they're all 1s because the derivative of\n",
    "# a summation function is always 1\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "\n",
    "# their chain rules\n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2\n",
    "drelu_db   = drelu_dz * dsum_db\n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db)\n",
    "\n",
    "\n",
    "# partial derivates of the multiplications\n",
    "# for a multiplication function, f(x, y) = xy\n",
    "# the partial derivative of the function w.r.t\n",
    "# either of the multiplicants is always the other\n",
    "# multiplicant, hence:\n",
    "dmul_dx0 = w[0]\n",
    "dmul_dx1 = w[1]\n",
    "dmul_dx2 = w[2]\n",
    "\n",
    "dmul_dw0 = x[0]\n",
    "dmul_dw1 = x[1]\n",
    "dmul_dw2 = x[2]\n",
    "\n",
    "#their chain rules\n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0\n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0\n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1\n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1\n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2\n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2\n",
    "\n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f990e87",
   "metadata": {},
   "source": [
    "#### Simplifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e66e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "drelu_dz = dvalue * (1. if z > 0 else 0.)\n",
    "\n",
    "dsum_dxw0 = 1\n",
    "drelu_0 = drelu_dz * dsum_dxw0\n",
    "\n",
    "dmul_dz0 = w[0]\n",
    "drelu_dz0 = drelu_dxw0 * dmul_dx0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843d85e0",
   "metadata": {},
   "source": [
    "Can be simplified down to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5dd69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "drelu_dz0 = dvalue * (1. if z > 0 else 0.) * w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9f057",
   "metadata": {},
   "source": [
    "All together, these final partial derivatives make up our gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a223882",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = [drelu_dx0, drelu_dx1, drelu_dx2]\n",
    "dw = [drelu_dw0, drelu_dw1, drelu_dw2]\n",
    "db = drelu_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04344ae9",
   "metadata": {},
   "source": [
    "The dw and db gradients will be used to update our weights and biases in an intelligent manner to decrease the output of the loss function. This can be done by adding a negative fraction of this gradient to our weights and biases.\n",
    "\n",
    "The dx gradient will be passed down to the previous layer, so it can do the same calculations and alter its weights and biases accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84589f09",
   "metadata": {},
   "source": [
    "### Partial Derivative of a Layer of Neurons\n",
    "Each neuron in the layer gives its output to every neuron in the next layer, so every neuron in the next layer will be giving each neuron in this layer a partial derivative with respect to its output. So each neuron in this layer receives an array of partial derivates from the next layer. Since each value in this array represents how much the loss function changes per unit increase in the output of this individual neuron, we sum this entire array and work with that.\n",
    "\n",
    "The partial derivates w.r.t an input, the corresponding weight is multiplied with the chaining partial derivative. And likewise, the partial derivatives of w.r.t a weight, the corresponding input is multiplied with the chaining partial derivative.\n",
    "\n",
    "In the following example, we have 3 neurons, but with 4 inputs, so we have 3 sets of 4 weights, preparing the partial derivatives to send to the previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cfcdb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44  -0.137 -0.07   1.37 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# passeed in gradient from the next layer\n",
    "dvalues = np.array([[1., 1., 1.,]])\n",
    "# is actually an array of arrays, because of the batches\n",
    "# but for now, we'll only be concerned with the 0th element\n",
    "\n",
    "# 3 sets of weights, one for each neuron\n",
    "# 4 inputs, thus 4 weights\n",
    "# recall the weights are kept transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -.027, 0.17, 0.87]]).T\n",
    "\n",
    "# sum weights of given input and multiply\n",
    "# by the passed in gradient for this neuron\n",
    "dx0 = sum(weights[0] * dvalues[0])\n",
    "dx1 = sum(weights[1] * dvalues[0])\n",
    "dx2 = sum(weights[2] * dvalues[0])\n",
    "dx3 = sum(weights[3] * dvalues[0])\n",
    "\n",
    "# the gradient of the neuron with respect to the inputs\n",
    "dinputs = np.array([dx0, dx1, dx2, dx3])\n",
    "# it will be passed along into the previous layer\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761d506",
   "metadata": {},
   "source": [
    "The same operations, but done through the numpy dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b42402c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44  -0.137 -0.07   1.37 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# passeed in gradient from the next layer\n",
    "dvalues = np.array([[1., 1., 1.,]])\n",
    "# is actually an array of arrays, because of the batches\n",
    "# but for now, we'll only be concerned with the 0th element\n",
    "\n",
    "# 3 sets of weights, one for each neuron\n",
    "# 4 inputs, thus 4 weights\n",
    "# recall the weights are kept transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -.027, 0.17, 0.87]]).T\n",
    "\n",
    "# sum weights of given input and multiply\n",
    "# by the passed in gradient for this neuron\n",
    "dinputs = np.dot(dvalues[0], weights.T)\n",
    "\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea8fb26",
   "metadata": {},
   "source": [
    "And finally, handling a batch of samples (more items in the dvalues array). Nothing needs to be changed in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab837ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44  -0.137 -0.07   1.37 ]\n",
      " [ 0.88  -0.274 -0.14   2.74 ]\n",
      " [ 1.32  -0.411 -0.21   4.11 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# passeed in gradient from the next layer\n",
    "dvalues = np.array([[1., 1., 1.,],\n",
    "                    [2., 2., 2.,],\n",
    "                    [3., 3., 3.,]])\n",
    "\n",
    "# 3 sets of weights, one for each neuron\n",
    "# 4 inputs, thus 4 weights\n",
    "# recall the weights are kept transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -.027, 0.17, 0.87]]).T\n",
    "\n",
    "# sum weights of given input and multiply\n",
    "# by the passed in gradient for this neuron\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e32172",
   "metadata": {},
   "source": [
    "#### Gradients with respect to weights\n",
    "Very similar to calculating w.r.t inputs. Just chain the inputs instead of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e0ecea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# passeed in gradient from the next layer\n",
    "dvalues = np.array([[1., 1., 1.,],\n",
    "                    [2., 2., 2.,],\n",
    "                    [3., 3., 3.,]])\n",
    "\n",
    "# 3 3 sets of inputs - samples\n",
    "inputs = np.array([[1, 2, 3, 2.5],\n",
    "                   [2., 5., -1., 2],\n",
    "                   [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "# sum weights of given input and multiply\n",
    "# by the passed in gradient for this neuron\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "\n",
    "\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61266a07",
   "metadata": {},
   "source": [
    "The output shape matches the weights, rather than the dvalues, becase we summed the inputs for each weight then multiplied them by the input gradient. The dweights will now be used to update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45ef05",
   "metadata": {},
   "source": [
    "#### Gradients with respect to biases\n",
    "Since the biases are only used in the sum operation, and the derivative of the sum is always equal to 1, which is multiplied to the chaining gradient, we just have to sum the list of gradients, to combine the gradient from all the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8de5808b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# passeed in gradient from the next layer\n",
    "dvalues = np.array([[1., 1., 1.,],\n",
    "                    [2., 2., 2.,],\n",
    "                    [3., 3., 3.,]])\n",
    "\n",
    "# one bias for each neuron\n",
    "# biases are the row vector with a shape (1, neurons)\n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# dbiases - sum values, over the samples (first axis),\n",
    "# and keepdims since this by default will produce a plain list\n",
    "dbiases = np.sum(dvalues, axis = 0, keepdims =True)\n",
    "\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1aaed",
   "metadata": {},
   "source": [
    "#### Gradient with respect to the ReLU\n",
    "When chained, basically means if the input into the ReLU is more than 0, return the chaining gradient so far, and else return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74246f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example layer output\n",
    "z = np.array([[1, 2, -3 ,-4],\n",
    "              [2, -7, -1, 3],\n",
    "              [-1, 2, 5, -1]])\n",
    "\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "# ReLU activations's derivative\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z > 0] = 1\n",
    "\n",
    "print(drelu)\n",
    "\n",
    "# its chain rule\n",
    "drelu *= dvalues\n",
    "\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cae7e",
   "metadata": {},
   "source": [
    "##### Simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d964ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example layer output\n",
    "z = np.array([[1, 2, -3 ,-4],\n",
    "              [2, -7, -1, 3],\n",
    "              [-1, 2, 5, -1]])\n",
    "\n",
    "dvalues = np.array([[1, 2, 3, 4],\n",
    "                    [5, 6, 7, 8],\n",
    "                    [9, 10, 11, 12]])\n",
    "\n",
    "drelu = dvalues.copy()\n",
    "drelu[z <= 0] = 0\n",
    "\n",
    "print(drelu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07daf8c3",
   "metadata": {},
   "source": [
    "#### Put together into our class\n",
    "To calculate the partial derivate with respect to the weight, our Dense Layer needs to remember the input, we make the appropriate alteration\n",
    "\n",
    "Meanwhile, for backpropagation, the partial derivative will be passed back, all the way from the loss function\n",
    "\n",
    "A layer receives the partial derivative, and uses it to calculate the partial derivative of the loss function with respect to the weight, and using that gradient to update the weights, and with respect to the inputs, and passes it along backwards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fafbab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # dvalues is the partial derivate of the loss function\n",
    "        # with respect to the next functin in the sequence, in\n",
    "        # this case being the ReLU\n",
    "        \n",
    "        # every neuron in the next layer is giving us a partial derivative\n",
    "        # for every input from every neuron in this layer, so we're receiving\n",
    "        # a 2D array, but also because of the batches, we're receiving a 3D array\n",
    "        \n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # gradient on values to pass further back\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# same for the ReLU class\n",
    "# Rectified Linear Unit Activation Function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # since we need to modify the original variable,\n",
    "        # we make a copy so we keep the original around\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3616bfaa",
   "metadata": {},
   "source": [
    "### Categorical Cross-Entropy Loss Derivative\n",
    "The partial derivative of the loss with respect to the predicted value, y, boils down to the negative real answer divided by the predicted answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4750327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    #inheriting from the base Loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred will come from the neural network\n",
    "        # y_true will come from the training set\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # means scalar class values have been passed\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # one hot encoded values have been passed\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Backwards pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        # number of labels in every sample\n",
    "        # we'll use the first sample to count them\n",
    "        label = len(dvalues[0])\n",
    "        \n",
    "        # if labels are sparse, turn them into one-hot vector\n",
    "        # in case the shape is as [3, 0, 2] etc etc per sample\n",
    "        # we turn them into one-hot vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "            \n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        # with a larger number of batches, it's all summed together\n",
    "        # in the dot product, and some will be given more importance\n",
    "        # than others when we don't normaize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f0fc7",
   "metadata": {},
   "source": [
    "### Softmax Activation Derivative\n",
    "We need to find out the impact of one array (the output of the neural network) on another array (the exponentiated, then normalized outputs). We'll calculate the **Jacobian Matrix**\n",
    "\n",
    "Because the softmax formula is the exponentiated output divided by the sum of all the exponentiated outputs, we use the derivative of the division formula\n",
    "\n",
    "The math boils down to the partial derivative of the softmax of the j-th output of the i-th batch with respect to the k-th output of the i-th batch is {\n",
    "\n",
    "    S_i,j * ( 1 - S_i,k)    when j == k\n",
    "    \n",
    "    S_i,j * ( 0 - S_i,k)    when j != k\n",
    "} which simplifies as {\n",
    "\n",
    "    S_i,j * δ_i,j - S_i,j * S_i,k\n",
    "    \n",
    "} where δ_i,j is the **Kronecker Delta** {\n",
    "\n",
    "    1    when i == j\n",
    "    \n",
    "    0    when i != j\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "694df331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7]\n",
      " [0.1]\n",
      " [0.2]]\n"
     ]
    }
   ],
   "source": [
    "# example sample\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "\n",
    "# and shape it as a list of samples\n",
    "import numpy as np\n",
    "\n",
    "softmax_output = np.array(softmax_output).reshape(-1, 1)\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec51a1ab",
   "metadata": {},
   "source": [
    "The left side of the equation is multiplied by the Kronecker delta, which equals 1 when both inputs are equal and 0 otherwise, which is the behaviour the np.eye produces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9885b7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(np.eye(softmax_output.shape[0]))\n",
    "\n",
    "print(softmax_output * np.eye(softmax_output.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94370421",
   "metadata": {},
   "source": [
    "The np.diagflat method does the same but faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "276431be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7 0.  0. ]\n",
      " [0.  0.1 0. ]\n",
      " [0.  0.  0.2]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diagflat(softmax_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7284e9",
   "metadata": {},
   "source": [
    "The right side of the equation is S_i,j * S_i,k, where we're multiplying the Softmaax outputs, iterating over the j and k indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee83489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49 0.07 0.14]\n",
      " [0.07 0.01 0.02]\n",
      " [0.14 0.02 0.04]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b956f",
   "metadata": {},
   "source": [
    "And the final output, the **Jacobian Matrix** is the difference between these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "55df514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20999999 -0.07       -0.14      ]\n",
      " [-0.07        0.09       -0.02      ]\n",
      " [-0.14       -0.02        0.16      ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.diagflat(softmax_output) - np.dot(softmax_output, softmax_output.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c875e93c",
   "metadata": {},
   "source": [
    "This is the partial derivative of all the combination of both input vectors. The partial derivative of every output of the Softmax function with respect to each input, because each input affects eac output in the normalization process.\n",
    "\n",
    "This gives every neuron an array of partial derivatives, and so, as before, we need to sum this and chain together the partial derivative from the Loss function. Both operations can be done through a single dot product. It'll take the row from the Jacobian Matrix and multiply it by the corresponding value from the loss function's gradient, then sum it, returning a single value. And with the samples taken into account, this will return a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82f8dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        #create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # calculate jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # calculate sample wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689230ff",
   "metadata": {},
   "source": [
    "The partial derivative of the Categorical Cross-Entrpoy Loss and Softma activation being calculated together can result in the math being significantly simplified down to the subtractin of the predicted and the ground truth values\n",
    "\n",
    "To code this, because the softmax and the loss functions are in two different classes, we need a new class that includes both of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0cde9e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # creates activation and loss function objects inside\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # forward pass\n",
    "    def foward(self, inputs, y_true):\n",
    "        # output layer's activation function\n",
    "        self.activation.foward(inputs)\n",
    "        # set the output\n",
    "        self.output = self.activation.output\n",
    "        # calculate and return the loss value\n",
    "        return self.loss.calculate(self.ouput, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #if labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "            \n",
    "        # copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8fc4bd",
   "metadata": {},
   "source": [
    "### Everything So Far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b29bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        #create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # calculate jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # calculate sample wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    #inheriting from the base Loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred will come from the neural network\n",
    "        # y_true will come from the training set\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # means scalar class values have been passed\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # one hot encoded values have been passed\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Backwards pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        # number of labels in every sample\n",
    "        # we'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # if labels are sparse, turn them into one-hot vector\n",
    "        # in case the shape is as [3, 0, 2] etc etc per sample\n",
    "        # we turn them into one-hot vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        # with a larger number of batches, it's all summed together\n",
    "        # in the dot product, and some will be given more importance\n",
    "        # than others when we don't normaize\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # creates activation and loss function objects inside\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # forward pass\n",
    "    def foward(self, inputs, y_true):\n",
    "        # output layer's activation function\n",
    "        self.activation.foward(inputs)\n",
    "        # set the output\n",
    "        self.output = self.activation.output\n",
    "        # calculate and return the loss value\n",
    "        return self.loss.calculate(self.ouput, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #if labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "            \n",
    "        # copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab2bbf",
   "metadata": {},
   "source": [
    "Testing if the combined partial derivative returns the same answer as the individual functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3bede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined gradient\n",
      "[[-0.1         0.03333333  0.06666667]\n",
      " [ 0.03333333 -0.16666667  0.13333333]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n",
      "Separate gradient\n",
      "[[-0.09999999  0.03333334  0.06666667]\n",
      " [ 0.03333334 -0.16666667  0.13333334]\n",
      " [ 0.00666667 -0.03333333  0.02666667]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "softmax_loss.backward(softmax_outputs, class_targets)\n",
    "dvalues1 = softmax_loss.dinputs\n",
    "\n",
    "activation = Activation_Softmax()\n",
    "activation.output = softmax_outputs\n",
    "loss = Loss_CategoricalCrossEntropy()\n",
    "loss.backward(softmax_outputs, class_targets)\n",
    "activation.backward(loss.dinputs)\n",
    "dvalues2 = activation.dinputs\n",
    "\n",
    "print(\"Combined gradient\")\n",
    "print(dvalues1)\n",
    "print(\"Separate gradient\")\n",
    "print(dvalues2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
