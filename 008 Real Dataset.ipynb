{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8560086d",
   "metadata": {},
   "source": [
    "### Fashion-MNIST\n",
    "Not the original MNIST, because it's comically easy and does not represent the deep learning used today\n",
    "\n",
    "Downloaded from [here](https://nnfs.io/datasets/fashion_mnist_images.zip)\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Label</th>\n",
    "        <th>Decription</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>0</td>\n",
    "        <td>T-Shirt/Top</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1</td>\n",
    "        <td>Trouser</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2</td>\n",
    "        <td>Pullover</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3</td>\n",
    "        <td>Dress</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4</td>\n",
    "        <td>Coat</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5</td>\n",
    "        <td>Sandal</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6</td>\n",
    "        <td>Shirt</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>7</td>\n",
    "        <td>Sneaker</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8</td>\n",
    "        <td>Bag</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9</td>\n",
    "        <td>Ankle Boot</td>\n",
    "    </tr>\n",
    "    \n",
    "</table>\n",
    "\n",
    "This data set is **grayscaled** (go from 3-channel RGB values per pixel to a single black to white range of 0-255 per pixel), **normalized** (resized into the same dimensions, 28x28 in this case), **balanced** (equal number of training data per label, 6000 in this case). A non-balanced dataset can result in a model developing a bias towards the most common label, as that result in the quickest gradient descent, but gets stuck in a rather poor accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa5ab13",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fa94606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '6', '7', '5', '3', '8', '4', '9', '2', '1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('fashion_mnist_images/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece43ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3643.png', '5275.png', '1367.png', '4286.png', '2085.png', '4936.png', '1270.png', '2775.png', '0003.png', '1866.png']\n",
      "6000\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('fashion_mnist_images/train/0')\n",
    "\n",
    "print(files[:10])\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07375a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image_data = cv2.imread('fashion_mnist_images/train/0/0005.png', cv2.IMREAD_UNCHANGED)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc3027",
   "metadata": {},
   "source": [
    "The IMREAD_UNCHANGED argument tells cv2 not to convert the image data into a 3 color channel image\n",
    "\n",
    "We can tell pyplot to use the gray colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4333d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image_data = cv2.imread('fashion_mnist_images/train/3/0011.png', cv2.IMREAD_UNCHANGED)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image_data, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aaf8886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "    \n",
    "    # scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "    \n",
    "    # create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # for each label folder\n",
    "    for label in labels:\n",
    "        # and for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # read the image\n",
    "            image = cv2.imread(os.path.join(\n",
    "                path, dataset, label, file\n",
    "            ), cv2.IMREAD_UNCHANGED)\n",
    "            \n",
    "            # and append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "    \n",
    "    # convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "def create_data_mnist(path):\n",
    "    \n",
    "    # load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "    \n",
    "    # and return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf1a93",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "The data is currently between 0 and 255. We can either:\n",
    "* Subtract half, and divide so it's between -1 and 1\n",
    "* Just divide right away so it's between 0 and 1\n",
    "\n",
    "The exact one needed to be used generally needs to be tested. Here, the former will be used initially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22dfce44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "\n",
    "# scale features\n",
    "X = (X.astype(np.float32) - 127.5) / 127.5 # instead of float64\n",
    "X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd91e69",
   "metadata": {},
   "source": [
    "60k 28x28 array. Since the dense array can only take a single dimension, we need to flatten the array.\n",
    "\n",
    "A **convolutional neural** network will accept a 2D array as it is, but not the dense layers. In NumPy, when reshaping, -1 means \"however many elements are there\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60aec3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491376f",
   "metadata": {},
   "source": [
    "### Data Shuffling\n",
    "Because the data is currently ordered in their labels, and this is a much larger dataset than the previous spiral dataset, where training on the entire set is infeasible, so batches are used,  we need to shuffle the data. Not shuffling has the same problems as an unbalanced dataset.\n",
    "\n",
    "Because we have the data, and the labels, that needs to be be in the same order, we can't simply call shuffle() on both arrays, so instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59a1d483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[ 3048 19563 58303  8870 40228 31488 21860 56864   845 25770]\n"
     ]
    }
   ],
   "source": [
    "keys = np.array(range(X.shape[0]))\n",
    "print(keys[:10])\n",
    "\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "np.random.shuffle(keys)\n",
    "print(keys[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac198ba",
   "metadata": {},
   "source": [
    "And now this shuffled keys array will be simply used as the indices for both arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8c5a6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[keys]\n",
    "y = y[keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6048b8",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Real models often deal with datasets that are up to terabytes in size, where the entire dataset can't be fed into the model as a single batch at once, so division into batches is necessary.\n",
    "\n",
    "If the dataset is balanced and shuffled and the batches are large enough, then the gradient provided by any given batch would be a good representative of the direction towards a global minimum\n",
    "\n",
    "Too small batches can result in  direction of the slope to fluctuate too much, lengthening training time\n",
    "\n",
    "Too large batches can slow down training, with high number of epochs\n",
    "\n",
    "Common sizes are 32 and 128, though can be made smaller if memory is limited, and made bigger for faster training, and for accuracy and loss improvements with diminishing returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20389907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# create dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128 # 128 samples at once\n",
    "\n",
    "# calculating number of steps\n",
    "steps = X.shape[0] // BATCH_SIZE # integer division\n",
    "# dividing rounds down. if there are some remaining data,\n",
    "# but not a full batch, it won't be included\n",
    "# so add 1 to the steps to create one last batch from the remainders\n",
    "if steps * BATCH_SIZE < X.shape[0]:\n",
    "    steps += 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step in range(steps):\n",
    "        batch_X = X[step * BATCH_SIZE: (step + 1) * BATCH_SIZE]\n",
    "        batch_y = y[step * BATCH_SIZE: (step + 1) * BATCH_SIZE]\n",
    "        \n",
    "        # forward pass, loss calculation, backward pass\n",
    "        # and updating of parameters here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7594ed",
   "metadata": {},
   "source": [
    "The model, the accuracy, and the loss functions all need to be updated to work with batches now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5af4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # If not in the training mode - return values\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# Input \"layer\"\n",
    "class Layer_Input:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "\n",
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        # iterate all trainable layers\n",
    "        for layer in self.trainable_layers:\n",
    "\n",
    "            # L1 regularization - weights\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.weights))\n",
    "\n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                       np.sum(layer.weights * \\\n",
    "                                              layer.weights)\n",
    "\n",
    "            # L1 regularization - biases\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.biases))\n",
    "\n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                       np.sum(layer.biases * \\\n",
    "                                              layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "\n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Add accumulated sum of losses and sample count\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):  # L2 loss\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):  # L1 loss\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "\n",
    "    # Calculates an accuracy\n",
    "    # given predictions and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "\n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "\n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated(self):\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "\n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "\n",
    "    def __init__(self, *, binary=False):\n",
    "        # Binary mode?\n",
    "        self.binary = binary\n",
    "\n",
    "    # No initialization is needed\n",
    "    def init(self, y):\n",
    "        pass\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y\n",
    "\n",
    "\n",
    "# Accuracy calculation for regression model\n",
    "class Accuracy_Regression(Accuracy):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create precision property\n",
    "        self.precision = None\n",
    "\n",
    "    # Calculates precision value\n",
    "    # based on passed-in ground truth values\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        self.loss.remember_trainable_layers(\n",
    "            self.trainable_layers\n",
    "        )\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            # Create an object of combined activation\n",
    "            # and loss functions\n",
    "            self.softmax_classifier_output = \\\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
    "              print_every=1, validation_data=None):\n",
    "\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "\n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data but nor full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            # Print epoch number\n",
    "            print(f'epoch: {epoch}')\n",
    "\n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = \\\n",
    "                    self.loss.calculate(output, batch_y,\n",
    "                                        include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(\n",
    "                                  output)\n",
    "                accuracy = self.accuracy.calculate(predictions,\n",
    "                                                   batch_y)\n",
    "\n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "\n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = \\\n",
    "                self.loss.calculate_accumulated(\n",
    "                    include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "\n",
    "                # Reset accumulated values in loss\n",
    "                # and accuracy objects\n",
    "                self.loss.new_pass()\n",
    "                self.accuracy.new_pass()\n",
    "\n",
    "                # Iterate over steps\n",
    "                for step in range(validation_steps):\n",
    "\n",
    "                    # If batch size is not set -\n",
    "                    # train using one step and full dataset\n",
    "                    if batch_size is None:\n",
    "                        batch_X = X_val\n",
    "                        batch_y = y_val\n",
    "\n",
    "\n",
    "                    # Otherwise slice a batch\n",
    "                    else:\n",
    "                        batch_X = X_val[\n",
    "                            step*batch_size:(step+1)*batch_size\n",
    "                        ]\n",
    "                        batch_y = y_val[\n",
    "                            step*batch_size:(step+1)*batch_size\n",
    "                        ]\n",
    "\n",
    "                    # Perform the forward pass\n",
    "                    output = self.forward(batch_X, training=False)\n",
    "\n",
    "                    # Calculate the loss\n",
    "                    self.loss.calculate(output, batch_y)\n",
    "\n",
    "                    # Get predictions and calculate an accuracy\n",
    "                    predictions = self.output_layer_activation.predictions(\n",
    "                                      output)\n",
    "                    self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "                # Get and print validation loss and accuracy\n",
    "                validation_loss = self.loss.calculate_accumulated()\n",
    "                validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "                # Print a summary\n",
    "                print(f'validation, ' +\n",
    "                      f'acc: {validation_accuracy:.3f}, ' +\n",
    "                      f'loss: {validation_loss:.3f}')\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "\n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = \\\n",
    "                self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "\n",
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(\n",
    "                        os.path.join(path, dataset, label, file),\n",
    "                        cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7dc0990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
    "             127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1207ae99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.078, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 100, acc: 0.719, loss: 0.735 (data_loss: 0.735, reg_loss: 0.000), lr: 0.0009090909090909091\n",
      "step: 200, acc: 0.789, loss: 0.592 (data_loss: 0.592, reg_loss: 0.000), lr: 0.0008333333333333334\n",
      "step: 300, acc: 0.828, loss: 0.518 (data_loss: 0.518, reg_loss: 0.000), lr: 0.0007692307692307692\n",
      "step: 400, acc: 0.844, loss: 0.429 (data_loss: 0.429, reg_loss: 0.000), lr: 0.0007142857142857143\n",
      "step: 468, acc: 0.812, loss: 0.489 (data_loss: 0.489, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.744, loss: 0.671 (data_loss: 0.671, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "validation, acc: 0.812, loss: 0.504\n",
      "epoch: 2\n",
      "step: 0, acc: 0.727, loss: 0.667 (data_loss: 0.667, reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 100, acc: 0.828, loss: 0.449 (data_loss: 0.449, reg_loss: 0.000), lr: 0.0006373486297004461\n",
      "step: 200, acc: 0.867, loss: 0.369 (data_loss: 0.369, reg_loss: 0.000), lr: 0.0005991611743559018\n",
      "step: 300, acc: 0.883, loss: 0.350 (data_loss: 0.350, reg_loss: 0.000), lr: 0.0005652911249293386\n",
      "step: 400, acc: 0.898, loss: 0.313 (data_loss: 0.313, reg_loss: 0.000), lr: 0.0005350454788657037\n",
      "step: 468, acc: 0.823, loss: 0.456 (data_loss: 0.456, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.843, loss: 0.428 (data_loss: 0.428, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "validation, acc: 0.843, loss: 0.426\n",
      "epoch: 3\n",
      "step: 0, acc: 0.828, loss: 0.505 (data_loss: 0.505, reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 100, acc: 0.844, loss: 0.381 (data_loss: 0.381, reg_loss: 0.000), lr: 0.0004906771344455348\n",
      "step: 200, acc: 0.898, loss: 0.318 (data_loss: 0.318, reg_loss: 0.000), lr: 0.0004677268475210477\n",
      "step: 300, acc: 0.883, loss: 0.270 (data_loss: 0.270, reg_loss: 0.000), lr: 0.00044682752457551384\n",
      "step: 400, acc: 0.906, loss: 0.275 (data_loss: 0.275, reg_loss: 0.000), lr: 0.00042771599657827206\n",
      "step: 468, acc: 0.844, loss: 0.415 (data_loss: 0.415, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.863, loss: 0.372 (data_loss: 0.372, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "validation, acc: 0.856, loss: 0.394\n",
      "epoch: 4\n",
      "step: 0, acc: 0.836, loss: 0.426 (data_loss: 0.426, reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 100, acc: 0.867, loss: 0.351 (data_loss: 0.351, reg_loss: 0.000), lr: 0.00039888312724371757\n",
      "step: 200, acc: 0.891, loss: 0.272 (data_loss: 0.272, reg_loss: 0.000), lr: 0.0003835826620636747\n",
      "step: 300, acc: 0.914, loss: 0.233 (data_loss: 0.233, reg_loss: 0.000), lr: 0.0003694126339120798\n",
      "step: 400, acc: 0.891, loss: 0.262 (data_loss: 0.262, reg_loss: 0.000), lr: 0.0003562522265764161\n",
      "step: 468, acc: 0.854, loss: 0.400 (data_loss: 0.400, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.874, loss: 0.339 (data_loss: 0.339, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "validation, acc: 0.863, loss: 0.379\n",
      "epoch: 5\n",
      "step: 0, acc: 0.883, loss: 0.379 (data_loss: 0.379, reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 100, acc: 0.867, loss: 0.350 (data_loss: 0.350, reg_loss: 0.000), lr: 0.00033602150537634406\n",
      "step: 200, acc: 0.922, loss: 0.246 (data_loss: 0.246, reg_loss: 0.000), lr: 0.00032509752925877764\n",
      "step: 300, acc: 0.922, loss: 0.210 (data_loss: 0.210, reg_loss: 0.000), lr: 0.00031486146095717883\n",
      "step: 400, acc: 0.914, loss: 0.252 (data_loss: 0.252, reg_loss: 0.000), lr: 0.00030525030525030525\n",
      "step: 468, acc: 0.844, loss: 0.393 (data_loss: 0.393, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.883, loss: 0.318 (data_loss: 0.318, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "validation, acc: 0.866, loss: 0.370\n",
      "epoch: 6\n",
      "step: 0, acc: 0.883, loss: 0.348 (data_loss: 0.348, reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 100, acc: 0.867, loss: 0.353 (data_loss: 0.353, reg_loss: 0.000), lr: 0.00029027576197387516\n",
      "step: 200, acc: 0.930, loss: 0.229 (data_loss: 0.229, reg_loss: 0.000), lr: 0.0002820874471086037\n",
      "step: 300, acc: 0.914, loss: 0.196 (data_loss: 0.196, reg_loss: 0.000), lr: 0.00027434842249657066\n",
      "step: 400, acc: 0.922, loss: 0.234 (data_loss: 0.234, reg_loss: 0.000), lr: 0.000267022696929239\n",
      "step: 468, acc: 0.844, loss: 0.384 (data_loss: 0.384, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.889, loss: 0.301 (data_loss: 0.301, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "validation, acc: 0.870, loss: 0.362\n",
      "epoch: 7\n",
      "step: 0, acc: 0.906, loss: 0.317 (data_loss: 0.317, reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 100, acc: 0.867, loss: 0.338 (data_loss: 0.338, reg_loss: 0.000), lr: 0.00025549310168625444\n",
      "step: 200, acc: 0.922, loss: 0.220 (data_loss: 0.220, reg_loss: 0.000), lr: 0.00024912805181863477\n",
      "step: 300, acc: 0.930, loss: 0.183 (data_loss: 0.183, reg_loss: 0.000), lr: 0.0002430724355858046\n",
      "step: 400, acc: 0.930, loss: 0.212 (data_loss: 0.212, reg_loss: 0.000), lr: 0.00023730422401518745\n",
      "step: 468, acc: 0.844, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.894, loss: 0.287 (data_loss: 0.287, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "validation, acc: 0.873, loss: 0.353\n",
      "epoch: 8\n",
      "step: 0, acc: 0.914, loss: 0.290 (data_loss: 0.290, reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "step: 100, acc: 0.875, loss: 0.320 (data_loss: 0.320, reg_loss: 0.000), lr: 0.00022815423226100847\n",
      "step: 200, acc: 0.922, loss: 0.213 (data_loss: 0.213, reg_loss: 0.000), lr: 0.0002230649118893598\n",
      "step: 300, acc: 0.930, loss: 0.176 (data_loss: 0.176, reg_loss: 0.000), lr: 0.00021819768710451667\n",
      "step: 400, acc: 0.938, loss: 0.191 (data_loss: 0.191, reg_loss: 0.000), lr: 0.00021353833013025838\n",
      "step: 468, acc: 0.865, loss: 0.367 (data_loss: 0.367, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.899, loss: 0.275 (data_loss: 0.275, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "validation, acc: 0.875, loss: 0.347\n",
      "epoch: 9\n",
      "step: 0, acc: 0.914, loss: 0.268 (data_loss: 0.268, reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 100, acc: 0.859, loss: 0.308 (data_loss: 0.308, reg_loss: 0.000), lr: 0.0002061005770816158\n",
      "step: 200, acc: 0.914, loss: 0.205 (data_loss: 0.205, reg_loss: 0.000), lr: 0.00020193861066235866\n",
      "step: 300, acc: 0.930, loss: 0.170 (data_loss: 0.170, reg_loss: 0.000), lr: 0.0001979414093428345\n",
      "step: 400, acc: 0.938, loss: 0.182 (data_loss: 0.182, reg_loss: 0.000), lr: 0.0001940993788819876\n",
      "step: 468, acc: 0.865, loss: 0.360 (data_loss: 0.360, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.903, loss: 0.264 (data_loss: 0.264, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "validation, acc: 0.879, loss: 0.342\n",
      "epoch: 10\n",
      "step: 0, acc: 0.906, loss: 0.255 (data_loss: 0.255, reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 100, acc: 0.867, loss: 0.298 (data_loss: 0.298, reg_loss: 0.000), lr: 0.00018793459875963167\n",
      "step: 200, acc: 0.922, loss: 0.200 (data_loss: 0.200, reg_loss: 0.000), lr: 0.00018446781036709093\n",
      "step: 300, acc: 0.930, loss: 0.165 (data_loss: 0.165, reg_loss: 0.000), lr: 0.00018112660749864155\n",
      "step: 400, acc: 0.938, loss: 0.176 (data_loss: 0.176, reg_loss: 0.000), lr: 0.00017790428749332856\n",
      "step: 468, acc: 0.875, loss: 0.353 (data_loss: 0.353, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.906, loss: 0.255 (data_loss: 0.255, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "validation, acc: 0.881, loss: 0.339\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 256))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(256, 256))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(256, 256))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(256, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=1e-3),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, validation_data=(X_test, y_test),\n",
    "            epochs=10, batch_size=128, print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9612853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
