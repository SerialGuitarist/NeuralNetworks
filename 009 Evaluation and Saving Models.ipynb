{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b9622f",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "With the model we have now, it can't easily do predictions or run test on data\n",
    "\n",
    "By copying over the evaluation bit in the model from the training function into its own function, the model now has an interface for easy evaluation operation on any given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39542fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # If not in the training mode - return values\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# Input \"layer\"\n",
    "class Layer_Input:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "\n",
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        # iterate all trainable layers\n",
    "        for layer in self.trainable_layers:\n",
    "\n",
    "            # L1 regularization - weights\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.weights))\n",
    "\n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                       np.sum(layer.weights * \\\n",
    "                                              layer.weights)\n",
    "\n",
    "            # L1 regularization - biases\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.biases))\n",
    "\n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                       np.sum(layer.biases * \\\n",
    "                                              layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "\n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Add accumulated sum of losses and sample count\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):  # L2 loss\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):  # L1 loss\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "\n",
    "    # Calculates an accuracy\n",
    "    # given predictions and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "\n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "\n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated(self):\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "\n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "\n",
    "    def __init__(self, *, binary=False):\n",
    "        # Binary mode?\n",
    "        self.binary = binary\n",
    "\n",
    "    # No initialization is needed\n",
    "    def init(self, y):\n",
    "        pass\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y\n",
    "\n",
    "\n",
    "# Accuracy calculation for regression model\n",
    "class Accuracy_Regression(Accuracy):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create precision property\n",
    "        self.precision = None\n",
    "\n",
    "    # Calculates precision value\n",
    "    # based on passed-in ground truth values\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        self.loss.remember_trainable_layers(\n",
    "            self.trainable_layers\n",
    "        )\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            # Create an object of combined activation\n",
    "            # and loss functions\n",
    "            self.softmax_classifier_output = \\\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
    "              print_every=1, validation_data=None):\n",
    "\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "\n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data but nor full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            # Print epoch number\n",
    "            print(f'epoch: {epoch}')\n",
    "\n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = \\\n",
    "                    self.loss.calculate(output, batch_y,\n",
    "                                        include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(\n",
    "                                  output)\n",
    "                accuracy = self.accuracy.calculate(predictions,\n",
    "                                                   batch_y)\n",
    "\n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "\n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = \\\n",
    "                self.loss.calculate_accumulated(\n",
    "                    include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "\n",
    "                # evaluate the model\n",
    "                self.evaluate(*validation_data,\n",
    "                              batch_size = batch_size)\n",
    "                # * explodes the data into individual arguments\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "\n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = \\\n",
    "                self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "    \n",
    "    # evaluates the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size = None):\n",
    "        \n",
    "        # default value if batch size is not being set\n",
    "        validation_steps = 1\n",
    "        \n",
    "        # calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            # another batch from remainders\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "\n",
    "        # Reset accumulated values in loss\n",
    "        # and accuracy objects\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        # Iterate over steps\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            # If batch size is not set -\n",
    "            # train using one step and full dataset\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "\n",
    "            # Otherwise slice a batch\n",
    "            else:\n",
    "                batch_X = X_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "                batch_y = y_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(batch_X, training=False)\n",
    "\n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "\n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(\n",
    "                              output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        # Get and print validation loss and accuracy\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        # Print a summary\n",
    "        print(f'validation, ' +\n",
    "              f'acc: {validation_accuracy:.3f}, ' +\n",
    "              f'loss: {validation_loss:.3f}')\n",
    "\n",
    "\n",
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(\n",
    "                        os.path.join(path, dataset, label, file),\n",
    "                        cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86dcb8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y, X_test, y_test = create_data_mnist('fashion_mnist_images')\n",
    "\n",
    "# Shuffle the training dataset\n",
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X = X[keys]\n",
    "y = y[keys]\n",
    "\n",
    "# Scale and reshape samples\n",
    "X = (X.reshape(X.shape[0], -1).astype(np.float32) - 127.5) / 127.5\n",
    "X_test = (X_test.reshape(X_test.shape[0], -1).astype(np.float32) -\n",
    "             127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd063799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.078, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 128, acc: 0.742, loss: 0.672 (data_loss: 0.672, reg_loss: 0.000), lr: 0.0008865248226950354\n",
      "step: 256, acc: 0.844, loss: 0.473 (data_loss: 0.473, reg_loss: 0.000), lr: 0.0007961783439490446\n",
      "step: 384, acc: 0.789, loss: 0.518 (data_loss: 0.518, reg_loss: 0.000), lr: 0.0007225433526011561\n",
      "step: 468, acc: 0.885, loss: 0.355 (data_loss: 0.355, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.761, loss: 0.651 (data_loss: 0.651, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "validation, acc: 0.819, loss: 0.497\n",
      "epoch: 2\n",
      "step: 0, acc: 0.836, loss: 0.454 (data_loss: 0.454, reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 128, acc: 0.836, loss: 0.520 (data_loss: 0.520, reg_loss: 0.000), lr: 0.0006261740763932373\n",
      "step: 256, acc: 0.891, loss: 0.354 (data_loss: 0.354, reg_loss: 0.000), lr: 0.0005797101449275362\n",
      "step: 384, acc: 0.828, loss: 0.441 (data_loss: 0.441, reg_loss: 0.000), lr: 0.0005396654074473827\n",
      "step: 468, acc: 0.896, loss: 0.269 (data_loss: 0.269, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.846, loss: 0.426 (data_loss: 0.426, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "validation, acc: 0.847, loss: 0.430\n",
      "epoch: 3\n",
      "step: 0, acc: 0.852, loss: 0.408 (data_loss: 0.408, reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 128, acc: 0.836, loss: 0.477 (data_loss: 0.477, reg_loss: 0.000), lr: 0.00048402710551790907\n",
      "step: 256, acc: 0.898, loss: 0.327 (data_loss: 0.327, reg_loss: 0.000), lr: 0.00045578851412944393\n",
      "step: 384, acc: 0.836, loss: 0.400 (data_loss: 0.400, reg_loss: 0.000), lr: 0.0004306632213608957\n",
      "step: 468, acc: 0.906, loss: 0.245 (data_loss: 0.245, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.862, loss: 0.383 (data_loss: 0.383, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "validation, acc: 0.855, loss: 0.405\n",
      "epoch: 4\n",
      "step: 0, acc: 0.859, loss: 0.381 (data_loss: 0.381, reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 128, acc: 0.820, loss: 0.458 (data_loss: 0.458, reg_loss: 0.000), lr: 0.0003944773175542406\n",
      "step: 256, acc: 0.898, loss: 0.310 (data_loss: 0.310, reg_loss: 0.000), lr: 0.00037551633496057073\n",
      "step: 384, acc: 0.852, loss: 0.376 (data_loss: 0.376, reg_loss: 0.000), lr: 0.00035829451809387314\n",
      "step: 468, acc: 0.917, loss: 0.229 (data_loss: 0.229, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.870, loss: 0.359 (data_loss: 0.359, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "validation, acc: 0.860, loss: 0.391\n",
      "epoch: 5\n",
      "step: 0, acc: 0.867, loss: 0.362 (data_loss: 0.362, reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 128, acc: 0.828, loss: 0.441 (data_loss: 0.441, reg_loss: 0.000), lr: 0.00033288948069241014\n",
      "step: 256, acc: 0.898, loss: 0.300 (data_loss: 0.300, reg_loss: 0.000), lr: 0.00031928480204342275\n",
      "step: 384, acc: 0.859, loss: 0.358 (data_loss: 0.358, reg_loss: 0.000), lr: 0.00030674846625766873\n",
      "step: 468, acc: 0.917, loss: 0.214 (data_loss: 0.214, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.876, loss: 0.342 (data_loss: 0.342, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "validation, acc: 0.863, loss: 0.381\n",
      "epoch: 6\n",
      "step: 0, acc: 0.859, loss: 0.346 (data_loss: 0.346, reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 128, acc: 0.820, loss: 0.427 (data_loss: 0.427, reg_loss: 0.000), lr: 0.0002879355024474518\n",
      "step: 256, acc: 0.898, loss: 0.296 (data_loss: 0.296, reg_loss: 0.000), lr: 0.00027770063871146905\n",
      "step: 384, acc: 0.852, loss: 0.345 (data_loss: 0.345, reg_loss: 0.000), lr: 0.0002681684097613301\n",
      "step: 468, acc: 0.917, loss: 0.205 (data_loss: 0.205, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.880, loss: 0.330 (data_loss: 0.330, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "validation, acc: 0.866, loss: 0.374\n",
      "epoch: 7\n",
      "step: 0, acc: 0.875, loss: 0.336 (data_loss: 0.336, reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 128, acc: 0.828, loss: 0.412 (data_loss: 0.412, reg_loss: 0.000), lr: 0.0002536783358701167\n",
      "step: 256, acc: 0.891, loss: 0.291 (data_loss: 0.291, reg_loss: 0.000), lr: 0.0002457002457002457\n",
      "step: 384, acc: 0.844, loss: 0.335 (data_loss: 0.335, reg_loss: 0.000), lr: 0.00023820867079561695\n",
      "step: 468, acc: 0.917, loss: 0.198 (data_loss: 0.198, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.884, loss: 0.319 (data_loss: 0.319, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "validation, acc: 0.868, loss: 0.369\n",
      "epoch: 8\n",
      "step: 0, acc: 0.875, loss: 0.328 (data_loss: 0.328, reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "step: 128, acc: 0.844, loss: 0.401 (data_loss: 0.401, reg_loss: 0.000), lr: 0.00022670596236681027\n",
      "step: 256, acc: 0.891, loss: 0.285 (data_loss: 0.285, reg_loss: 0.000), lr: 0.00022031284423881914\n",
      "step: 384, acc: 0.836, loss: 0.331 (data_loss: 0.331, reg_loss: 0.000), lr: 0.0002142704092564817\n",
      "step: 468, acc: 0.917, loss: 0.190 (data_loss: 0.190, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.886, loss: 0.310 (data_loss: 0.310, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "validation, acc: 0.869, loss: 0.364\n",
      "epoch: 9\n",
      "step: 0, acc: 0.875, loss: 0.323 (data_loss: 0.323, reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 128, acc: 0.844, loss: 0.395 (data_loss: 0.395, reg_loss: 0.000), lr: 0.00020491803278688525\n",
      "step: 256, acc: 0.891, loss: 0.281 (data_loss: 0.281, reg_loss: 0.000), lr: 0.00019968051118210864\n",
      "step: 384, acc: 0.836, loss: 0.328 (data_loss: 0.328, reg_loss: 0.000), lr: 0.00019470404984423675\n",
      "step: 468, acc: 0.917, loss: 0.182 (data_loss: 0.182, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.889, loss: 0.303 (data_loss: 0.303, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "validation, acc: 0.871, loss: 0.360\n",
      "epoch: 10\n",
      "step: 0, acc: 0.875, loss: 0.317 (data_loss: 0.317, reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 128, acc: 0.844, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.00018695083193120211\n",
      "step: 256, acc: 0.891, loss: 0.278 (data_loss: 0.278, reg_loss: 0.000), lr: 0.0001825817053131276\n",
      "step: 384, acc: 0.828, loss: 0.326 (data_loss: 0.326, reg_loss: 0.000), lr: 0.0001784121320249777\n",
      "step: 468, acc: 0.917, loss: 0.175 (data_loss: 0.175, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.892, loss: 0.296 (data_loss: 0.296, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "validation, acc: 0.872, loss: 0.357\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=1e-3),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, validation_data=(X_test, y_test),\n",
    "            epochs=10, batch_size=128, print_every=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e6f6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.896, loss: 0.286\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b37f09d",
   "metadata": {},
   "source": [
    "### Saving and Loading Models\n",
    "#### Getting Parameters\n",
    "The Layer_Dense class gets a get_parameter function, which returns its weights and biases, and the model gets a get_parameter, which iterates over its trainable layers, calling their get_parameter function, collecting the parameters and returns it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93e08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "    # Retrive layer paremters\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # If not in the training mode - return values\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# Input \"layer\"\n",
    "class Layer_Input:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "\n",
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        # iterate all trainable layers\n",
    "        for layer in self.trainable_layers:\n",
    "\n",
    "            # L1 regularization - weights\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.weights))\n",
    "\n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                       np.sum(layer.weights * \\\n",
    "                                              layer.weights)\n",
    "\n",
    "            # L1 regularization - biases\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.biases))\n",
    "\n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                       np.sum(layer.biases * \\\n",
    "                                              layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "\n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Add accumulated sum of losses and sample count\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):  # L2 loss\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):  # L1 loss\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "\n",
    "    # Calculates an accuracy\n",
    "    # given predictions and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "\n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "\n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated(self):\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "\n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "\n",
    "    def __init__(self, *, binary=False):\n",
    "        # Binary mode?\n",
    "        self.binary = binary\n",
    "\n",
    "    # No initialization is needed\n",
    "    def init(self, y):\n",
    "        pass\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y\n",
    "\n",
    "\n",
    "# Accuracy calculation for regression model\n",
    "class Accuracy_Regression(Accuracy):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create precision property\n",
    "        self.precision = None\n",
    "\n",
    "    # Calculates precision value\n",
    "    # based on passed-in ground truth values\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set(self, *, loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        self.loss.remember_trainable_layers(\n",
    "            self.trainable_layers\n",
    "        )\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            # Create an object of combined activation\n",
    "            # and loss functions\n",
    "            self.softmax_classifier_output = \\\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
    "              print_every=1, validation_data=None):\n",
    "\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "\n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data but nor full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            # Print epoch number\n",
    "            print(f'epoch: {epoch}')\n",
    "\n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = \\\n",
    "                    self.loss.calculate(output, batch_y,\n",
    "                                        include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(\n",
    "                                  output)\n",
    "                accuracy = self.accuracy.calculate(predictions,\n",
    "                                                   batch_y)\n",
    "\n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "\n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = \\\n",
    "                self.loss.calculate_accumulated(\n",
    "                    include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "\n",
    "                # evaluate the model\n",
    "                self.evaluate(*validation_data,\n",
    "                              batch_size = batch_size)\n",
    "                # * explodes the data into individual arguments\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "\n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = \\\n",
    "                self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "    \n",
    "    # evaluates the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size = None):\n",
    "        \n",
    "        # default value if batch size is not being set\n",
    "        validation_steps = 1\n",
    "        \n",
    "        # calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            # another batch from remainders\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "\n",
    "        # Reset accumulated values in loss\n",
    "        # and accuracy objects\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        # Iterate over steps\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            # If batch size is not set -\n",
    "            # train using one step and full dataset\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "\n",
    "            # Otherwise slice a batch\n",
    "            else:\n",
    "                batch_X = X_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "                batch_y = y_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(batch_X, training=False)\n",
    "\n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "\n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(\n",
    "                              output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        # Get and print validation loss and accuracy\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        # Print a summary\n",
    "        print(f'validation, ' +\n",
    "              f'acc: {validation_accuracy:.3f}, ' +\n",
    "              f'loss: {validation_loss:.3f}')\n",
    "\n",
    "    # Retrieves and returns parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        \n",
    "        # create a list for paremeters\n",
    "        parameters = []\n",
    "        \n",
    "        # iterate trainable layers and get their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            \n",
    "        # return a list\n",
    "        return parameters\n",
    "\n",
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(\n",
    "                        os.path.join(path, dataset, label, file),\n",
    "                        cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3eccbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.117, loss: 2.303 (data_loss: 2.303, reg_loss: 0.000), lr: 0.001\n",
      "step: 128, acc: 0.711, loss: 0.764 (data_loss: 0.764, reg_loss: 0.000), lr: 0.0008865248226950354\n",
      "step: 256, acc: 0.805, loss: 0.543 (data_loss: 0.543, reg_loss: 0.000), lr: 0.0007961783439490446\n",
      "step: 384, acc: 0.766, loss: 0.621 (data_loss: 0.621, reg_loss: 0.000), lr: 0.0007225433526011561\n",
      "step: 468, acc: 0.844, loss: 0.441 (data_loss: 0.441, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "training, acc: 0.720, loss: 0.757 (data_loss: 0.757, reg_loss: 0.000), lr: 0.000681198910081744\n",
      "validation, acc: 0.790, loss: 0.569\n",
      "epoch: 2\n",
      "step: 0, acc: 0.820, loss: 0.513 (data_loss: 0.513, reg_loss: 0.000), lr: 0.0006807351940095304\n",
      "step: 128, acc: 0.828, loss: 0.560 (data_loss: 0.560, reg_loss: 0.000), lr: 0.0006261740763932373\n",
      "step: 256, acc: 0.859, loss: 0.413 (data_loss: 0.413, reg_loss: 0.000), lr: 0.0005797101449275362\n",
      "step: 384, acc: 0.828, loss: 0.505 (data_loss: 0.505, reg_loss: 0.000), lr: 0.0005396654074473827\n",
      "step: 468, acc: 0.865, loss: 0.345 (data_loss: 0.345, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "training, acc: 0.823, loss: 0.492 (data_loss: 0.492, reg_loss: 0.000), lr: 0.0005162622612287042\n",
      "validation, acc: 0.825, loss: 0.491\n",
      "epoch: 3\n",
      "step: 0, acc: 0.852, loss: 0.439 (data_loss: 0.439, reg_loss: 0.000), lr: 0.0005159958720330237\n",
      "step: 128, acc: 0.836, loss: 0.525 (data_loss: 0.525, reg_loss: 0.000), lr: 0.00048402710551790907\n",
      "step: 256, acc: 0.875, loss: 0.359 (data_loss: 0.359, reg_loss: 0.000), lr: 0.00045578851412944393\n",
      "step: 384, acc: 0.828, loss: 0.457 (data_loss: 0.457, reg_loss: 0.000), lr: 0.0004306632213608957\n",
      "step: 468, acc: 0.875, loss: 0.312 (data_loss: 0.312, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "training, acc: 0.840, loss: 0.444 (data_loss: 0.444, reg_loss: 0.000), lr: 0.0004156275976724854\n",
      "validation, acc: 0.837, loss: 0.459\n",
      "epoch: 4\n",
      "step: 0, acc: 0.844, loss: 0.421 (data_loss: 0.421, reg_loss: 0.000), lr: 0.0004154549231408392\n",
      "step: 128, acc: 0.844, loss: 0.502 (data_loss: 0.502, reg_loss: 0.000), lr: 0.0003944773175542406\n",
      "step: 256, acc: 0.891, loss: 0.337 (data_loss: 0.337, reg_loss: 0.000), lr: 0.00037551633496057073\n",
      "step: 384, acc: 0.828, loss: 0.427 (data_loss: 0.427, reg_loss: 0.000), lr: 0.00035829451809387314\n",
      "step: 468, acc: 0.896, loss: 0.289 (data_loss: 0.289, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "training, acc: 0.849, loss: 0.418 (data_loss: 0.418, reg_loss: 0.000), lr: 0.00034782608695652176\n",
      "validation, acc: 0.844, loss: 0.442\n",
      "epoch: 5\n",
      "step: 0, acc: 0.867, loss: 0.406 (data_loss: 0.406, reg_loss: 0.000), lr: 0.0003477051460361613\n",
      "step: 128, acc: 0.836, loss: 0.485 (data_loss: 0.485, reg_loss: 0.000), lr: 0.00033288948069241014\n",
      "step: 256, acc: 0.898, loss: 0.324 (data_loss: 0.324, reg_loss: 0.000), lr: 0.00031928480204342275\n",
      "step: 384, acc: 0.828, loss: 0.411 (data_loss: 0.411, reg_loss: 0.000), lr: 0.00030674846625766873\n",
      "step: 468, acc: 0.906, loss: 0.273 (data_loss: 0.273, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "training, acc: 0.855, loss: 0.400 (data_loss: 0.400, reg_loss: 0.000), lr: 0.0002990430622009569\n",
      "validation, acc: 0.847, loss: 0.432\n",
      "epoch: 6\n",
      "step: 0, acc: 0.867, loss: 0.396 (data_loss: 0.396, reg_loss: 0.000), lr: 0.0002989536621823617\n",
      "step: 128, acc: 0.828, loss: 0.471 (data_loss: 0.471, reg_loss: 0.000), lr: 0.0002879355024474518\n",
      "step: 256, acc: 0.891, loss: 0.320 (data_loss: 0.320, reg_loss: 0.000), lr: 0.00027770063871146905\n",
      "step: 384, acc: 0.852, loss: 0.402 (data_loss: 0.402, reg_loss: 0.000), lr: 0.0002681684097613301\n",
      "step: 468, acc: 0.906, loss: 0.257 (data_loss: 0.257, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "training, acc: 0.860, loss: 0.387 (data_loss: 0.387, reg_loss: 0.000), lr: 0.00026226068712300026\n",
      "validation, acc: 0.851, loss: 0.423\n",
      "epoch: 7\n",
      "step: 0, acc: 0.867, loss: 0.391 (data_loss: 0.391, reg_loss: 0.000), lr: 0.00026219192448872575\n",
      "step: 128, acc: 0.812, loss: 0.466 (data_loss: 0.466, reg_loss: 0.000), lr: 0.0002536783358701167\n",
      "step: 256, acc: 0.883, loss: 0.319 (data_loss: 0.319, reg_loss: 0.000), lr: 0.0002457002457002457\n",
      "step: 384, acc: 0.836, loss: 0.397 (data_loss: 0.397, reg_loss: 0.000), lr: 0.00023820867079561695\n",
      "step: 468, acc: 0.896, loss: 0.245 (data_loss: 0.245, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "training, acc: 0.863, loss: 0.377 (data_loss: 0.377, reg_loss: 0.000), lr: 0.00023353573096683791\n",
      "validation, acc: 0.855, loss: 0.415\n",
      "epoch: 8\n",
      "step: 0, acc: 0.859, loss: 0.387 (data_loss: 0.387, reg_loss: 0.000), lr: 0.00023348120476301658\n",
      "step: 128, acc: 0.828, loss: 0.461 (data_loss: 0.461, reg_loss: 0.000), lr: 0.00022670596236681027\n",
      "step: 256, acc: 0.875, loss: 0.320 (data_loss: 0.320, reg_loss: 0.000), lr: 0.00022031284423881914\n",
      "step: 384, acc: 0.844, loss: 0.394 (data_loss: 0.394, reg_loss: 0.000), lr: 0.0002142704092564817\n",
      "step: 468, acc: 0.896, loss: 0.234 (data_loss: 0.234, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "training, acc: 0.867, loss: 0.369 (data_loss: 0.369, reg_loss: 0.000), lr: 0.00021048200378867611\n",
      "validation, acc: 0.856, loss: 0.409\n",
      "epoch: 9\n",
      "step: 0, acc: 0.859, loss: 0.384 (data_loss: 0.384, reg_loss: 0.000), lr: 0.0002104377104377104\n",
      "step: 128, acc: 0.828, loss: 0.457 (data_loss: 0.457, reg_loss: 0.000), lr: 0.00020491803278688525\n",
      "step: 256, acc: 0.875, loss: 0.321 (data_loss: 0.321, reg_loss: 0.000), lr: 0.00019968051118210864\n",
      "step: 384, acc: 0.828, loss: 0.391 (data_loss: 0.391, reg_loss: 0.000), lr: 0.00019470404984423675\n",
      "step: 468, acc: 0.896, loss: 0.227 (data_loss: 0.227, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "training, acc: 0.869, loss: 0.362 (data_loss: 0.362, reg_loss: 0.000), lr: 0.00019157088122605365\n",
      "validation, acc: 0.857, loss: 0.404\n",
      "epoch: 10\n",
      "step: 0, acc: 0.859, loss: 0.381 (data_loss: 0.381, reg_loss: 0.000), lr: 0.0001915341888527102\n",
      "step: 128, acc: 0.828, loss: 0.452 (data_loss: 0.452, reg_loss: 0.000), lr: 0.00018695083193120211\n",
      "step: 256, acc: 0.875, loss: 0.321 (data_loss: 0.321, reg_loss: 0.000), lr: 0.0001825817053131276\n",
      "step: 384, acc: 0.828, loss: 0.386 (data_loss: 0.386, reg_loss: 0.000), lr: 0.0001784121320249777\n",
      "step: 468, acc: 0.906, loss: 0.222 (data_loss: 0.222, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "training, acc: 0.871, loss: 0.356 (data_loss: 0.356, reg_loss: 0.000), lr: 0.00017577781683951485\n",
      "validation, acc: 0.860, loss: 0.399\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# Set loss, optimizer and accuracy objects\n",
    "model.set(\n",
    "    loss=Loss_CategoricalCrossentropy(),\n",
    "    optimizer=Optimizer_Adam(decay=1e-3),\n",
    "    accuracy=Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# Train the model\n",
    "model.train(X, y, validation_data=(X_test, y_test),\n",
    "            epochs=10, batch_size=128, print_every=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f95689ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([[-0.00133056, -0.05170796, -0.02924731, ...,  0.02310152,\n",
       "           0.02297807, -0.00057005],\n",
       "         [-0.00834672, -0.03731248, -0.01029693, ...,  0.01382491,\n",
       "           0.03985465,  0.00298305],\n",
       "         [-0.00208189, -0.03477024, -0.03202893, ...,  0.00529394,\n",
       "           0.01095841, -0.00655686],\n",
       "         ...,\n",
       "         [ 0.00190655, -0.00857151, -0.02222547, ..., -0.00202852,\n",
       "           0.05273269, -0.02323852],\n",
       "         [-0.00242284, -0.03910648, -0.03085627, ..., -0.00428375,\n",
       "           0.04648045, -0.02139636],\n",
       "         [-0.00873461, -0.03150497, -0.02892148, ...,  0.00985378,\n",
       "           0.0271633 ,  0.00644244]], dtype=float32),\n",
       "  array([[-0.00132657,  0.02968659,  0.02638146,  0.03310021,  0.01821299,\n",
       "           0.03370177, -0.00973993,  0.07469406, -0.02096456,  0.01457173,\n",
       "           0.00321073, -0.03237454,  0.06332753,  0.00736964, -0.02930776,\n",
       "          -0.01573812,  0.03923583,  0.01517208,  0.00075245,  0.04483398,\n",
       "           0.01215104, -0.00523664, -0.01025061,  0.0287751 ,  0.0199609 ,\n",
       "           0.04014448, -0.04510188,  0.0301289 ,  0.04801669, -0.01149255,\n",
       "           0.07029375, -0.01336717, -0.03165403, -0.02363437, -0.00699998,\n",
       "           0.00295869,  0.02936626,  0.00449665,  0.01304342, -0.03152934,\n",
       "          -0.00385407,  0.04664988,  0.05160915,  0.04579559,  0.00674199,\n",
       "           0.01139587,  0.03559926,  0.00963979,  0.04283814, -0.00100557,\n",
       "           0.01633763,  0.04091173,  0.03403753, -0.00430278, -0.01807792,\n",
       "           0.02233911, -0.0084749 ,  0.01993696,  0.00744747, -0.00270028,\n",
       "           0.02863988, -0.01380102, -0.02645659,  0.01258131]],\n",
       "        dtype=float32)),\n",
       " (array([[ 0.07611866, -0.07383323,  0.16631694, ...,  0.0485372 ,\n",
       "           0.07946601, -0.11753751],\n",
       "         [-0.04044082, -0.08735991,  0.08473358, ...,  0.0738106 ,\n",
       "           0.02350371, -0.06392395],\n",
       "         [-0.05653544,  0.00107893, -0.12386287, ..., -0.11389201,\n",
       "          -0.13557705,  0.10400083],\n",
       "         ...,\n",
       "         [ 0.04828115, -0.06054718, -0.06796802, ..., -0.12963046,\n",
       "           0.01652445,  0.10934702],\n",
       "         [ 0.0192345 , -0.09352829,  0.12572616, ...,  0.02907525,\n",
       "           0.09470154, -0.07770275],\n",
       "         [ 0.03906136,  0.03267773, -0.00023419, ..., -0.0859769 ,\n",
       "           0.05099799,  0.04235664]], dtype=float32),\n",
       "  array([[-0.0087331 ,  0.08537427,  0.03639673,  0.02131918,  0.03682358,\n",
       "           0.04951628, -0.04844133,  0.0377414 ,  0.01737367,  0.09233613,\n",
       "           0.00925349, -0.02748994,  0.02657328, -0.00487874, -0.00777102,\n",
       "           0.06423113,  0.10964807, -0.00581507,  0.01921617,  0.02152284,\n",
       "          -0.00595589, -0.02526087,  0.01719186,  0.05383575, -0.00696424,\n",
       "           0.04580384, -0.00273012,  0.04747224, -0.02842221, -0.00174455,\n",
       "          -0.02323832,  0.02331744,  0.02332693, -0.01912512, -0.04122118,\n",
       "          -0.00898858, -0.00725576,  0.02901171,  0.02486391,  0.04578463,\n",
       "           0.01931268, -0.02382534, -0.01544349, -0.00598346,  0.01441149,\n",
       "           0.05096627, -0.0112261 , -0.06208856, -0.00599694, -0.00594206,\n",
       "          -0.00431537, -0.00782548,  0.04736663,  0.00351576,  0.0029042 ,\n",
       "          -0.0054839 ,  0.01133092, -0.02078447, -0.00595554,  0.        ,\n",
       "           0.05915365,  0.0018298 ,  0.00067439,  0.04991018]],\n",
       "        dtype=float32)),\n",
       " (array([[-8.03611800e-02,  4.73492220e-02, -8.88733380e-03,\n",
       "           1.83151606e-02, -7.04421252e-02,  7.06128553e-02,\n",
       "          -2.68112049e-02,  7.64233693e-02, -6.72721267e-02,\n",
       "          -4.34815288e-02],\n",
       "         [ 1.36809936e-02, -1.44785503e-02, -5.42966947e-02,\n",
       "          -2.54699662e-02, -1.58965334e-01,  2.33232863e-02,\n",
       "          -8.49481449e-02,  5.35903126e-02,  6.11608140e-02,\n",
       "           6.85489029e-02],\n",
       "         [ 2.72030197e-02,  4.07357626e-02,  8.45646635e-02,\n",
       "          -1.60624110e-03,  1.12069175e-02, -1.66087225e-01,\n",
       "          -4.76815626e-02, -1.35335177e-01, -1.05793392e-02,\n",
       "          -1.53076621e-02],\n",
       "         [ 9.01784226e-02, -2.79930849e-02, -1.21916741e-01,\n",
       "          -2.27307230e-02, -1.36515737e-01, -7.70723373e-02,\n",
       "           9.91064161e-02, -1.39187232e-01, -1.19786538e-01,\n",
       "          -7.36290663e-02],\n",
       "         [ 6.61108792e-02,  6.85748532e-02,  1.48810716e-02,\n",
       "          -1.68327019e-01,  4.25393041e-03,  1.46386400e-01,\n",
       "           2.89856791e-02, -1.43174618e-01,  9.52065587e-02,\n",
       "          -2.20554218e-01],\n",
       "         [-8.32665041e-02, -1.08086146e-01,  2.58976743e-02,\n",
       "          -7.69985691e-02, -6.70558307e-03,  4.17869054e-02,\n",
       "          -2.33243443e-02,  5.30833043e-02,  6.40478581e-02,\n",
       "           4.49638404e-02],\n",
       "         [ 6.20322162e-03,  1.81707721e-02,  7.16571808e-02,\n",
       "           1.81092601e-02,  2.41570938e-02,  9.27290693e-02,\n",
       "           2.00992264e-02, -1.74226061e-01, -1.13082789e-01,\n",
       "          -1.40641928e-01],\n",
       "         [ 6.72089383e-02, -1.02142639e-01,  4.97347713e-02,\n",
       "          -2.53493041e-02,  4.62429831e-03,  7.00816438e-02,\n",
       "           1.93968173e-02, -1.82825938e-01,  5.25268652e-02,\n",
       "          -8.23768526e-02],\n",
       "         [-1.06452391e-01, -5.90825938e-02,  9.25222295e-04,\n",
       "           3.72615014e-03,  1.35498326e-02, -6.02949560e-02,\n",
       "           3.58265229e-02, -2.88647730e-02, -8.70016403e-03,\n",
       "           8.39356780e-02],\n",
       "         [-6.94983080e-03,  2.40866877e-02,  5.87310875e-03,\n",
       "           2.67924499e-02,  1.19245397e-02, -1.27744973e-01,\n",
       "           1.21671930e-02, -1.02291986e-01,  8.98180157e-02,\n",
       "          -2.20244005e-01],\n",
       "         [ 1.90360006e-02, -1.35511532e-01,  4.34265193e-03,\n",
       "          -2.66567282e-02,  4.76442091e-02, -2.00533438e-02,\n",
       "           4.82476735e-03,  1.55060273e-02,  1.79553144e-02,\n",
       "           5.34500927e-02],\n",
       "         [ 1.29184853e-02, -1.09191984e-02,  3.20167691e-02,\n",
       "          -1.04534887e-02,  2.40642764e-02,  9.45347697e-02,\n",
       "           5.55345193e-02, -1.59389108e-01, -8.40388089e-02,\n",
       "          -1.16938345e-01],\n",
       "         [-3.21382210e-02, -8.41639042e-02,  1.01696411e-02,\n",
       "           2.52311081e-02,  4.78151776e-02, -8.33120197e-02,\n",
       "          -9.80058312e-03, -2.19902787e-02,  5.59453033e-02,\n",
       "           6.00759685e-02],\n",
       "         [-7.19929934e-02,  1.51241615e-01, -4.78812419e-02,\n",
       "          -9.41268951e-02, -6.56884909e-02, -1.45570217e-02,\n",
       "           2.58220006e-02,  1.63402081e-01, -7.92123750e-03,\n",
       "          -1.78104401e-01],\n",
       "         [ 1.05891814e-02, -6.78176060e-03,  9.60543938e-03,\n",
       "           1.79141741e-02,  7.16751721e-03,  1.46150449e-02,\n",
       "          -2.28317245e-03,  2.02258173e-02, -4.22707805e-03,\n",
       "           6.03058701e-03],\n",
       "         [ 3.41528654e-02, -1.77910939e-01, -7.40753440e-03,\n",
       "           1.87461954e-02, -8.15067813e-02, -1.94009244e-02,\n",
       "           3.66826318e-02, -7.09692538e-02,  9.92354527e-02,\n",
       "           5.43607632e-03],\n",
       "         [ 3.04356590e-02,  1.20111797e-02,  2.16015447e-02,\n",
       "           6.93876017e-03,  2.62819137e-02, -3.51209372e-01,\n",
       "           1.04196835e-02, -5.59630580e-02,  6.11456782e-02,\n",
       "          -5.95592223e-02],\n",
       "         [ 1.31961638e-02,  1.36570185e-02,  1.22915078e-02,\n",
       "          -1.90814808e-02, -1.11121265e-02,  3.70579917e-04,\n",
       "          -6.25195028e-03,  6.82847481e-03,  1.01984860e-02,\n",
       "          -1.16054704e-02],\n",
       "         [ 1.49984760e-02, -6.42196983e-02,  9.13850516e-02,\n",
       "           6.04148582e-02, -1.07256062e-02, -8.10672417e-02,\n",
       "          -4.17373106e-02, -1.12864740e-01,  8.24321527e-03,\n",
       "           1.39886484e-01],\n",
       "         [-2.75174882e-02,  3.33769359e-02,  2.86314115e-02,\n",
       "           1.79996789e-02,  6.79725632e-02, -1.60356790e-01,\n",
       "           3.53796370e-02, -9.40834209e-02,  7.41975242e-03,\n",
       "          -1.42289460e-01],\n",
       "         [-7.92645849e-03,  1.15178907e-02,  1.07119502e-02,\n",
       "          -5.42661082e-03, -7.76739698e-03,  2.11558454e-02,\n",
       "           2.02899259e-02, -1.06659941e-02, -9.24985111e-03,\n",
       "          -8.70501250e-03],\n",
       "         [-6.28605410e-02,  6.29502684e-02, -4.24681455e-02,\n",
       "           4.36902558e-03,  8.26950148e-02,  2.22735237e-02,\n",
       "          -1.58551618e-01,  1.06282786e-01,  5.48525862e-02,\n",
       "          -1.19681627e-01],\n",
       "         [ 9.32679698e-03, -8.50624815e-02, -3.73711847e-02,\n",
       "          -8.56394600e-03, -8.46983958e-03,  1.11366153e-01,\n",
       "           7.27005303e-02, -4.02361825e-02,  1.00759596e-01,\n",
       "          -1.89157397e-01],\n",
       "         [-4.17092058e-04, -1.11201778e-01, -1.73126273e-02,\n",
       "          -7.81157464e-02, -1.53445870e-01,  6.79116026e-02,\n",
       "          -1.66100846e-03,  4.68699932e-02, -3.84787396e-02,\n",
       "           3.36952209e-02],\n",
       "         [-4.69646044e-03,  5.30818030e-02,  5.02763018e-02,\n",
       "           2.42458340e-02,  5.66116124e-02, -1.32768080e-01,\n",
       "          -1.38621420e-01,  1.35435238e-01, -3.15861427e-03,\n",
       "          -1.11850329e-01],\n",
       "         [ 3.04976515e-02,  1.89317480e-01, -8.27784091e-02,\n",
       "          -7.05154240e-02, -1.62457705e-01, -1.09523371e-01,\n",
       "          -2.70098429e-02, -5.39422408e-02,  1.88283548e-01,\n",
       "          -4.45352793e-02],\n",
       "         [ 3.51996794e-02, -3.98567989e-02, -1.22160427e-02,\n",
       "           3.26931626e-02,  4.57396172e-03, -9.57522541e-03,\n",
       "           6.49839789e-02, -1.27371773e-01,  1.11214118e-02,\n",
       "          -1.51069090e-01],\n",
       "         [ 1.63625795e-02, -3.98338027e-02,  3.98164578e-02,\n",
       "          -1.80859089e-01, -5.89217879e-02,  6.84451163e-02,\n",
       "          -7.29349181e-02,  6.75512329e-02, -1.14098564e-01,\n",
       "           7.49103501e-02],\n",
       "         [-3.83225270e-02,  4.62781936e-02,  8.70023742e-02,\n",
       "           3.61668020e-02,  5.88746630e-02,  1.41146379e-02,\n",
       "          -3.48974206e-02, -2.33536363e-01, -1.39754608e-01,\n",
       "          -5.30459061e-02],\n",
       "         [ 3.92068066e-02,  2.29485473e-03,  5.15216552e-02,\n",
       "           2.78177485e-02, -2.92076953e-02, -7.23249242e-02,\n",
       "           3.84668224e-02, -1.87035248e-01, -8.10658634e-02,\n",
       "          -7.05703869e-02],\n",
       "         [-3.42100412e-02,  5.72962686e-02, -1.41599223e-01,\n",
       "           5.23829386e-02,  1.04534641e-01, -7.75200203e-02,\n",
       "          -1.11503989e-01, -2.74175286e-01, -1.71674136e-02,\n",
       "          -2.75231395e-02],\n",
       "         [-3.20682675e-02,  1.11898473e-02, -1.01957485e-01,\n",
       "           7.59098977e-02, -4.09765244e-02, -1.90511420e-01,\n",
       "           1.75868664e-02,  2.43355916e-03,  3.32995877e-02,\n",
       "           6.75910935e-02],\n",
       "         [-1.14340559e-01,  7.48161450e-02,  2.70136409e-02,\n",
       "          -6.54011965e-02,  1.02323942e-01, -8.35364393e-04,\n",
       "          -5.83825931e-02,  8.70295838e-02,  5.17559759e-02,\n",
       "          -1.42523348e-01],\n",
       "         [-8.05468559e-02,  5.74004687e-02, -6.96844608e-02,\n",
       "          -2.69714538e-02, -1.00155413e-01,  5.82979620e-02,\n",
       "          -4.80877310e-02,  7.84503296e-02, -1.24831542e-01,\n",
       "           9.11809411e-03],\n",
       "         [-2.86653615e-03,  6.70404686e-03, -2.78415508e-03,\n",
       "           2.95270551e-02,  5.10326214e-02, -2.32365936e-01,\n",
       "           2.85359416e-02, -7.21775144e-02, -1.20864622e-01,\n",
       "           1.66822877e-02],\n",
       "         [ 3.71154514e-03,  1.16007831e-02, -3.02944165e-02,\n",
       "           3.79244797e-03, -6.29306724e-03,  2.85892142e-03,\n",
       "          -6.82395650e-03,  1.11390594e-02, -2.57489155e-03,\n",
       "           2.86321738e-03],\n",
       "         [ 1.15203582e-01,  1.10424832e-02, -1.40077442e-01,\n",
       "          -2.05391329e-02, -4.22030799e-02, -3.97091098e-02,\n",
       "          -6.05305396e-02,  1.53435230e-01, -2.66115498e-02,\n",
       "          -1.28584474e-01],\n",
       "         [-1.09092437e-01,  8.63253884e-03, -1.04611985e-01,\n",
       "           4.69381362e-02,  3.75971533e-02,  4.31615449e-02,\n",
       "          -2.28148904e-02,  3.31871137e-02,  7.80054852e-02,\n",
       "          -1.41246706e-01],\n",
       "         [ 2.92755198e-02,  8.62726644e-02, -2.59206325e-01,\n",
       "          -7.84771666e-02,  1.06882642e-03,  1.25229135e-02,\n",
       "           1.46685064e-01, -6.49498329e-02,  4.73317839e-02,\n",
       "           4.76425700e-02],\n",
       "         [ 4.01451439e-02,  1.94508769e-02,  6.91444650e-02,\n",
       "           5.56243844e-02, -1.85223117e-01, -2.24874783e-02,\n",
       "          -1.94236636e-03, -2.44677439e-01,  1.61093213e-02,\n",
       "          -7.92583451e-02],\n",
       "         [ 3.36629823e-02, -2.05640569e-02,  2.86815193e-04,\n",
       "          -2.32271920e-03,  2.18258016e-02,  5.95654920e-02,\n",
       "           3.31659578e-02, -1.61061272e-01,  2.96076126e-02,\n",
       "          -1.90874249e-01],\n",
       "         [-6.56716079e-02, -6.69007301e-02,  3.27728577e-02,\n",
       "           1.02390964e-02,  1.94841474e-02,  2.31289174e-02,\n",
       "           2.33017597e-02, -6.96979910e-02, -1.42975017e-01,\n",
       "           8.44673514e-02],\n",
       "         [-1.10020965e-01,  1.03195906e-01, -6.94582239e-02,\n",
       "          -1.02779521e-02, -9.65736061e-02,  7.08725536e-03,\n",
       "          -3.90939415e-02,  1.02404058e-01, -7.49230608e-02,\n",
       "          -2.71349046e-02],\n",
       "         [ 4.99258516e-03, -3.33155901e-03, -1.33821350e-02,\n",
       "          -1.29019208e-02, -6.12901617e-03, -1.57827772e-02,\n",
       "          -1.49797497e-03,  1.29077043e-02, -1.26368059e-02,\n",
       "          -7.26997526e-03],\n",
       "         [ 2.38164477e-02, -1.07209068e-02,  5.34376875e-02,\n",
       "           4.41589467e-02, -1.08301587e-01, -1.17546367e-02,\n",
       "           2.68780813e-02, -2.61574179e-01, -4.68664132e-02,\n",
       "          -7.76719600e-02],\n",
       "         [-2.01107915e-02,  2.21545100e-01, -1.25168204e-01,\n",
       "          -2.73813400e-02, -1.38106361e-01, -4.80122082e-02,\n",
       "          -7.47754201e-02, -1.49703771e-02,  1.85136199e-01,\n",
       "          -3.89227122e-02],\n",
       "         [-2.08132975e-02,  4.51686978e-02,  3.73372473e-02,\n",
       "           4.89132963e-02,  2.66001355e-02, -2.26368785e-01,\n",
       "          -2.99716392e-03, -8.73560607e-02, -9.13913324e-02,\n",
       "          -3.86569872e-02],\n",
       "         [ 2.41036993e-02,  4.40257043e-02,  1.16111347e-02,\n",
       "           1.57061871e-02,  5.57981208e-02, -8.22286308e-02,\n",
       "           5.33053018e-02, -1.34047717e-01, -1.92796305e-01,\n",
       "           5.73899969e-03],\n",
       "         [-1.71125866e-03,  7.58734904e-03, -8.59948248e-03,\n",
       "           1.40499137e-03,  6.34647207e-03, -7.40956562e-03,\n",
       "           3.60601029e-04, -1.03341416e-02,  5.15444728e-04,\n",
       "           1.04488917e-02],\n",
       "         [ 6.61881501e-03, -1.46554038e-02, -2.36785337e-02,\n",
       "          -1.42098591e-03, -2.86881509e-03, -3.75789893e-03,\n",
       "          -1.37939514e-03, -1.95109602e-02, -1.62183354e-03,\n",
       "          -1.56313814e-02],\n",
       "         [ 3.17558311e-02, -1.83853880e-02,  2.65195388e-02,\n",
       "           1.79143772e-02,  1.68439113e-02,  6.69266656e-02,\n",
       "           5.18834330e-02, -1.31904453e-01, -2.39409953e-02,\n",
       "          -1.54785037e-01],\n",
       "         [ 1.23378254e-01,  7.75078806e-05, -1.37181759e-01,\n",
       "           1.57139730e-02, -1.39638454e-01, -7.83175156e-02,\n",
       "           8.40923470e-03, -1.11168548e-01, -6.33630157e-02,\n",
       "          -4.99688052e-02],\n",
       "         [-8.53509679e-02, -4.41097729e-02, -7.49709308e-02,\n",
       "          -7.21222488e-03, -3.18883434e-02,  1.90317277e-02,\n",
       "          -1.15293622e-01,  4.52945046e-02,  6.94286302e-02,\n",
       "           6.75396994e-02],\n",
       "         [ 4.17859992e-03, -2.23075971e-03,  1.56764444e-02,\n",
       "          -2.10342221e-02,  1.23568363e-02, -8.58535897e-03,\n",
       "          -1.29506541e-02, -1.38141233e-02, -3.05095967e-02,\n",
       "          -1.72051638e-02],\n",
       "         [ 1.64599735e-02, -4.20404691e-03,  4.02735732e-02,\n",
       "          -3.05841072e-03,  2.07703505e-02,  5.59509657e-02,\n",
       "           3.01150084e-02, -1.54620126e-01, -3.11951563e-02,\n",
       "          -1.52389362e-01],\n",
       "         [-1.08999103e-01,  4.23046760e-02, -7.12326542e-02,\n",
       "           2.58391947e-02, -4.53185290e-02,  2.55398061e-02,\n",
       "          -1.09384939e-01,  7.20149726e-02, -3.70308086e-02,\n",
       "           8.39931890e-02],\n",
       "         [ 5.89707568e-02, -7.66477734e-02,  1.91481840e-02,\n",
       "          -1.90697461e-02,  3.59150171e-02,  7.34096542e-02,\n",
       "           1.61872990e-02, -1.73684984e-01, -1.16880937e-03,\n",
       "          -1.34052575e-01],\n",
       "         [-8.05205572e-03,  8.70339107e-03, -7.40265753e-03,\n",
       "           1.03368703e-02,  3.16780992e-02, -1.52050341e-02,\n",
       "           2.17444487e-02, -1.33056659e-03, -2.02647895e-02,\n",
       "          -6.00833306e-03],\n",
       "         [ 3.80675076e-03,  6.56684535e-03, -7.83093367e-03,\n",
       "           1.11466823e-02, -8.36342014e-03,  1.00871529e-02,\n",
       "           1.46117778e-02,  3.17875086e-03, -2.20129881e-02,\n",
       "          -5.44556184e-03],\n",
       "         [ 5.68563165e-03,  2.58362386e-03,  1.76387641e-06,\n",
       "           1.59060061e-02, -7.64545659e-03, -9.29298252e-03,\n",
       "           1.88386310e-02, -8.69097468e-03,  1.42606301e-03,\n",
       "          -5.07691503e-03],\n",
       "         [-4.29281816e-02,  1.07596934e-01,  2.63558440e-02,\n",
       "          -1.56802565e-01,  1.98465511e-02,  1.64803509e-02,\n",
       "          -6.34055259e-03, -1.06479257e-01,  4.54608575e-02,\n",
       "           9.64635462e-02],\n",
       "         [ 8.48432481e-02,  3.89001518e-03, -1.95760295e-01,\n",
       "          -3.80146690e-03, -7.41771460e-02, -4.35220525e-02,\n",
       "           6.64457753e-02, -2.10354224e-01, -5.60168512e-02,\n",
       "          -1.35578951e-02],\n",
       "         [-2.88769300e-03, -7.33603165e-02,  8.50366801e-02,\n",
       "           4.34822552e-02, -1.26727298e-02,  2.65167244e-02,\n",
       "          -6.05259687e-02, -6.75763562e-02, -1.40431538e-01,\n",
       "           1.25562087e-01],\n",
       "         [-7.69766420e-02, -7.32626244e-02, -1.30695269e-01,\n",
       "          -1.14726881e-03, -6.15351051e-02,  3.59160230e-02,\n",
       "           1.62275732e-02,  4.49064374e-02,  2.22117808e-02,\n",
       "           5.77413179e-02]], dtype=float32),\n",
       "  array([[ 0.00998389,  0.00462808,  0.00538132, -0.0392677 , -0.04658761,\n",
       "          -0.00929451, -0.02650512,  0.01716711,  0.11596692,  0.02330743]],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = model.get_parameters()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfd9f0",
   "metadata": {},
   "source": [
    "### Setting Parameters\n",
    "By loading weights and biases directly, models that don't need any optimizing are now possible. To account for them, finalize and the set methods in the model are now updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9b46157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "        \n",
    "    # Retrive layer paremters\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "    \n",
    "    # Set weights and biases in a layer instance\n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # If not in the training mode - return values\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# Input \"layer\"\n",
    "class Layer_Input:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        self.output = inputs\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return (outputs > 0.5) * 1\n",
    "\n",
    "\n",
    "# Linear activation\n",
    "class Activation_Linear:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, training):\n",
    "        # Just remember values\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "    # Calculate predictions for outputs\n",
    "    def predictions(self, outputs):\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # Calculate regularization loss\n",
    "        # iterate all trainable layers\n",
    "        for layer in self.trainable_layers:\n",
    "\n",
    "            # L1 regularization - weights\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.weights))\n",
    "\n",
    "            # L2 regularization - weights\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                       np.sum(layer.weights * \\\n",
    "                                              layer.weights)\n",
    "\n",
    "            # L1 regularization - biases\n",
    "            # calculate only when factor greater than 0\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                       np.sum(np.abs(layer.biases))\n",
    "\n",
    "            # L2 regularization - biases\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                       np.sum(layer.biases * \\\n",
    "                                              layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "\n",
    "    # Set/remember trainable layers\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Add accumulated sum of losses and sample count\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Calculates accumulated loss\n",
    "    def calculate_accumulated(self, *, include_regularization=False):\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # If just data loss - return it\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    # Reset variables for accumulated loss\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):  # L2 loss\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):  # L1 loss\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "\n",
    "    # Calculates an accuracy\n",
    "    # given predictions and ground truth values\n",
    "    def calculate(self, predictions, y):\n",
    "\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "\n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "\n",
    "    # Calculates accumulated accuracy\n",
    "    def calculate_accumulated(self):\n",
    "\n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "\n",
    "    # Reset variables for accumulated accuracy\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "\n",
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "\n",
    "    def __init__(self, *, binary=False):\n",
    "        # Binary mode?\n",
    "        self.binary = binary\n",
    "\n",
    "    # No initialization is needed\n",
    "    def init(self, y):\n",
    "        pass\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        if not self.binary and len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y\n",
    "\n",
    "\n",
    "# Accuracy calculation for regression model\n",
    "class Accuracy_Regression(Accuracy):\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create precision property\n",
    "        self.precision = None\n",
    "\n",
    "    # Calculates precision value\n",
    "    # based on passed-in ground truth values\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    # Compares predictions to the ground truth values\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y) < self.precision\n",
    "\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set(self, *, loss = None, optimizer = None, accuracy = None):\n",
    "        if loss is not None:\n",
    "            self.loss = loss\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy is not None:\n",
    "            self.accuracy = accuracy\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        if self.loss is not None:\n",
    "            self.loss.remember_trainable_layers(\n",
    "                self.trainable_layers\n",
    "            )\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            # Create an object of combined activation\n",
    "            # and loss functions\n",
    "            self.softmax_classifier_output = \\\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
    "              print_every=1, validation_data=None):\n",
    "\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "\n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data but nor full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            # Print epoch number\n",
    "            print(f'epoch: {epoch}')\n",
    "\n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = \\\n",
    "                    self.loss.calculate(output, batch_y,\n",
    "                                        include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(\n",
    "                                  output)\n",
    "                accuracy = self.accuracy.calculate(predictions,\n",
    "                                                   batch_y)\n",
    "\n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "\n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = \\\n",
    "                self.loss.calculate_accumulated(\n",
    "                    include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "\n",
    "                # evaluate the model\n",
    "                self.evaluate(*validation_data,\n",
    "                              batch_size = batch_size)\n",
    "                # * explodes the data into individual arguments\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "\n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = \\\n",
    "                self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "    \n",
    "    # evaluates the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size = None):\n",
    "        \n",
    "        # default value if batch size is not being set\n",
    "        validation_steps = 1\n",
    "        \n",
    "        # calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            # another batch from remainders\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "\n",
    "        # Reset accumulated values in loss\n",
    "        # and accuracy objects\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        # Iterate over steps\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            # If batch size is not set -\n",
    "            # train using one step and full dataset\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "\n",
    "            # Otherwise slice a batch\n",
    "            else:\n",
    "                batch_X = X_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "                batch_y = y_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(batch_X, training=False)\n",
    "\n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "\n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(\n",
    "                              output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        # Get and print validation loss and accuracy\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        # Print a summary\n",
    "        print(f'validation, ' +\n",
    "              f'acc: {validation_accuracy:.3f}, ' +\n",
    "              f'loss: {validation_loss:.3f}')\n",
    "\n",
    "    # Retrieves and returns parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        \n",
    "        # create a list for paremeters\n",
    "        parameters = []\n",
    "        \n",
    "        # iterate trainable layers and get their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            \n",
    "        # return a list\n",
    "        return parameters\n",
    "    \n",
    "    # Updates the model with new parameters\n",
    "    def set_parameters(self, parameters):\n",
    "        \n",
    "        # iterate over the parameters and layers\n",
    "        # and update each layers with each set of the parameters\n",
    "        for parameter_set, layer in zip(parameters, \n",
    "                                        self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "\n",
    "# Loads a MNIST dataset\n",
    "def load_mnist_dataset(dataset, path):\n",
    "\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels = os.listdir(os.path.join(path, dataset))\n",
    "\n",
    "    # Create lists for samples and labels\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            # Read the image\n",
    "            image = cv2.imread(\n",
    "                        os.path.join(path, dataset, label, file),\n",
    "                        cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(label)\n",
    "\n",
    "    # Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y).astype('uint8')\n",
    "\n",
    "\n",
    "# MNIST dataset (train + test)\n",
    "def create_data_mnist(path):\n",
    "\n",
    "    # Load both sets separately\n",
    "    X, y = load_mnist_dataset('train', path)\n",
    "    X_test, y_test = load_mnist_dataset('test', path)\n",
    "\n",
    "    # And return all the data\n",
    "    return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18819936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.860, loss: 0.399\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# set only loss and accuracy because optimizer is now unneeded\n",
    "model.set(\n",
    "    loss = Loss_CategoricalCrossentropy(),\n",
    "    accuracy = Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# set model with parameters instead of training it\n",
    "model.set_parameters(parameters)\n",
    "\n",
    "# evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36c4ce",
   "metadata": {},
   "source": [
    "### Saving and Loading Parameters\n",
    "The Pickle library can serialize any Python object into binary representation, and either return the bytes or save them to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9f9a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set(self, *, loss = None, optimizer = None, accuracy = None):\n",
    "        if loss is not None:\n",
    "            self.loss = loss\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy is not None:\n",
    "            self.accuracy = accuracy\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        if self.loss is not None:\n",
    "            self.loss.remember_trainable_layers(\n",
    "                self.trainable_layers\n",
    "            )\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            # Create an object of combined activation\n",
    "            # and loss functions\n",
    "            self.softmax_classifier_output = \\\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
    "              print_every=1, validation_data=None):\n",
    "\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "\n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data but nor full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            # Print epoch number\n",
    "            print(f'epoch: {epoch}')\n",
    "\n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = \\\n",
    "                    self.loss.calculate(output, batch_y,\n",
    "                                        include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(\n",
    "                                  output)\n",
    "                accuracy = self.accuracy.calculate(predictions,\n",
    "                                                   batch_y)\n",
    "\n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "\n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = \\\n",
    "                self.loss.calculate_accumulated(\n",
    "                    include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "\n",
    "                # evaluate the model\n",
    "                self.evaluate(*validation_data,\n",
    "                              batch_size = batch_size)\n",
    "                # * explodes the data into individual arguments\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "\n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = \\\n",
    "                self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "    \n",
    "    # evaluates the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size = None):\n",
    "        \n",
    "        # default value if batch size is not being set\n",
    "        validation_steps = 1\n",
    "        \n",
    "        # calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            # another batch from remainders\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "\n",
    "        # Reset accumulated values in loss\n",
    "        # and accuracy objects\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        # Iterate over steps\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            # If batch size is not set -\n",
    "            # train using one step and full dataset\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "\n",
    "            # Otherwise slice a batch\n",
    "            else:\n",
    "                batch_X = X_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "                batch_y = y_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(batch_X, training=False)\n",
    "\n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "\n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(\n",
    "                              output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        # Get and print validation loss and accuracy\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        # Print a summary\n",
    "        print(f'validation, ' +\n",
    "              f'acc: {validation_accuracy:.3f}, ' +\n",
    "              f'loss: {validation_loss:.3f}')\n",
    "\n",
    "    # Retrieves and returns parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        \n",
    "        # create a list for paremeters\n",
    "        parameters = []\n",
    "        \n",
    "        # iterate trainable layers and get their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            \n",
    "        # return a list\n",
    "        return parameters\n",
    "    \n",
    "    # Updates the model with new parameters\n",
    "    def set_parameters(self, parameters):\n",
    "        \n",
    "        # iterate over the parameters and layers\n",
    "        # and update each layers with each set of the parameters\n",
    "        for parameter_set, layer in zip(parameters, \n",
    "                                        self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "            \n",
    "    # saves the parameters to a file\n",
    "    def save_parameters(self, path):\n",
    "        \n",
    "        # open a file in the binary-write mode\n",
    "        # and save parameters to it\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.get_parameters(), f)\n",
    "            \n",
    "    # loads the weights and updates a model instance with them\n",
    "    def load_paremeters(self, path):\n",
    "        \n",
    "        # open file in the binary-read mode,\n",
    "        # load weights and update trainabe layers\n",
    "        with open(path, 'rb') as f:\n",
    "            self.set_parameters(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80521dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.860, loss: 0.399\n",
      "validation, acc: 0.860, loss: 0.399\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Add layers\n",
    "model.add(Layer_Dense(X.shape[1], 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 64))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(64, 10))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "# set only loss and accuracy because optimizer is now unneeded\n",
    "model.set(\n",
    "    loss = Loss_CategoricalCrossentropy(),\n",
    "    accuracy = Accuracy_Categorical()\n",
    ")\n",
    "\n",
    "# Finalize the model\n",
    "model.finalize()\n",
    "\n",
    "# set model with parameters instead of training it\n",
    "model.set_parameters(parameters)\n",
    "\n",
    "# evaluate the model\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "model.save_parameters('fashion_mnist.parms')\n",
    "\n",
    "model.load_paremeters('fashion_mnist.parms')\n",
    "\n",
    "# evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48334c2c",
   "metadata": {},
   "source": [
    "### Saving and Loading the Entire Model\n",
    "Larger models may want to be able to be saved with all of its properties, including the optimizer, as the training process can take very long times, so needs to be saved and loaded\n",
    "\n",
    "For this purpose, the copy.deepcopy method will be used to copy the mode, the new_pass() method being called on it to reset accumulated values, the output from the input layer popped, the dinputs from the output (loss) layer popped, and the dinputs, output, dinputs, dweights, dbiases from all the layers popped before being stored as a binary object into a given file\n",
    "\n",
    "As for loading, since the loading functin will likely be called before a model object is even created, the @staticmethod decorator will be used to make it possible to use the load function in the case self object does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8a3e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create a list of network objects\n",
    "        self.layers = []\n",
    "        # Softmax classifier's output object\n",
    "        self.softmax_classifier_output = None\n",
    "\n",
    "    # Add objects to the model\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "    # Set loss, optimizer and accuracy\n",
    "    def set(self, *, loss = None, optimizer = None, accuracy = None):\n",
    "        if loss is not None:\n",
    "            self.loss = loss\n",
    "        if optimizer is not None:\n",
    "            self.optimizer = optimizer\n",
    "        if accuracy is not None:\n",
    "            self.accuracy = accuracy\n",
    "\n",
    "    # Finalize the model\n",
    "    def finalize(self):\n",
    "\n",
    "        # Create and set the input layer\n",
    "        self.input_layer = Layer_Input()\n",
    "\n",
    "        # Count all the objects\n",
    "        layer_count = len(self.layers)\n",
    "\n",
    "        # Initialize a list containing trainable layers:\n",
    "        self.trainable_layers = []\n",
    "\n",
    "        # Iterate the objects\n",
    "        for i in range(layer_count):\n",
    "\n",
    "            # If it's the first layer,\n",
    "            # the previous layer object is the input layer\n",
    "            if i == 0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # All layers except for the first and the last\n",
    "            elif i < layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "\n",
    "            # The last layer - the next object is the loss\n",
    "            # Also let's save aside the reference to the last object\n",
    "            # whose output is the model's output\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "\n",
    "            # If layer contains an attribute called \"weights\",\n",
    "            # it's a trainable layer -\n",
    "            # add it to the list of trainable layers\n",
    "            # We don't need to check for biases -\n",
    "            # checking for weights is enough\n",
    "            if hasattr(self.layers[i], 'weights'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "\n",
    "\n",
    "        # Update loss object with trainable layers\n",
    "        if self.loss is not None:\n",
    "            self.loss.remember_trainable_layers(\n",
    "                self.trainable_layers\n",
    "            )\n",
    "\n",
    "        # If output activation is Softmax and\n",
    "        # loss function is Categorical Cross-Entropy\n",
    "        # create an object of combined activation\n",
    "        # and loss function containing\n",
    "        # faster gradient calculation\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and \\\n",
    "           isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            # Create an object of combined activation\n",
    "            # and loss functions\n",
    "            self.softmax_classifier_output = \\\n",
    "                Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Train the model\n",
    "    def train(self, X, y, *, epochs=1, batch_size=None,\n",
    "              print_every=1, validation_data=None):\n",
    "\n",
    "        # Initialize accuracy object\n",
    "        self.accuracy.init(y)\n",
    "\n",
    "        # Default value if batch size is not being set\n",
    "        train_steps = 1\n",
    "\n",
    "        # If there is validation data passed,\n",
    "        # set default number of steps for validation as well\n",
    "        if validation_data is not None:\n",
    "            validation_steps = 1\n",
    "\n",
    "            # For better readability\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        # Calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            # Dividing rounds down. If there are some remaining\n",
    "            # data but not a full batch, this won't include it\n",
    "            # Add `1` to include this not full batch\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps += 1\n",
    "\n",
    "            if validation_data is not None:\n",
    "                validation_steps = len(X_val) // batch_size\n",
    "\n",
    "                # Dividing rounds down. If there are some remaining\n",
    "                # data but nor full batch, this won't include it\n",
    "                # Add `1` to include this not full batch\n",
    "                if validation_steps * batch_size < len(X_val):\n",
    "                    validation_steps += 1\n",
    "\n",
    "        # Main training loop\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            # Print epoch number\n",
    "            print(f'epoch: {epoch}')\n",
    "\n",
    "            # Reset accumulated values in loss and accuracy objects\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "\n",
    "            # Iterate over steps\n",
    "            for step in range(train_steps):\n",
    "\n",
    "                # If batch size is not set -\n",
    "                # train using one step and full dataset\n",
    "                if batch_size is None:\n",
    "                    batch_X = X\n",
    "                    batch_y = y\n",
    "\n",
    "                # Otherwise slice a batch\n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "                # Perform the forward pass\n",
    "                output = self.forward(batch_X, training=True)\n",
    "\n",
    "                # Calculate loss\n",
    "                data_loss, regularization_loss = \\\n",
    "                    self.loss.calculate(output, batch_y,\n",
    "                                        include_regularization=True)\n",
    "                loss = data_loss + regularization_loss\n",
    "\n",
    "                # Get predictions and calculate an accuracy\n",
    "                predictions = self.output_layer_activation.predictions(\n",
    "                                  output)\n",
    "                accuracy = self.accuracy.calculate(predictions,\n",
    "                                                   batch_y)\n",
    "\n",
    "                # Perform backward pass\n",
    "                self.backward(output, batch_y)\n",
    "\n",
    "\n",
    "                # Optimize (update parameters)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "\n",
    "                # Print a summary\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step}, ' +\n",
    "                          f'acc: {accuracy:.3f}, ' +\n",
    "                          f'loss: {loss:.3f} (' +\n",
    "                          f'data_loss: {data_loss:.3f}, ' +\n",
    "                          f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "                          f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # Get and print epoch loss and accuracy\n",
    "            epoch_data_loss, epoch_regularization_loss = \\\n",
    "                self.loss.calculate_accumulated(\n",
    "                    include_regularization=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "            print(f'training, ' +\n",
    "                  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "                  f'loss: {epoch_loss:.3f} (' +\n",
    "                  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "                  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "                  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "            # If there is the validation data\n",
    "            if validation_data is not None:\n",
    "\n",
    "                # evaluate the model\n",
    "                self.evaluate(*validation_data,\n",
    "                              batch_size = batch_size)\n",
    "                # * explodes the data into individual arguments\n",
    "\n",
    "    # Performs forward pass\n",
    "    def forward(self, X, training):\n",
    "\n",
    "        # Call forward method on the input layer\n",
    "        # this will set the output property that\n",
    "        # the first layer in \"prev\" object is expecting\n",
    "        self.input_layer.forward(X, training)\n",
    "\n",
    "        # Call forward method of every object in a chain\n",
    "        # Pass output of the previous object as a parameter\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "\n",
    "        # \"layer\" is now the last object from the list,\n",
    "        # return its output\n",
    "        return layer.output\n",
    "\n",
    "\n",
    "    # Performs backward pass\n",
    "    def backward(self, output, y):\n",
    "\n",
    "        # If softmax classifier\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            # First call backward method\n",
    "            # on the combined activation/loss\n",
    "            # this will set dinputs property\n",
    "            self.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "            # Since we'll not call backward method of the last layer\n",
    "            # which is Softmax activation\n",
    "            # as we used combined activation/loss\n",
    "            # object, let's set dinputs in this object\n",
    "            self.layers[-1].dinputs = \\\n",
    "                self.softmax_classifier_output.dinputs\n",
    "\n",
    "            # Call backward method going through\n",
    "            # all the objects but last\n",
    "            # in reversed order passing dinputs as a parameter\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "\n",
    "            return\n",
    "\n",
    "        # First call backward method on the loss\n",
    "        # this will set dinputs property that the last\n",
    "        # layer will try to access shortly\n",
    "        self.loss.backward(output, y)\n",
    "\n",
    "        # Call backward method going through all the objects\n",
    "        # in reversed order passing dinputs as a parameter\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "    \n",
    "    # evaluates the model using passed in dataset\n",
    "    def evaluate(self, X_val, y_val, *, batch_size = None):\n",
    "        \n",
    "        # default value if batch size is not being set\n",
    "        validation_steps = 1\n",
    "        \n",
    "        # calculate number of steps\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            # another batch from remainders\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "\n",
    "        # Reset accumulated values in loss\n",
    "        # and accuracy objects\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "\n",
    "        # Iterate over steps\n",
    "        for step in range(validation_steps):\n",
    "\n",
    "            # If batch size is not set -\n",
    "            # train using one step and full dataset\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "\n",
    "\n",
    "            # Otherwise slice a batch\n",
    "            else:\n",
    "                batch_X = X_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "                batch_y = y_val[\n",
    "                    step*batch_size:(step+1)*batch_size\n",
    "                ]\n",
    "\n",
    "            # Perform the forward pass\n",
    "            output = self.forward(batch_X, training=False)\n",
    "\n",
    "            # Calculate the loss\n",
    "            self.loss.calculate(output, batch_y)\n",
    "\n",
    "            # Get predictions and calculate an accuracy\n",
    "            predictions = self.output_layer_activation.predictions(\n",
    "                              output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        # Get and print validation loss and accuracy\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "        # Print a summary\n",
    "        print(f'validation, ' +\n",
    "              f'acc: {validation_accuracy:.3f}, ' +\n",
    "              f'loss: {validation_loss:.3f}')\n",
    "\n",
    "    # Retrieves and returns parameters of trainable layers\n",
    "    def get_parameters(self):\n",
    "        \n",
    "        # create a list for paremeters\n",
    "        parameters = []\n",
    "        \n",
    "        # iterate trainable layers and get their parameters\n",
    "        for layer in self.trainable_layers:\n",
    "            parameters.append(layer.get_parameters())\n",
    "            \n",
    "        # return a list\n",
    "        return parameters\n",
    "    \n",
    "    # Updates the model with new parameters\n",
    "    def set_parameters(self, parameters):\n",
    "        \n",
    "        # iterate over the parameters and layers\n",
    "        # and update each layers with each set of the parameters\n",
    "        for parameter_set, layer in zip(parameters, \n",
    "                                        self.trainable_layers):\n",
    "            layer.set_parameters(*parameter_set)\n",
    "            \n",
    "    # saves the parameters to a file\n",
    "    def save_parameters(self, path):\n",
    "        \n",
    "        # open a file in the binary-write mode\n",
    "        # and save parameters to it\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.get_parameters(), f)\n",
    "            \n",
    "    # loads the weights and updates a model instance with them\n",
    "    def load_paremeters(self, path):\n",
    "        \n",
    "        # open file in the binary-read mode,\n",
    "        # load weights and update trainabe layers\n",
    "        with open(path, 'rb') as f:\n",
    "            self.set_parameters(pickle.load(f))\n",
    "            \n",
    "    def save(self, path):\n",
    "        \n",
    "        # make a deepy copy of the current model instance\n",
    "        model = copy.deepcopy(self)\n",
    "        \n",
    "        # reset accumulated values in loss and accuracy objects\n",
    "        model.loss.new_pass()\n",
    "        mode.accuracy.new_pass()\n",
    "        \n",
    "        # remove data from the input layer\n",
    "        # and gradients from the loss object\n",
    "        model.input_layer.__dict__.pop('output', None)\n",
    "        model.loss.__dict__.pop('dinputs', None)\n",
    "        \n",
    "        # for each layer remove inputs, outputs and gradient properties\n",
    "        for layer in model.layers:\n",
    "            for property in ['inputs', 'output', 'dinputs', \n",
    "                             'dweights', 'dbiases']:\n",
    "                layer.__dict__pop(property, None)\n",
    "                \n",
    "        # open a file in the binary-write mode and save the model\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "            \n",
    "    # loads and returns a model\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \n",
    "        # open file in the binary-read mode\n",
    "        with open(path, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8717141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
