{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df019e0",
   "metadata": {},
   "source": [
    "## Full Code Up Until This Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ca304e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):        \n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # gradient on values to pass further back\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# Rectified Linear Unit Activation Function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        #create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # calculate jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # calculate sample wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    #inheriting from the base Loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred will come from the neural network\n",
    "        # y_true will come from the training set\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # means scalar class values have been passed\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # one hot encoded values have been passed\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Backwards pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        # number of labels in every sample\n",
    "        # we'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # if labels are sparse, turn them into one-hot vector\n",
    "        # in case the shape is as [3, 0, 2] etc etc per sample\n",
    "        # we turn them into one-hot vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        # with a larger number of batches, it's all summed together\n",
    "        # in the dot product, and some will be given more importance\n",
    "        # than others when we don't normaize\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # creates activation and loss function objects inside\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # set the output\n",
    "        self.output = self.activation.output\n",
    "        # calculate and return the loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #if labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "            \n",
    "        # copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1.0, decay = 0., momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.momentum:\n",
    "            \n",
    "            #if layer does not contain momentum arrays\n",
    "            # create them and fill with zeroes\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                \n",
    "                # if there is no momentum array for weights\n",
    "                # the array doesn't exist for the biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "            # build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        # otherwise vanillae SGD\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "        \n",
    "        # update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1., decay = 0., epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                    (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                    (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                layer.weight_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                                layer.bias_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start wit h1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        # get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            weight_momentums_corrected / \\\n",
    "                            (np.sqrt(weight_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            bias_momentums_corrected / \\\n",
    "                            (np.sqrt(bias_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d3b9207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.670, loss: 0.705, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.797, loss: 0.522, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.847, loss: 0.430, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.887, loss: 0.344, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.910, loss: 0.303, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.907, loss: 0.276, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.917, loss: 0.252, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.920, loss: 0.245, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.930, loss: 0.228, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.940, loss: 0.217, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.937, loss: 0.205, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.947, loss: 0.192, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.947, loss: 0.184, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.943, loss: 0.183, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.943, loss: 0.189, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.943, loss: 0.165, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.943, loss: 0.161, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.943, loss: 0.158, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.943, loss: 0.155, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.947, loss: 0.151, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.943, loss: 0.148, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.953, loss: 0.145, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.957, loss: 0.142, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.957, loss: 0.138, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.957, loss: 0.135, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.960, loss: 0.132, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.957, loss: 0.130, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.937, loss: 0.159, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.960, loss: 0.125, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.960, loss: 0.123, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.960, loss: 0.121, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.960, loss: 0.119, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.960, loss: 0.117, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.960, loss: 0.115, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.950, loss: 0.131, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.957, loss: 0.118, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.960, loss: 0.113, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.960, loss: 0.111, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.960, loss: 0.110, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.960, loss: 0.109, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.960, loss: 0.107, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.960, loss: 0.106, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.960, loss: 0.104, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.960, loss: 0.103, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.960, loss: 0.102, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.960, loss: 0.101, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.960, loss: 0.100, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.960, loss: 0.099, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.960, loss: 0.098, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.960, loss: 0.098, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.967, loss: 0.107, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.960, loss: 0.097, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.960, loss: 0.096, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.960, loss: 0.095, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.960, loss: 0.094, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.960, loss: 0.094, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.960, loss: 0.093, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.960, loss: 0.092, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.960, loss: 0.092, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.960, loss: 0.091, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.960, loss: 0.090, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.960, loss: 0.089, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.960, loss: 0.089, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.960, loss: 0.088, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.953, loss: 0.126, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.960, loss: 0.090, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.960, loss: 0.088, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.960, loss: 0.088, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.960, loss: 0.087, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.960, loss: 0.086, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.960, loss: 0.086, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.960, loss: 0.085, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.960, loss: 0.085, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.960, loss: 0.084, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.960, loss: 0.083, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.960, loss: 0.083, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.960, loss: 0.082, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.960, loss: 0.082, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.963, loss: 0.081, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.963, loss: 0.081, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.963, loss: 0.080, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.963, loss: 0.084, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.963, loss: 0.080, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.963, loss: 0.079, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.963, loss: 0.079, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.963, loss: 0.078, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.963, loss: 0.078, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.963, loss: 0.077, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.963, loss: 0.077, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.963, loss: 0.076, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.963, loss: 0.076, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.963, loss: 0.075, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.963, loss: 0.075, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.707, loss: 1.982, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.963, loss: 0.079, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.963, loss: 0.076, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.963, loss: 0.076, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.963, loss: 0.075, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.963, loss: 0.074, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.967, loss: 0.074, lr: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-7)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600972df",
   "metadata": {},
   "source": [
    "With such a large scale, and enough neurons, the model can easily memorize any data; hence why we do not just throw bigger models at any given problem. Since a model can **overfit** and give us these very high accuracy percents, we can't trust them right away and must do further testing with unseen data, ie **testing**, ie **out-of-sample data**.\n",
    "\n",
    "The training and the testing data must differ enough, and mind biases that can leak through such as sensor data taken every second, so certain individual data points being just a second apart and being very similar. The model giving very different accuracies for training and testing data is a good indicator of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "607f2d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.840, loss: 0.809\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X_test, y_test = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# calculating accuracy of output of activation 2 and targets\n",
    "# calculate values along first axis because of batches\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b5cfb4",
   "metadata": {},
   "source": [
    "Here the model gives an accuracy of 84%, and anything more than a 10% difference is very bad. Try to minimize this divergence.\n",
    "\n",
    "Overfitting can be caused by multiple factors such as:\n",
    "* Learning rate is too high\n",
    "* Too many training epochs\n",
    "* The model is too large\n",
    "\n",
    "A model that's not learning is too small. A model that's learning but overfitting is too large.\n",
    "**The fewer neurons you have, the smaller the chance the model can memorize the data**\n",
    "\n",
    "One general rule to follow when selecting initial model hyperparameters is to find the smallest model possible that still learns. Trying different model settings is called **hyperparameter searching**.\n",
    "\n",
    "You can let the model run for a short duration (perhaps a few minutes) to see if it's learning and if it is, let it run for longer, and compare it to a list of other hypermeters. \n",
    "\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "If there is too little data to create a separate validation set you can either:\n",
    "* Split the training data into training and validation sets, and pick the hyperparameters based on how it performs on the smaller validation set, and use the best hyperparameters to train the entire set\n",
    "* **Cross-Validation**: Given a dataset too small for a validation set, divide the dataset into 5 or so pieces and train the data on everything but one, and validate using the last one, then go through all of them like this in a prcoess called **k-fold cross validation**\n",
    "\n",
    "### Training Data\n",
    "Neural networks perform best when the training data is normalized between 0 and 1 or -1 and 1. The popular activation functions output values within this range:\n",
    "* Softmax: 0 and 1\n",
    "* Sigmoid: 0 and 1\n",
    "* tanh   :-1 and 1\n",
    "\n",
    "Also using numbers bigger than 1 can result in the weights being multiplied by them to caused overflows along the way. **It's easier to control the training process with smaller numbers**\n",
    "\n",
    "The data is made to fit this through **preprocessing**. All the datasets, the training, the validation, and the testing need to go through the same preprocessing steps.\n",
    "\n",
    "In the case where there is limited training data, **data augmentation** can be used to alter the existing data in a manner you can expect to see irl, such as rotating an images in a model to differentiate dogs and cats, but be careful in situations where rotating the image can change its meaning. \n",
    "\n",
    "### Regularization\n",
    "Regularization methods decrease generalization errors. One such method is **L1 and L2 Regularization**, which calculates a **penalty** to be the added to the loss for having weights or biases too big, which is an indicator of the model memorizing the data, and it's generally better when there are lots of neurons affecting the output, rather than a select few with big weights. \n",
    "\n",
    "* L1 penalty is the sum of all absolute values of the weights and biases: punishes smaller weights and biases more and causes the model to only variant to really big inputs\n",
    "* L2 penalty is the sum of all the squared weights and biases: punishes larger weights and biases a lot more\n",
    "\n",
    "L1 is rarely used alone, if at all. Moreover a **lambda** value is multiplied to the penalty to dictate how much the penalty will affect the model. Overall loss is now:\n",
    "\n",
    "    Loss = DataLoss + L_1w + L_1b + L_2w + L_2b\n",
    "\n",
    "\n",
    "#### Forward Pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cec0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1 = 0, weight_regularizer_l2 = 0, \n",
    "                 bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        #the lambda values of the regularization methods\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):        \n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # gradient on values to pass further back\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3197e684",
   "metadata": {},
   "source": [
    "The lambda is now added as a hyperparameter. Next, the loss function needs to be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb211403",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        \n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # calculate only when the factor is greater than 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        if layer.weights_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 regularization - weights\n",
    "        if layer.weights_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * \\\n",
    "                                           layer.weights)\n",
    "        \n",
    "        # L1 regularization - bias\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.biases))\n",
    "        \n",
    "        # L1 regularization - bias\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * \\\n",
    "                                           layer.weights)\n",
    "        \n",
    "        return regularization_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76253225",
   "metadata": {},
   "source": [
    "#### Backward Pass\n",
    "Since L2 means summing up all the squares then multiplying them by the lambda, in code, it means we multiply all by 2\\*lambda for the derivative\n",
    "\n",
    "Since L1 means summing up all the absolutes then multiplying them by the lambda, in code, it means we multiply the lambda by 1 if the weight is positive, and -1 if the weight is negative for every weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe82be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    \n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1 = 0, weight_regularizer_l2 = 0, \n",
    "                 bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        #the lambda values of the regularization methods\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):        \n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on the weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += self.weights * 2 \\\n",
    "                            * self.weight_regularizer_l2\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on the weights\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += self.biases * 2 \\\n",
    "                            * self.bias_regularizer_l2\n",
    "        \n",
    "        # gradient on values to pass further back\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe40c0",
   "metadata": {},
   "source": [
    "### Full Code Up Until This Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac94d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    \n",
    "    # layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1 = 0, weight_regularizer_l2 = 0, \n",
    "                 bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        #the lambda values of the regularization methods\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "        \n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "        \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):        \n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        \n",
    "        # gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on the weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += self.weights * 2 \\\n",
    "                            * self.weight_regularizer_l2\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on the weights\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += self.biases * 2 \\\n",
    "                            * self.bias_regularizer_l2\n",
    "        \n",
    "        # gradient on values to pass further back\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "# Rectified Linear Unit Activation Function\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        \n",
    "        # zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues):\n",
    "        \n",
    "        #create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # calculate jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            \n",
    "            # calculate sample wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "            \n",
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        \n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # calculate only when the factor is greater than 0\n",
    "        \n",
    "        # L1 regularization - weights\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * \\\n",
    "                                           layer.weights)\n",
    "        \n",
    "        # L1 regularization - bias\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                    np.sum(np.abs(layer.biases))\n",
    "        \n",
    "        # L1 regularization - bias\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                    np.sum(layer.weights * \\\n",
    "                                           layer.weights)\n",
    "        \n",
    "        return regularization_loss\n",
    "    \n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    #inheriting from the base Loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred will come from the neural network\n",
    "        # y_true will come from the training set\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # means scalar class values have been passed\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # one hot encoded values have been passed\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "    # Backwards pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        # number of labels in every sample\n",
    "        # we'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # if labels are sparse, turn them into one-hot vector\n",
    "        # in case the shape is as [3, 0, 2] etc etc per sample\n",
    "        # we turn them into one-hot vectors\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        # with a larger number of batches, it's all summed together\n",
    "        # in the dot product, and some will be given more importance\n",
    "        # than others when we don't normaize\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    # creates activation and loss function objects inside\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # set the output\n",
    "        self.output = self.activation.output\n",
    "        # calculate and return the loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "    \n",
    "    # backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #if labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "            \n",
    "        # copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1.0, decay = 0., momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.momentum:\n",
    "            \n",
    "            #if layer does not contain momentum arrays\n",
    "            # create them and fill with zeroes\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                \n",
    "                # if there is no momentum array for weights\n",
    "                # the array doesn't exist for the biases yet either\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "                \n",
    "            # build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "        # otherwise vanillae SGD\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "        \n",
    "        # update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adagrad:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 1., decay = 0., epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights ** 2\n",
    "        layer.bias_cache += layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_RMSprop:\n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "                    (1 - self.rho) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "                    (1 - self.rho) * layer.dbiases ** 2\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            layer.dweights / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            layer.dbiases / \\\n",
    "                            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_Adam:\n",
    "    \n",
    "    # initialize optimizer - set settings,\n",
    "    # learning rate of 1. is the default for this optimizer\n",
    "    # default decay rate of 0 (no decay)\n",
    "    def __init__(self, learning_rate = 0.001, decay = 0., epsilon = 1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        \n",
    "    # call once before any parameter update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / ( 1. + self.decay * self.iterations ))\n",
    "        \n",
    "    # update parameters\n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        # if layer does not contain cache array,\n",
    "        # create them and fill with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # update momentum with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                layer.weight_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                                layer.bias_momentums + \\\n",
    "                                (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start wit h1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        \n",
    "        # update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights ** 2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases ** 2\n",
    "        \n",
    "        # get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        \n",
    "        # vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                            weight_momentums_corrected / \\\n",
    "                            (np.sqrt(weight_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                            bias_momentums_corrected / \\\n",
    "                            (np.sqrt(bias_cache_corrected) + \\\n",
    "                                self.epsilon)\n",
    "    \n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2aa3875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, data_loss: 1.099, reg_loss: 0.000, lr: 0.02\n",
      "epoch: 100, acc: 0.690, loss: 0.872, data_loss: 0.807, reg_loss: 0.065, lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.773, loss: 0.743, data_loss: 0.614, reg_loss: 0.128, lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.817, loss: 0.681, data_loss: 0.526, reg_loss: 0.155, lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.823, loss: 0.636, data_loss: 0.469, reg_loss: 0.167, lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.830, loss: 0.606, data_loss: 0.432, reg_loss: 0.174, lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.843, loss: 0.576, data_loss: 0.399, reg_loss: 0.177, lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.857, loss: 0.554, data_loss: 0.376, reg_loss: 0.178, lr: 0.01999301244215147\n",
      "epoch: 800, acc: 0.857, loss: 0.535, data_loss: 0.357, reg_loss: 0.178, lr: 0.0199920131907303\n",
      "epoch: 900, acc: 0.873, loss: 0.521, data_loss: 0.344, reg_loss: 0.176, lr: 0.019991014039189386\n",
      "epoch: 1000, acc: 0.870, loss: 0.506, data_loss: 0.331, reg_loss: 0.174, lr: 0.019990014987513734\n",
      "epoch: 1100, acc: 0.880, loss: 0.494, data_loss: 0.321, reg_loss: 0.172, lr: 0.01998901603568839\n",
      "epoch: 1200, acc: 0.883, loss: 0.483, data_loss: 0.313, reg_loss: 0.170, lr: 0.019988017183698373\n",
      "epoch: 1300, acc: 0.887, loss: 0.473, data_loss: 0.305, reg_loss: 0.168, lr: 0.01998701843152872\n",
      "epoch: 1400, acc: 0.890, loss: 0.465, data_loss: 0.299, reg_loss: 0.166, lr: 0.019986019779164473\n",
      "epoch: 1500, acc: 0.893, loss: 0.457, data_loss: 0.293, reg_loss: 0.164, lr: 0.019985021226590672\n",
      "epoch: 1600, acc: 0.900, loss: 0.449, data_loss: 0.286, reg_loss: 0.162, lr: 0.01998402277379235\n",
      "epoch: 1700, acc: 0.897, loss: 0.443, data_loss: 0.283, reg_loss: 0.160, lr: 0.01998302442075457\n",
      "epoch: 1800, acc: 0.900, loss: 0.435, data_loss: 0.277, reg_loss: 0.158, lr: 0.019982026167462367\n",
      "epoch: 1900, acc: 0.900, loss: 0.429, data_loss: 0.273, reg_loss: 0.156, lr: 0.019981028013900805\n",
      "epoch: 2000, acc: 0.903, loss: 0.424, data_loss: 0.270, reg_loss: 0.154, lr: 0.019980029960054924\n",
      "epoch: 2100, acc: 0.907, loss: 0.419, data_loss: 0.267, reg_loss: 0.152, lr: 0.019979032005909798\n",
      "epoch: 2200, acc: 0.900, loss: 0.415, data_loss: 0.265, reg_loss: 0.150, lr: 0.01997803415145048\n",
      "epoch: 2300, acc: 0.903, loss: 0.410, data_loss: 0.262, reg_loss: 0.148, lr: 0.019977036396662037\n",
      "epoch: 2400, acc: 0.903, loss: 0.404, data_loss: 0.258, reg_loss: 0.146, lr: 0.019976038741529537\n",
      "epoch: 2500, acc: 0.920, loss: 0.398, data_loss: 0.253, reg_loss: 0.144, lr: 0.01997504118603805\n",
      "epoch: 2600, acc: 0.893, loss: 0.402, data_loss: 0.259, reg_loss: 0.143, lr: 0.01997404373017264\n",
      "epoch: 2700, acc: 0.900, loss: 0.391, data_loss: 0.250, reg_loss: 0.141, lr: 0.0199730463739184\n",
      "epoch: 2800, acc: 0.920, loss: 0.385, data_loss: 0.245, reg_loss: 0.140, lr: 0.019972049117260395\n",
      "epoch: 2900, acc: 0.907, loss: 0.381, data_loss: 0.243, reg_loss: 0.139, lr: 0.019971051960183714\n",
      "epoch: 3000, acc: 0.907, loss: 0.378, data_loss: 0.241, reg_loss: 0.137, lr: 0.019970054902673444\n",
      "epoch: 3100, acc: 0.907, loss: 0.375, data_loss: 0.239, reg_loss: 0.136, lr: 0.019969057944714663\n",
      "epoch: 3200, acc: 0.910, loss: 0.370, data_loss: 0.235, reg_loss: 0.134, lr: 0.019968061086292475\n",
      "epoch: 3300, acc: 0.907, loss: 0.366, data_loss: 0.233, reg_loss: 0.133, lr: 0.019967064327391967\n",
      "epoch: 3400, acc: 0.917, loss: 0.365, data_loss: 0.233, reg_loss: 0.132, lr: 0.019966067667998237\n",
      "epoch: 3500, acc: 0.917, loss: 0.360, data_loss: 0.229, reg_loss: 0.131, lr: 0.019965071108096383\n",
      "epoch: 3600, acc: 0.923, loss: 0.357, data_loss: 0.227, reg_loss: 0.130, lr: 0.01996407464767152\n",
      "epoch: 3700, acc: 0.920, loss: 0.354, data_loss: 0.225, reg_loss: 0.129, lr: 0.019963078286708732\n",
      "epoch: 3800, acc: 0.920, loss: 0.350, data_loss: 0.222, reg_loss: 0.128, lr: 0.019962082025193145\n",
      "epoch: 3900, acc: 0.907, loss: 0.349, data_loss: 0.222, reg_loss: 0.127, lr: 0.019961085863109868\n",
      "epoch: 4000, acc: 0.913, loss: 0.344, data_loss: 0.219, reg_loss: 0.126, lr: 0.019960089800444013\n",
      "epoch: 4100, acc: 0.913, loss: 0.343, data_loss: 0.218, reg_loss: 0.125, lr: 0.019959093837180697\n",
      "epoch: 4200, acc: 0.917, loss: 0.341, data_loss: 0.217, reg_loss: 0.124, lr: 0.01995809797330505\n",
      "epoch: 4300, acc: 0.930, loss: 0.345, data_loss: 0.222, reg_loss: 0.123, lr: 0.01995710220880218\n",
      "epoch: 4400, acc: 0.923, loss: 0.335, data_loss: 0.213, reg_loss: 0.122, lr: 0.019956106543657228\n",
      "epoch: 4500, acc: 0.910, loss: 0.341, data_loss: 0.220, reg_loss: 0.121, lr: 0.019955110977855316\n",
      "epoch: 4600, acc: 0.927, loss: 0.330, data_loss: 0.210, reg_loss: 0.120, lr: 0.01995411551138158\n",
      "epoch: 4700, acc: 0.853, loss: 0.555, data_loss: 0.429, reg_loss: 0.126, lr: 0.019953120144221154\n",
      "epoch: 4800, acc: 0.927, loss: 0.334, data_loss: 0.209, reg_loss: 0.125, lr: 0.019952124876359174\n",
      "epoch: 4900, acc: 0.930, loss: 0.331, data_loss: 0.207, reg_loss: 0.124, lr: 0.01995112970778079\n",
      "epoch: 5000, acc: 0.930, loss: 0.330, data_loss: 0.206, reg_loss: 0.123, lr: 0.019950134638471142\n",
      "epoch: 5100, acc: 0.930, loss: 0.328, data_loss: 0.205, reg_loss: 0.123, lr: 0.019949139668415376\n",
      "epoch: 5200, acc: 0.930, loss: 0.327, data_loss: 0.204, reg_loss: 0.122, lr: 0.01994814479759864\n",
      "epoch: 5300, acc: 0.930, loss: 0.325, data_loss: 0.204, reg_loss: 0.122, lr: 0.019947150026006097\n",
      "epoch: 5400, acc: 0.930, loss: 0.324, data_loss: 0.203, reg_loss: 0.121, lr: 0.019946155353622895\n",
      "epoch: 5500, acc: 0.930, loss: 0.322, data_loss: 0.201, reg_loss: 0.121, lr: 0.019945160780434196\n",
      "epoch: 5600, acc: 0.930, loss: 0.320, data_loss: 0.199, reg_loss: 0.121, lr: 0.019944166306425162\n",
      "epoch: 5700, acc: 0.930, loss: 0.319, data_loss: 0.198, reg_loss: 0.120, lr: 0.01994317193158096\n",
      "epoch: 5800, acc: 0.930, loss: 0.317, data_loss: 0.197, reg_loss: 0.120, lr: 0.019942177655886757\n",
      "epoch: 5900, acc: 0.930, loss: 0.316, data_loss: 0.196, reg_loss: 0.120, lr: 0.019941183479327725\n",
      "epoch: 6000, acc: 0.930, loss: 0.315, data_loss: 0.195, reg_loss: 0.119, lr: 0.019940189401889033\n",
      "epoch: 6100, acc: 0.930, loss: 0.313, data_loss: 0.194, reg_loss: 0.119, lr: 0.01993919542355587\n",
      "epoch: 6200, acc: 0.930, loss: 0.312, data_loss: 0.193, reg_loss: 0.118, lr: 0.019938201544313403\n",
      "epoch: 6300, acc: 0.930, loss: 0.310, data_loss: 0.192, reg_loss: 0.118, lr: 0.01993720776414682\n",
      "epoch: 6400, acc: 0.927, loss: 0.309, data_loss: 0.191, reg_loss: 0.118, lr: 0.019936214083041307\n",
      "epoch: 6500, acc: 0.930, loss: 0.307, data_loss: 0.190, reg_loss: 0.117, lr: 0.01993522050098206\n",
      "epoch: 6600, acc: 0.930, loss: 0.306, data_loss: 0.189, reg_loss: 0.117, lr: 0.019934227017954262\n",
      "epoch: 6700, acc: 0.930, loss: 0.305, data_loss: 0.188, reg_loss: 0.116, lr: 0.01993323363394311\n",
      "epoch: 6800, acc: 0.933, loss: 0.304, data_loss: 0.188, reg_loss: 0.116, lr: 0.0199322403489338\n",
      "epoch: 6900, acc: 0.940, loss: 0.302, data_loss: 0.187, reg_loss: 0.115, lr: 0.019931247162911534\n",
      "epoch: 7000, acc: 0.930, loss: 0.306, data_loss: 0.192, reg_loss: 0.115, lr: 0.019930254075861523\n",
      "epoch: 7100, acc: 0.937, loss: 0.299, data_loss: 0.184, reg_loss: 0.114, lr: 0.019929261087768962\n",
      "epoch: 7200, acc: 0.937, loss: 0.297, data_loss: 0.183, reg_loss: 0.114, lr: 0.01992826819861907\n",
      "epoch: 7300, acc: 0.930, loss: 0.297, data_loss: 0.183, reg_loss: 0.114, lr: 0.019927275408397054\n",
      "epoch: 7400, acc: 0.933, loss: 0.295, data_loss: 0.182, reg_loss: 0.113, lr: 0.019926282717088132\n",
      "epoch: 7500, acc: 0.937, loss: 0.293, data_loss: 0.180, reg_loss: 0.113, lr: 0.01992529012467752\n",
      "epoch: 7600, acc: 0.943, loss: 0.293, data_loss: 0.180, reg_loss: 0.112, lr: 0.019924297631150445\n",
      "epoch: 7700, acc: 0.937, loss: 0.292, data_loss: 0.179, reg_loss: 0.112, lr: 0.019923305236492123\n",
      "epoch: 7800, acc: 0.947, loss: 0.290, data_loss: 0.178, reg_loss: 0.112, lr: 0.01992231294068779\n",
      "epoch: 7900, acc: 0.943, loss: 0.289, data_loss: 0.178, reg_loss: 0.111, lr: 0.019921320743722666\n",
      "epoch: 8000, acc: 0.940, loss: 0.286, data_loss: 0.176, reg_loss: 0.111, lr: 0.019920328645582\n",
      "epoch: 8100, acc: 0.947, loss: 0.288, data_loss: 0.178, reg_loss: 0.110, lr: 0.019919336646251007\n",
      "epoch: 8200, acc: 0.940, loss: 0.285, data_loss: 0.175, reg_loss: 0.110, lr: 0.019918344745714942\n",
      "epoch: 8300, acc: 0.937, loss: 0.284, data_loss: 0.173, reg_loss: 0.110, lr: 0.019917352943959042\n",
      "epoch: 8400, acc: 0.947, loss: 0.281, data_loss: 0.171, reg_loss: 0.109, lr: 0.019916361240968555\n",
      "epoch: 8500, acc: 0.943, loss: 0.279, data_loss: 0.171, reg_loss: 0.109, lr: 0.01991536963672872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8600, acc: 0.947, loss: 0.278, data_loss: 0.170, reg_loss: 0.108, lr: 0.019914378131224802\n",
      "epoch: 8700, acc: 0.950, loss: 0.279, data_loss: 0.171, reg_loss: 0.108, lr: 0.01991338672444204\n",
      "epoch: 8800, acc: 0.943, loss: 0.279, data_loss: 0.171, reg_loss: 0.107, lr: 0.0199123954163657\n",
      "epoch: 8900, acc: 0.953, loss: 0.291, data_loss: 0.176, reg_loss: 0.115, lr: 0.019911404206981037\n",
      "epoch: 9000, acc: 0.950, loss: 0.280, data_loss: 0.166, reg_loss: 0.114, lr: 0.019910413096273318\n",
      "epoch: 9100, acc: 0.950, loss: 0.279, data_loss: 0.166, reg_loss: 0.113, lr: 0.019909422084227805\n",
      "epoch: 9200, acc: 0.950, loss: 0.278, data_loss: 0.166, reg_loss: 0.112, lr: 0.019908431170829768\n",
      "epoch: 9300, acc: 0.950, loss: 0.277, data_loss: 0.166, reg_loss: 0.111, lr: 0.01990744035606448\n",
      "epoch: 9400, acc: 0.950, loss: 0.276, data_loss: 0.166, reg_loss: 0.110, lr: 0.01990644963991721\n",
      "epoch: 9500, acc: 0.950, loss: 0.275, data_loss: 0.166, reg_loss: 0.109, lr: 0.01990545902237324\n",
      "epoch: 9600, acc: 0.950, loss: 0.274, data_loss: 0.165, reg_loss: 0.108, lr: 0.019904468503417844\n",
      "epoch: 9700, acc: 0.950, loss: 0.273, data_loss: 0.165, reg_loss: 0.108, lr: 0.019903478083036316\n",
      "epoch: 9800, acc: 0.950, loss: 0.272, data_loss: 0.165, reg_loss: 0.107, lr: 0.019902487761213932\n",
      "epoch: 9900, acc: 0.950, loss: 0.271, data_loss: 0.165, reg_loss: 0.107, lr: 0.019901497537935988\n",
      "epoch: 10000, acc: 0.950, loss: 0.271, data_loss: 0.165, reg_loss: 0.106, lr: 0.019900507413187767\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # regularizatin penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "    \n",
    "    # overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e3fbfb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.863, loss: 0.388\n"
     ]
    }
   ],
   "source": [
    "# validate the model\n",
    "\n",
    "# creating dataset\n",
    "X_test, y_test = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# calculating accuracy of output of activation 2 and targets\n",
    "# calculate values along first axis because of batches\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a3478",
   "metadata": {},
   "source": [
    "With the regularization added, the model is fiting to the validation set lot better both in terms of accuracy and loss.\n",
    "\n",
    "The following is an example of how larger training data can help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37387d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.348, loss: 1.099, data_loss: 1.099, reg_loss: 0.000, lr: 0.02\n",
      "epoch: 100, acc: 0.584, loss: 0.923, data_loss: 0.872, reg_loss: 0.052, lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.750, loss: 0.790, data_loss: 0.671, reg_loss: 0.119, lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.789, loss: 0.728, data_loss: 0.581, reg_loss: 0.147, lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.810, loss: 0.689, data_loss: 0.530, reg_loss: 0.160, lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.823, loss: 0.658, data_loss: 0.491, reg_loss: 0.167, lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.833, loss: 0.636, data_loss: 0.468, reg_loss: 0.167, lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.840, loss: 0.616, data_loss: 0.451, reg_loss: 0.165, lr: 0.01999301244215147\n",
      "epoch: 800, acc: 0.845, loss: 0.600, data_loss: 0.438, reg_loss: 0.162, lr: 0.0199920131907303\n",
      "epoch: 900, acc: 0.847, loss: 0.586, data_loss: 0.427, reg_loss: 0.159, lr: 0.019991014039189386\n",
      "epoch: 1000, acc: 0.855, loss: 0.575, data_loss: 0.419, reg_loss: 0.156, lr: 0.019990014987513734\n",
      "epoch: 1100, acc: 0.859, loss: 0.564, data_loss: 0.411, reg_loss: 0.153, lr: 0.01998901603568839\n",
      "epoch: 1200, acc: 0.861, loss: 0.553, data_loss: 0.402, reg_loss: 0.151, lr: 0.019988017183698373\n",
      "epoch: 1300, acc: 0.858, loss: 0.544, data_loss: 0.395, reg_loss: 0.149, lr: 0.01998701843152872\n",
      "epoch: 1400, acc: 0.863, loss: 0.536, data_loss: 0.389, reg_loss: 0.147, lr: 0.019986019779164473\n",
      "epoch: 1500, acc: 0.864, loss: 0.529, data_loss: 0.384, reg_loss: 0.145, lr: 0.019985021226590672\n",
      "epoch: 1600, acc: 0.854, loss: 0.527, data_loss: 0.384, reg_loss: 0.143, lr: 0.01998402277379235\n",
      "epoch: 1700, acc: 0.868, loss: 0.517, data_loss: 0.376, reg_loss: 0.142, lr: 0.01998302442075457\n",
      "epoch: 1800, acc: 0.860, loss: 0.511, data_loss: 0.371, reg_loss: 0.140, lr: 0.019982026167462367\n",
      "epoch: 1900, acc: 0.862, loss: 0.506, data_loss: 0.368, reg_loss: 0.139, lr: 0.019981028013900805\n",
      "epoch: 2000, acc: 0.870, loss: 0.502, data_loss: 0.365, reg_loss: 0.137, lr: 0.019980029960054924\n",
      "epoch: 2100, acc: 0.870, loss: 0.498, data_loss: 0.362, reg_loss: 0.136, lr: 0.019979032005909798\n",
      "epoch: 2200, acc: 0.870, loss: 0.493, data_loss: 0.359, reg_loss: 0.134, lr: 0.01997803415145048\n",
      "epoch: 2300, acc: 0.873, loss: 0.487, data_loss: 0.354, reg_loss: 0.133, lr: 0.019977036396662037\n",
      "epoch: 2400, acc: 0.865, loss: 0.484, data_loss: 0.352, reg_loss: 0.132, lr: 0.019976038741529537\n",
      "epoch: 2500, acc: 0.870, loss: 0.479, data_loss: 0.348, reg_loss: 0.131, lr: 0.01997504118603805\n",
      "epoch: 2600, acc: 0.874, loss: 0.478, data_loss: 0.349, reg_loss: 0.129, lr: 0.01997404373017264\n",
      "epoch: 2700, acc: 0.871, loss: 0.472, data_loss: 0.344, reg_loss: 0.128, lr: 0.0199730463739184\n",
      "epoch: 2800, acc: 0.870, loss: 0.471, data_loss: 0.344, reg_loss: 0.127, lr: 0.019972049117260395\n",
      "epoch: 2900, acc: 0.871, loss: 0.470, data_loss: 0.344, reg_loss: 0.126, lr: 0.019971051960183714\n",
      "epoch: 3000, acc: 0.875, loss: 0.466, data_loss: 0.341, reg_loss: 0.125, lr: 0.019970054902673444\n",
      "epoch: 3100, acc: 0.876, loss: 0.461, data_loss: 0.337, reg_loss: 0.124, lr: 0.019969057944714663\n",
      "epoch: 3200, acc: 0.871, loss: 0.458, data_loss: 0.336, reg_loss: 0.122, lr: 0.019968061086292475\n",
      "epoch: 3300, acc: 0.873, loss: 0.455, data_loss: 0.334, reg_loss: 0.121, lr: 0.019967064327391967\n",
      "epoch: 3400, acc: 0.871, loss: 0.452, data_loss: 0.332, reg_loss: 0.120, lr: 0.019966067667998237\n",
      "epoch: 3500, acc: 0.871, loss: 0.450, data_loss: 0.330, reg_loss: 0.120, lr: 0.019965071108096383\n",
      "epoch: 3600, acc: 0.876, loss: 0.448, data_loss: 0.329, reg_loss: 0.119, lr: 0.01996407464767152\n",
      "epoch: 3700, acc: 0.873, loss: 0.447, data_loss: 0.329, reg_loss: 0.118, lr: 0.019963078286708732\n",
      "epoch: 3800, acc: 0.878, loss: 0.445, data_loss: 0.328, reg_loss: 0.117, lr: 0.019962082025193145\n",
      "epoch: 3900, acc: 0.877, loss: 0.443, data_loss: 0.327, reg_loss: 0.116, lr: 0.019961085863109868\n",
      "epoch: 4000, acc: 0.873, loss: 0.439, data_loss: 0.324, reg_loss: 0.116, lr: 0.019960089800444013\n",
      "epoch: 4100, acc: 0.876, loss: 0.437, data_loss: 0.322, reg_loss: 0.115, lr: 0.019959093837180697\n",
      "epoch: 4200, acc: 0.875, loss: 0.436, data_loss: 0.322, reg_loss: 0.114, lr: 0.01995809797330505\n",
      "epoch: 4300, acc: 0.871, loss: 0.436, data_loss: 0.323, reg_loss: 0.113, lr: 0.01995710220880218\n",
      "epoch: 4400, acc: 0.874, loss: 0.433, data_loss: 0.320, reg_loss: 0.112, lr: 0.019956106543657228\n",
      "epoch: 4500, acc: 0.874, loss: 0.431, data_loss: 0.319, reg_loss: 0.112, lr: 0.019955110977855316\n",
      "epoch: 4600, acc: 0.872, loss: 0.433, data_loss: 0.322, reg_loss: 0.111, lr: 0.01995411551138158\n",
      "epoch: 4700, acc: 0.879, loss: 0.427, data_loss: 0.317, reg_loss: 0.110, lr: 0.019953120144221154\n",
      "epoch: 4800, acc: 0.878, loss: 0.425, data_loss: 0.316, reg_loss: 0.109, lr: 0.019952124876359174\n",
      "epoch: 4900, acc: 0.879, loss: 0.425, data_loss: 0.316, reg_loss: 0.109, lr: 0.01995112970778079\n",
      "epoch: 5000, acc: 0.870, loss: 0.430, data_loss: 0.322, reg_loss: 0.108, lr: 0.019950134638471142\n",
      "epoch: 5100, acc: 0.879, loss: 0.421, data_loss: 0.313, reg_loss: 0.107, lr: 0.019949139668415376\n",
      "epoch: 5200, acc: 0.882, loss: 0.419, data_loss: 0.313, reg_loss: 0.106, lr: 0.01994814479759864\n",
      "epoch: 5300, acc: 0.875, loss: 0.420, data_loss: 0.315, reg_loss: 0.106, lr: 0.019947150026006097\n",
      "epoch: 5400, acc: 0.873, loss: 0.421, data_loss: 0.316, reg_loss: 0.105, lr: 0.019946155353622895\n",
      "epoch: 5500, acc: 0.877, loss: 0.415, data_loss: 0.310, reg_loss: 0.105, lr: 0.019945160780434196\n",
      "epoch: 5600, acc: 0.882, loss: 0.414, data_loss: 0.310, reg_loss: 0.104, lr: 0.019944166306425162\n",
      "epoch: 5700, acc: 0.876, loss: 0.413, data_loss: 0.309, reg_loss: 0.104, lr: 0.01994317193158096\n",
      "epoch: 5800, acc: 0.878, loss: 0.411, data_loss: 0.307, reg_loss: 0.103, lr: 0.019942177655886757\n",
      "epoch: 5900, acc: 0.883, loss: 0.409, data_loss: 0.307, reg_loss: 0.103, lr: 0.019941183479327725\n",
      "epoch: 6000, acc: 0.878, loss: 0.409, data_loss: 0.307, reg_loss: 0.102, lr: 0.019940189401889033\n",
      "epoch: 6100, acc: 0.883, loss: 0.410, data_loss: 0.308, reg_loss: 0.102, lr: 0.01993919542355587\n",
      "epoch: 6200, acc: 0.878, loss: 0.409, data_loss: 0.308, reg_loss: 0.101, lr: 0.019938201544313403\n",
      "epoch: 6300, acc: 0.878, loss: 0.406, data_loss: 0.306, reg_loss: 0.101, lr: 0.01993720776414682\n",
      "epoch: 6400, acc: 0.885, loss: 0.406, data_loss: 0.306, reg_loss: 0.100, lr: 0.019936214083041307\n",
      "epoch: 6500, acc: 0.882, loss: 0.407, data_loss: 0.307, reg_loss: 0.100, lr: 0.01993522050098206\n",
      "epoch: 6600, acc: 0.879, loss: 0.403, data_loss: 0.304, reg_loss: 0.099, lr: 0.019934227017954262\n",
      "epoch: 6700, acc: 0.885, loss: 0.405, data_loss: 0.307, reg_loss: 0.099, lr: 0.01993323363394311\n",
      "epoch: 6800, acc: 0.884, loss: 0.403, data_loss: 0.305, reg_loss: 0.098, lr: 0.0199322403489338\n",
      "epoch: 6900, acc: 0.884, loss: 0.398, data_loss: 0.300, reg_loss: 0.098, lr: 0.019931247162911534\n",
      "epoch: 7000, acc: 0.882, loss: 0.397, data_loss: 0.300, reg_loss: 0.097, lr: 0.019930254075861523\n",
      "epoch: 7100, acc: 0.881, loss: 0.397, data_loss: 0.300, reg_loss: 0.097, lr: 0.019929261087768962\n",
      "epoch: 7200, acc: 0.887, loss: 0.396, data_loss: 0.300, reg_loss: 0.096, lr: 0.01992826819861907\n",
      "epoch: 7300, acc: 0.882, loss: 0.395, data_loss: 0.299, reg_loss: 0.096, lr: 0.019927275408397054\n",
      "epoch: 7400, acc: 0.880, loss: 0.397, data_loss: 0.301, reg_loss: 0.095, lr: 0.019926282717088132\n",
      "epoch: 7500, acc: 0.877, loss: 0.396, data_loss: 0.301, reg_loss: 0.095, lr: 0.01992529012467752\n",
      "epoch: 7600, acc: 0.888, loss: 0.392, data_loss: 0.297, reg_loss: 0.095, lr: 0.019924297631150445\n",
      "epoch: 7700, acc: 0.880, loss: 0.392, data_loss: 0.298, reg_loss: 0.094, lr: 0.019923305236492123\n",
      "epoch: 7800, acc: 0.885, loss: 0.393, data_loss: 0.300, reg_loss: 0.094, lr: 0.01992231294068779\n",
      "epoch: 7900, acc: 0.886, loss: 0.388, data_loss: 0.295, reg_loss: 0.093, lr: 0.019921320743722666\n",
      "epoch: 8000, acc: 0.879, loss: 0.390, data_loss: 0.297, reg_loss: 0.093, lr: 0.019920328645582\n",
      "epoch: 8100, acc: 0.888, loss: 0.398, data_loss: 0.305, reg_loss: 0.092, lr: 0.019919336646251007\n",
      "epoch: 8200, acc: 0.872, loss: 0.402, data_loss: 0.310, reg_loss: 0.092, lr: 0.019918344745714942\n",
      "epoch: 8300, acc: 0.887, loss: 0.388, data_loss: 0.297, reg_loss: 0.092, lr: 0.019917352943959042\n",
      "epoch: 8400, acc: 0.877, loss: 0.389, data_loss: 0.298, reg_loss: 0.091, lr: 0.019916361240968555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8500, acc: 0.885, loss: 0.383, data_loss: 0.292, reg_loss: 0.091, lr: 0.01991536963672872\n",
      "epoch: 8600, acc: 0.884, loss: 0.384, data_loss: 0.293, reg_loss: 0.091, lr: 0.019914378131224802\n",
      "epoch: 8700, acc: 0.883, loss: 0.384, data_loss: 0.293, reg_loss: 0.091, lr: 0.01991338672444204\n",
      "epoch: 8800, acc: 0.883, loss: 0.383, data_loss: 0.293, reg_loss: 0.090, lr: 0.0199123954163657\n",
      "epoch: 8900, acc: 0.885, loss: 0.382, data_loss: 0.292, reg_loss: 0.090, lr: 0.019911404206981037\n",
      "epoch: 9000, acc: 0.889, loss: 0.384, data_loss: 0.294, reg_loss: 0.090, lr: 0.019910413096273318\n",
      "epoch: 9100, acc: 0.881, loss: 0.382, data_loss: 0.292, reg_loss: 0.089, lr: 0.019909422084227805\n",
      "epoch: 9200, acc: 0.889, loss: 0.381, data_loss: 0.292, reg_loss: 0.089, lr: 0.019908431170829768\n",
      "epoch: 9300, acc: 0.881, loss: 0.382, data_loss: 0.293, reg_loss: 0.089, lr: 0.01990744035606448\n",
      "epoch: 9400, acc: 0.887, loss: 0.378, data_loss: 0.289, reg_loss: 0.088, lr: 0.01990644963991721\n",
      "epoch: 9500, acc: 0.887, loss: 0.379, data_loss: 0.291, reg_loss: 0.088, lr: 0.01990545902237324\n",
      "epoch: 9600, acc: 0.888, loss: 0.377, data_loss: 0.290, reg_loss: 0.088, lr: 0.019904468503417844\n",
      "epoch: 9700, acc: 0.890, loss: 0.382, data_loss: 0.294, reg_loss: 0.087, lr: 0.019903478083036316\n",
      "epoch: 9800, acc: 0.885, loss: 0.378, data_loss: 0.290, reg_loss: 0.087, lr: 0.019902487761213932\n",
      "epoch: 9900, acc: 0.890, loss: 0.376, data_loss: 0.289, reg_loss: 0.087, lr: 0.019901497537935988\n",
      "epoch: 10000, acc: 0.889, loss: 0.373, data_loss: 0.286, reg_loss: 0.087, lr: 0.019900507413187767\n",
      "validation, acc: 0.883, loss: 0.325\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 1000, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # regularizatin penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "    \n",
    "    # overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "# validate the model\n",
    "\n",
    "# creating dataset\n",
    "X_test, y_test = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# calculating accuracy of output of activation 2 and targets\n",
    "# calculate values along first axis because of batches\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7849164",
   "metadata": {},
   "source": [
    "The delta between the training and validation is tiny compared to before the regulization. The regulization methods allow us to throw larger models into the problem without the fear of overfitting. Let's try throwing a larger model at the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2600c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.399, loss: 1.099, data_loss: 1.099, reg_loss: 0.000, lr: 0.02\n",
      "epoch: 100, acc: 0.761, loss: 0.767, data_loss: 0.665, reg_loss: 0.102, lr: 0.019999010049002574\n",
      "epoch: 200, acc: 0.844, loss: 0.622, data_loss: 0.467, reg_loss: 0.155, lr: 0.019998010197985302\n",
      "epoch: 300, acc: 0.868, loss: 0.554, data_loss: 0.381, reg_loss: 0.173, lr: 0.019997010446938183\n",
      "epoch: 400, acc: 0.881, loss: 0.514, data_loss: 0.341, reg_loss: 0.174, lr: 0.01999601079584623\n",
      "epoch: 500, acc: 0.889, loss: 0.486, data_loss: 0.318, reg_loss: 0.169, lr: 0.01999501124469445\n",
      "epoch: 600, acc: 0.891, loss: 0.464, data_loss: 0.301, reg_loss: 0.163, lr: 0.01999401179346786\n",
      "epoch: 700, acc: 0.897, loss: 0.447, data_loss: 0.291, reg_loss: 0.157, lr: 0.01999301244215147\n",
      "epoch: 800, acc: 0.740, loss: 0.793, data_loss: 0.642, reg_loss: 0.151, lr: 0.0199920131907303\n",
      "epoch: 900, acc: 0.901, loss: 0.436, data_loss: 0.275, reg_loss: 0.161, lr: 0.019991014039189386\n",
      "epoch: 1000, acc: 0.901, loss: 0.426, data_loss: 0.270, reg_loss: 0.156, lr: 0.019990014987513734\n",
      "epoch: 1100, acc: 0.901, loss: 0.418, data_loss: 0.266, reg_loss: 0.152, lr: 0.01998901603568839\n",
      "epoch: 1200, acc: 0.901, loss: 0.410, data_loss: 0.262, reg_loss: 0.148, lr: 0.019988017183698373\n",
      "epoch: 1300, acc: 0.903, loss: 0.405, data_loss: 0.261, reg_loss: 0.144, lr: 0.01998701843152872\n",
      "epoch: 1400, acc: 0.902, loss: 0.397, data_loss: 0.257, reg_loss: 0.141, lr: 0.019986019779164473\n",
      "epoch: 1500, acc: 0.908, loss: 0.388, data_loss: 0.251, reg_loss: 0.137, lr: 0.019985021226590672\n",
      "epoch: 1600, acc: 0.906, loss: 0.388, data_loss: 0.254, reg_loss: 0.134, lr: 0.01998402277379235\n",
      "epoch: 1700, acc: 0.908, loss: 0.376, data_loss: 0.245, reg_loss: 0.131, lr: 0.01998302442075457\n",
      "epoch: 1800, acc: 0.905, loss: 0.374, data_loss: 0.246, reg_loss: 0.128, lr: 0.019982026167462367\n",
      "epoch: 1900, acc: 0.904, loss: 0.376, data_loss: 0.250, reg_loss: 0.126, lr: 0.019981028013900805\n",
      "epoch: 2000, acc: 0.909, loss: 0.362, data_loss: 0.239, reg_loss: 0.123, lr: 0.019980029960054924\n",
      "epoch: 2100, acc: 0.910, loss: 0.359, data_loss: 0.238, reg_loss: 0.121, lr: 0.019979032005909798\n",
      "epoch: 2200, acc: 0.910, loss: 0.357, data_loss: 0.238, reg_loss: 0.118, lr: 0.01997803415145048\n",
      "epoch: 2300, acc: 0.908, loss: 0.358, data_loss: 0.242, reg_loss: 0.117, lr: 0.019977036396662037\n",
      "epoch: 2400, acc: 0.911, loss: 0.346, data_loss: 0.231, reg_loss: 0.115, lr: 0.019976038741529537\n",
      "epoch: 2500, acc: 0.909, loss: 0.347, data_loss: 0.234, reg_loss: 0.113, lr: 0.01997504118603805\n",
      "epoch: 2600, acc: 0.912, loss: 0.339, data_loss: 0.228, reg_loss: 0.111, lr: 0.01997404373017264\n",
      "epoch: 2700, acc: 0.911, loss: 0.340, data_loss: 0.231, reg_loss: 0.109, lr: 0.0199730463739184\n",
      "epoch: 2800, acc: 0.905, loss: 0.390, data_loss: 0.265, reg_loss: 0.125, lr: 0.019972049117260395\n",
      "epoch: 2900, acc: 0.910, loss: 0.367, data_loss: 0.245, reg_loss: 0.122, lr: 0.019971051960183714\n",
      "epoch: 3000, acc: 0.911, loss: 0.361, data_loss: 0.241, reg_loss: 0.120, lr: 0.019970054902673444\n",
      "epoch: 3100, acc: 0.911, loss: 0.357, data_loss: 0.239, reg_loss: 0.118, lr: 0.019969057944714663\n",
      "epoch: 3200, acc: 0.912, loss: 0.354, data_loss: 0.237, reg_loss: 0.117, lr: 0.019968061086292475\n",
      "epoch: 3300, acc: 0.912, loss: 0.351, data_loss: 0.235, reg_loss: 0.115, lr: 0.019967064327391967\n",
      "epoch: 3400, acc: 0.913, loss: 0.348, data_loss: 0.234, reg_loss: 0.114, lr: 0.019966067667998237\n",
      "epoch: 3500, acc: 0.912, loss: 0.345, data_loss: 0.233, reg_loss: 0.112, lr: 0.019965071108096383\n",
      "epoch: 3600, acc: 0.913, loss: 0.342, data_loss: 0.231, reg_loss: 0.111, lr: 0.01996407464767152\n",
      "epoch: 3700, acc: 0.914, loss: 0.340, data_loss: 0.230, reg_loss: 0.110, lr: 0.019963078286708732\n",
      "epoch: 3800, acc: 0.911, loss: 0.339, data_loss: 0.231, reg_loss: 0.108, lr: 0.019962082025193145\n",
      "epoch: 3900, acc: 0.911, loss: 0.336, data_loss: 0.229, reg_loss: 0.107, lr: 0.019961085863109868\n",
      "epoch: 4000, acc: 0.912, loss: 0.338, data_loss: 0.233, reg_loss: 0.105, lr: 0.019960089800444013\n",
      "epoch: 4100, acc: 0.913, loss: 0.332, data_loss: 0.227, reg_loss: 0.104, lr: 0.019959093837180697\n",
      "epoch: 4200, acc: 0.913, loss: 0.330, data_loss: 0.227, reg_loss: 0.103, lr: 0.01995809797330505\n",
      "epoch: 4300, acc: 0.911, loss: 0.332, data_loss: 0.230, reg_loss: 0.102, lr: 0.01995710220880218\n",
      "epoch: 4400, acc: 0.913, loss: 0.326, data_loss: 0.225, reg_loss: 0.101, lr: 0.019956106543657228\n",
      "epoch: 4500, acc: 0.914, loss: 0.322, data_loss: 0.222, reg_loss: 0.100, lr: 0.019955110977855316\n",
      "epoch: 4600, acc: 0.910, loss: 0.329, data_loss: 0.231, reg_loss: 0.099, lr: 0.01995411551138158\n",
      "epoch: 4700, acc: 0.907, loss: 0.330, data_loss: 0.232, reg_loss: 0.098, lr: 0.019953120144221154\n",
      "epoch: 4800, acc: 0.914, loss: 0.317, data_loss: 0.221, reg_loss: 0.097, lr: 0.019952124876359174\n",
      "epoch: 4900, acc: 0.907, loss: 0.323, data_loss: 0.227, reg_loss: 0.096, lr: 0.01995112970778079\n",
      "epoch: 5000, acc: 0.911, loss: 0.315, data_loss: 0.221, reg_loss: 0.095, lr: 0.019950134638471142\n",
      "epoch: 5100, acc: 0.913, loss: 0.315, data_loss: 0.221, reg_loss: 0.094, lr: 0.019949139668415376\n",
      "epoch: 5200, acc: 0.914, loss: 0.311, data_loss: 0.218, reg_loss: 0.093, lr: 0.01994814479759864\n",
      "epoch: 5300, acc: 0.915, loss: 0.309, data_loss: 0.217, reg_loss: 0.092, lr: 0.019947150026006097\n",
      "epoch: 5400, acc: 0.913, loss: 0.314, data_loss: 0.222, reg_loss: 0.091, lr: 0.019946155353622895\n",
      "epoch: 5500, acc: 0.909, loss: 0.313, data_loss: 0.222, reg_loss: 0.090, lr: 0.019945160780434196\n",
      "epoch: 5600, acc: 0.914, loss: 0.304, data_loss: 0.215, reg_loss: 0.089, lr: 0.019944166306425162\n",
      "epoch: 5700, acc: 0.909, loss: 0.308, data_loss: 0.219, reg_loss: 0.089, lr: 0.01994317193158096\n",
      "epoch: 5800, acc: 0.912, loss: 0.307, data_loss: 0.219, reg_loss: 0.088, lr: 0.019942177655886757\n",
      "epoch: 5900, acc: 0.911, loss: 0.309, data_loss: 0.222, reg_loss: 0.087, lr: 0.019941183479327725\n",
      "epoch: 6000, acc: 0.915, loss: 0.299, data_loss: 0.213, reg_loss: 0.086, lr: 0.019940189401889033\n",
      "epoch: 6100, acc: 0.913, loss: 0.299, data_loss: 0.214, reg_loss: 0.085, lr: 0.01993919542355587\n",
      "epoch: 6200, acc: 0.914, loss: 0.307, data_loss: 0.222, reg_loss: 0.085, lr: 0.019938201544313403\n",
      "epoch: 6300, acc: 0.912, loss: 0.321, data_loss: 0.222, reg_loss: 0.098, lr: 0.01993720776414682\n",
      "epoch: 6400, acc: 0.911, loss: 0.311, data_loss: 0.215, reg_loss: 0.096, lr: 0.019936214083041307\n",
      "epoch: 6500, acc: 0.912, loss: 0.308, data_loss: 0.213, reg_loss: 0.094, lr: 0.01993522050098206\n",
      "epoch: 6600, acc: 0.913, loss: 0.306, data_loss: 0.213, reg_loss: 0.093, lr: 0.019934227017954262\n",
      "epoch: 6700, acc: 0.915, loss: 0.304, data_loss: 0.211, reg_loss: 0.092, lr: 0.01993323363394311\n",
      "epoch: 6800, acc: 0.914, loss: 0.302, data_loss: 0.211, reg_loss: 0.091, lr: 0.0199322403489338\n",
      "epoch: 6900, acc: 0.916, loss: 0.301, data_loss: 0.211, reg_loss: 0.091, lr: 0.019931247162911534\n",
      "epoch: 7000, acc: 0.917, loss: 0.299, data_loss: 0.209, reg_loss: 0.090, lr: 0.019930254075861523\n",
      "epoch: 7100, acc: 0.915, loss: 0.302, data_loss: 0.213, reg_loss: 0.089, lr: 0.019929261087768962\n",
      "epoch: 7200, acc: 0.915, loss: 0.296, data_loss: 0.208, reg_loss: 0.089, lr: 0.01992826819861907\n",
      "epoch: 7300, acc: 0.919, loss: 0.295, data_loss: 0.207, reg_loss: 0.088, lr: 0.019927275408397054\n",
      "epoch: 7400, acc: 0.917, loss: 0.299, data_loss: 0.212, reg_loss: 0.087, lr: 0.019926282717088132\n",
      "epoch: 7500, acc: 0.916, loss: 0.297, data_loss: 0.211, reg_loss: 0.087, lr: 0.01992529012467752\n",
      "epoch: 7600, acc: 0.916, loss: 0.299, data_loss: 0.213, reg_loss: 0.086, lr: 0.019924297631150445\n",
      "epoch: 7700, acc: 0.917, loss: 0.290, data_loss: 0.205, reg_loss: 0.085, lr: 0.019923305236492123\n",
      "epoch: 7800, acc: 0.919, loss: 0.289, data_loss: 0.204, reg_loss: 0.085, lr: 0.01992231294068779\n",
      "epoch: 7900, acc: 0.915, loss: 0.300, data_loss: 0.209, reg_loss: 0.091, lr: 0.019921320743722666\n",
      "epoch: 8000, acc: 0.918, loss: 0.295, data_loss: 0.206, reg_loss: 0.089, lr: 0.019920328645582\n",
      "epoch: 8100, acc: 0.917, loss: 0.294, data_loss: 0.205, reg_loss: 0.088, lr: 0.019919336646251007\n",
      "epoch: 8200, acc: 0.919, loss: 0.292, data_loss: 0.205, reg_loss: 0.087, lr: 0.019918344745714942\n",
      "epoch: 8300, acc: 0.919, loss: 0.291, data_loss: 0.205, reg_loss: 0.086, lr: 0.019917352943959042\n",
      "epoch: 8400, acc: 0.919, loss: 0.289, data_loss: 0.204, reg_loss: 0.085, lr: 0.019916361240968555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8500, acc: 0.920, loss: 0.291, data_loss: 0.207, reg_loss: 0.084, lr: 0.01991536963672872\n",
      "epoch: 8600, acc: 0.917, loss: 0.290, data_loss: 0.207, reg_loss: 0.083, lr: 0.019914378131224802\n",
      "epoch: 8700, acc: 0.918, loss: 0.290, data_loss: 0.208, reg_loss: 0.083, lr: 0.01991338672444204\n",
      "epoch: 8800, acc: 0.920, loss: 0.286, data_loss: 0.204, reg_loss: 0.082, lr: 0.0199123954163657\n",
      "epoch: 8900, acc: 0.920, loss: 0.284, data_loss: 0.203, reg_loss: 0.081, lr: 0.019911404206981037\n",
      "epoch: 9000, acc: 0.920, loss: 0.283, data_loss: 0.203, reg_loss: 0.081, lr: 0.019910413096273318\n",
      "epoch: 9100, acc: 0.917, loss: 0.285, data_loss: 0.205, reg_loss: 0.080, lr: 0.019909422084227805\n",
      "epoch: 9200, acc: 0.915, loss: 0.288, data_loss: 0.209, reg_loss: 0.079, lr: 0.019908431170829768\n",
      "epoch: 9300, acc: 0.910, loss: 0.340, data_loss: 0.247, reg_loss: 0.093, lr: 0.01990744035606448\n",
      "epoch: 9400, acc: 0.916, loss: 0.314, data_loss: 0.225, reg_loss: 0.089, lr: 0.01990644963991721\n",
      "epoch: 9500, acc: 0.916, loss: 0.309, data_loss: 0.220, reg_loss: 0.089, lr: 0.01990545902237324\n",
      "epoch: 9600, acc: 0.918, loss: 0.306, data_loss: 0.217, reg_loss: 0.089, lr: 0.019904468503417844\n",
      "epoch: 9700, acc: 0.919, loss: 0.304, data_loss: 0.215, reg_loss: 0.089, lr: 0.019903478083036316\n",
      "epoch: 9800, acc: 0.902, loss: 0.329, data_loss: 0.240, reg_loss: 0.089, lr: 0.019902487761213932\n",
      "epoch: 9900, acc: 0.918, loss: 0.301, data_loss: 0.213, reg_loss: 0.088, lr: 0.019901497537935988\n",
      "epoch: 10000, acc: 0.919, loss: 0.299, data_loss: 0.211, reg_loss: 0.088, lr: 0.019900507413187767\n",
      "validation, acc: 0.890, loss: 0.233\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 1000, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 512, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(512, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.02, decay=5e-7)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # regularizatin penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "    \n",
    "    # overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "# validate the model\n",
    "\n",
    "# creating dataset\n",
    "X_test, y_test = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# calculating accuracy of output of activation 2 and targets\n",
    "# calculate values along first axis because of batches\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb99f0f",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Another regulization method, that randomly disables a specified portion of the neurons of a drouput layer per forward pass, forcing the model to learn from the data using multiple neurons. Also helps with **co-adoption** where a neuron doesn't learn on its own, but just passes along its input(s). Also helps with noise.\n",
    "\n",
    "In code, a **Bernoulli Distribution**, where the only two outputs are a discrete 1 or a 0, and the chance of it being a 1, is *p*, and chance of it being a 0 is *q* or *1 - p*. The Bernoilli Distribution is a special case of Binomal Distribution, and the binomal method from numpy will be used to create the array of 1s and 0s to multiply the outputs of this layer.\n",
    "\n",
    "One problem with zeroing out a specific portion of the outputs is that the total sum of the ouputs will be lessened by that amount, so the neurons in the next layer will learn incorrectly and will cause problem when predicting. Hence, the non-zeroed outputs are scaled up that much to make up for it, by being multipled by *1 - droput rate*\n",
    "\n",
    "As for the partial derivative, if the value isn't zeroed, then the derivative will be the scaling up factor, or 1/(1-dropout), and if the value is zeroed, the derivative will be 0. Both cases can be encapsulated by the single equation, r/(1-dropout), where r is the 1 or 0 associated with that output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a61a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dropout:\n",
    "    \n",
    "    # init\n",
    "    def __init__(self, rate):\n",
    "        # store keep rate, we invert it as the dropout rate instead\n",
    "        self.rate = 1 - rate\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inputs):\n",
    "        # save input values\n",
    "        self.inputs = inputs\n",
    "        # generate and save scald mask for calculating p.d. later\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                                             size = inputs.shape) / \\\n",
    "                                             self.rate\n",
    "        # apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a4b95",
   "metadata": {},
   "source": [
    "A higher learning and decay rates work better with a dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d1b8637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.351, loss: 1.099, data_loss: 1.099, reg_loss: 0.000, lr: 0.05\n",
      "epoch: 100, acc: 0.571, loss: 0.949, data_loss: 0.907, reg_loss: 0.042, lr: 0.04975371909050202\n",
      "epoch: 200, acc: 0.649, loss: 0.858, data_loss: 0.801, reg_loss: 0.057, lr: 0.049507401356502806\n",
      "epoch: 300, acc: 0.667, loss: 0.826, data_loss: 0.767, reg_loss: 0.059, lr: 0.0492635105177595\n",
      "epoch: 400, acc: 0.665, loss: 0.834, data_loss: 0.776, reg_loss: 0.058, lr: 0.04902201088288642\n",
      "epoch: 500, acc: 0.689, loss: 0.807, data_loss: 0.748, reg_loss: 0.058, lr: 0.048782867456949125\n",
      "epoch: 600, acc: 0.689, loss: 0.785, data_loss: 0.729, reg_loss: 0.056, lr: 0.04854604592455945\n",
      "epoch: 700, acc: 0.681, loss: 0.787, data_loss: 0.732, reg_loss: 0.055, lr: 0.048311512633460556\n",
      "epoch: 800, acc: 0.680, loss: 0.769, data_loss: 0.715, reg_loss: 0.054, lr: 0.04807923457858551\n",
      "epoch: 900, acc: 0.686, loss: 0.795, data_loss: 0.742, reg_loss: 0.053, lr: 0.04784917938657352\n",
      "epoch: 1000, acc: 0.685, loss: 0.797, data_loss: 0.745, reg_loss: 0.052, lr: 0.04762131530072861\n",
      "epoch: 1100, acc: 0.697, loss: 0.768, data_loss: 0.717, reg_loss: 0.052, lr: 0.04739561116640599\n",
      "epoch: 1200, acc: 0.701, loss: 0.765, data_loss: 0.711, reg_loss: 0.054, lr: 0.04717203641681212\n",
      "epoch: 1300, acc: 0.682, loss: 0.748, data_loss: 0.693, reg_loss: 0.055, lr: 0.04695056105920466\n",
      "epoch: 1400, acc: 0.688, loss: 0.742, data_loss: 0.686, reg_loss: 0.056, lr: 0.04673115566147951\n",
      "epoch: 1500, acc: 0.703, loss: 0.721, data_loss: 0.665, reg_loss: 0.056, lr: 0.046513791339132055\n",
      "epoch: 1600, acc: 0.704, loss: 0.742, data_loss: 0.686, reg_loss: 0.056, lr: 0.04629843974258068\n",
      "epoch: 1700, acc: 0.700, loss: 0.779, data_loss: 0.723, reg_loss: 0.055, lr: 0.046085073044840774\n",
      "epoch: 1800, acc: 0.692, loss: 0.747, data_loss: 0.692, reg_loss: 0.055, lr: 0.04587366392953806\n",
      "epoch: 1900, acc: 0.715, loss: 0.717, data_loss: 0.663, reg_loss: 0.054, lr: 0.04566418557925019\n",
      "epoch: 2000, acc: 0.706, loss: 0.745, data_loss: 0.692, reg_loss: 0.053, lr: 0.045456611664166556\n",
      "epoch: 2100, acc: 0.709, loss: 0.730, data_loss: 0.677, reg_loss: 0.053, lr: 0.045250916331055706\n",
      "epoch: 2200, acc: 0.690, loss: 0.774, data_loss: 0.722, reg_loss: 0.052, lr: 0.0450470741925312\n",
      "epoch: 2300, acc: 0.722, loss: 0.717, data_loss: 0.665, reg_loss: 0.052, lr: 0.04484506031660612\n",
      "epoch: 2400, acc: 0.723, loss: 0.700, data_loss: 0.648, reg_loss: 0.051, lr: 0.04464485021652753\n",
      "epoch: 2500, acc: 0.709, loss: 0.693, data_loss: 0.643, reg_loss: 0.050, lr: 0.044446419840881816\n",
      "epoch: 2600, acc: 0.707, loss: 0.742, data_loss: 0.692, reg_loss: 0.050, lr: 0.04424974556396301\n",
      "epoch: 2700, acc: 0.704, loss: 0.727, data_loss: 0.677, reg_loss: 0.050, lr: 0.04405480417639544\n",
      "epoch: 2800, acc: 0.713, loss: 0.706, data_loss: 0.657, reg_loss: 0.049, lr: 0.04386157287600334\n",
      "epoch: 2900, acc: 0.730, loss: 0.696, data_loss: 0.648, reg_loss: 0.048, lr: 0.04367002925891961\n",
      "epoch: 3000, acc: 0.725, loss: 0.702, data_loss: 0.654, reg_loss: 0.048, lr: 0.043480151310926564\n",
      "epoch: 3100, acc: 0.710, loss: 0.731, data_loss: 0.683, reg_loss: 0.048, lr: 0.04329191739902161\n",
      "epoch: 3200, acc: 0.695, loss: 0.755, data_loss: 0.706, reg_loss: 0.048, lr: 0.043105306263201\n",
      "epoch: 3300, acc: 0.711, loss: 0.715, data_loss: 0.668, reg_loss: 0.048, lr: 0.0429202970084553\n",
      "epoch: 3400, acc: 0.700, loss: 0.708, data_loss: 0.660, reg_loss: 0.048, lr: 0.04273686909696996\n",
      "epoch: 3500, acc: 0.721, loss: 0.718, data_loss: 0.672, reg_loss: 0.047, lr: 0.04255500234052514\n",
      "epoch: 3600, acc: 0.714, loss: 0.712, data_loss: 0.666, reg_loss: 0.046, lr: 0.042374676893088686\n",
      "epoch: 3700, acc: 0.713, loss: 0.726, data_loss: 0.680, reg_loss: 0.045, lr: 0.042195873243596776\n",
      "epoch: 3800, acc: 0.735, loss: 0.682, data_loss: 0.637, reg_loss: 0.045, lr: 0.04201857220891634\n",
      "epoch: 3900, acc: 0.704, loss: 0.716, data_loss: 0.671, reg_loss: 0.045, lr: 0.041842754926984395\n",
      "epoch: 4000, acc: 0.713, loss: 0.709, data_loss: 0.664, reg_loss: 0.045, lr: 0.04166840285011875\n",
      "epoch: 4100, acc: 0.689, loss: 0.720, data_loss: 0.676, reg_loss: 0.044, lr: 0.041495497738495375\n",
      "epoch: 4200, acc: 0.705, loss: 0.719, data_loss: 0.674, reg_loss: 0.045, lr: 0.041324021653787346\n",
      "epoch: 4300, acc: 0.725, loss: 0.693, data_loss: 0.648, reg_loss: 0.044, lr: 0.041153956952961035\n",
      "epoch: 4400, acc: 0.715, loss: 0.718, data_loss: 0.673, reg_loss: 0.044, lr: 0.040985286282224684\n",
      "epoch: 4500, acc: 0.717, loss: 0.711, data_loss: 0.667, reg_loss: 0.044, lr: 0.04081799257112535\n",
      "epoch: 4600, acc: 0.715, loss: 0.696, data_loss: 0.653, reg_loss: 0.043, lr: 0.04065205902678971\n",
      "epoch: 4700, acc: 0.720, loss: 0.722, data_loss: 0.677, reg_loss: 0.044, lr: 0.04048746912830479\n",
      "epoch: 4800, acc: 0.705, loss: 0.724, data_loss: 0.680, reg_loss: 0.044, lr: 0.04032420662123473\n",
      "epoch: 4900, acc: 0.707, loss: 0.682, data_loss: 0.638, reg_loss: 0.044, lr: 0.04016225551226957\n",
      "epoch: 5000, acc: 0.703, loss: 0.723, data_loss: 0.679, reg_loss: 0.044, lr: 0.04000160006400256\n",
      "epoch: 5100, acc: 0.703, loss: 0.705, data_loss: 0.661, reg_loss: 0.044, lr: 0.039842224789832265\n",
      "epoch: 5200, acc: 0.718, loss: 0.714, data_loss: 0.670, reg_loss: 0.044, lr: 0.03968411444898608\n",
      "epoch: 5300, acc: 0.735, loss: 0.666, data_loss: 0.622, reg_loss: 0.043, lr: 0.03952725404166173\n",
      "epoch: 5400, acc: 0.710, loss: 0.721, data_loss: 0.677, reg_loss: 0.044, lr: 0.03937162880428363\n",
      "epoch: 5500, acc: 0.715, loss: 0.699, data_loss: 0.655, reg_loss: 0.044, lr: 0.03921722420487078\n",
      "epoch: 5600, acc: 0.715, loss: 0.727, data_loss: 0.683, reg_loss: 0.044, lr: 0.03906402593851323\n",
      "epoch: 5700, acc: 0.726, loss: 0.690, data_loss: 0.645, reg_loss: 0.045, lr: 0.038912019922954205\n",
      "epoch: 5800, acc: 0.715, loss: 0.704, data_loss: 0.659, reg_loss: 0.045, lr: 0.038761192294274965\n",
      "epoch: 5900, acc: 0.701, loss: 0.677, data_loss: 0.631, reg_loss: 0.046, lr: 0.038611529402679645\n",
      "epoch: 6000, acc: 0.718, loss: 0.724, data_loss: 0.677, reg_loss: 0.046, lr: 0.03846301780837725\n",
      "epoch: 6100, acc: 0.716, loss: 0.716, data_loss: 0.670, reg_loss: 0.047, lr: 0.03831564427755853\n",
      "epoch: 6200, acc: 0.703, loss: 0.678, data_loss: 0.631, reg_loss: 0.047, lr: 0.03816939577846483\n",
      "epoch: 6300, acc: 0.727, loss: 0.712, data_loss: 0.664, reg_loss: 0.047, lr: 0.038024259477546674\n",
      "epoch: 6400, acc: 0.721, loss: 0.674, data_loss: 0.627, reg_loss: 0.047, lr: 0.03788022273570969\n",
      "epoch: 6500, acc: 0.717, loss: 0.683, data_loss: 0.637, reg_loss: 0.047, lr: 0.03773727310464546\n",
      "epoch: 6600, acc: 0.731, loss: 0.679, data_loss: 0.633, reg_loss: 0.046, lr: 0.03759539832324524\n",
      "epoch: 6700, acc: 0.713, loss: 0.676, data_loss: 0.630, reg_loss: 0.047, lr: 0.03745458631409416\n",
      "epoch: 6800, acc: 0.729, loss: 0.688, data_loss: 0.641, reg_loss: 0.048, lr: 0.03731482518004403\n",
      "epoch: 6900, acc: 0.741, loss: 0.671, data_loss: 0.624, reg_loss: 0.047, lr: 0.03717610320086248\n",
      "epoch: 7000, acc: 0.724, loss: 0.656, data_loss: 0.609, reg_loss: 0.047, lr: 0.03703840882995667\n",
      "epoch: 7100, acc: 0.721, loss: 0.689, data_loss: 0.642, reg_loss: 0.047, lr: 0.036901730691169414\n",
      "epoch: 7200, acc: 0.719, loss: 0.690, data_loss: 0.643, reg_loss: 0.046, lr: 0.03676605757564617\n",
      "epoch: 7300, acc: 0.729, loss: 0.684, data_loss: 0.638, reg_loss: 0.046, lr: 0.03663137843877066\n",
      "epoch: 7400, acc: 0.731, loss: 0.672, data_loss: 0.627, reg_loss: 0.046, lr: 0.03649768239716778\n",
      "epoch: 7500, acc: 0.717, loss: 0.673, data_loss: 0.627, reg_loss: 0.046, lr: 0.03636495872577185\n",
      "epoch: 7600, acc: 0.737, loss: 0.680, data_loss: 0.634, reg_loss: 0.046, lr: 0.03623319685495851\n",
      "epoch: 7700, acc: 0.726, loss: 0.693, data_loss: 0.647, reg_loss: 0.046, lr: 0.03610238636773891\n",
      "epoch: 7800, acc: 0.695, loss: 0.716, data_loss: 0.670, reg_loss: 0.046, lr: 0.03597251699701428\n",
      "epoch: 7900, acc: 0.723, loss: 0.690, data_loss: 0.644, reg_loss: 0.045, lr: 0.035843578622889706\n",
      "epoch: 8000, acc: 0.715, loss: 0.687, data_loss: 0.641, reg_loss: 0.046, lr: 0.03571556127004536\n",
      "epoch: 8100, acc: 0.743, loss: 0.645, data_loss: 0.600, reg_loss: 0.045, lr: 0.03558845510516389\n",
      "epoch: 8200, acc: 0.721, loss: 0.651, data_loss: 0.607, reg_loss: 0.045, lr: 0.03546225043441257\n",
      "epoch: 8300, acc: 0.733, loss: 0.667, data_loss: 0.622, reg_loss: 0.045, lr: 0.035336937700978836\n",
      "epoch: 8400, acc: 0.717, loss: 0.674, data_loss: 0.629, reg_loss: 0.045, lr: 0.03521250748265784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8500, acc: 0.709, loss: 0.696, data_loss: 0.652, reg_loss: 0.044, lr: 0.035088950489490865\n",
      "epoch: 8600, acc: 0.732, loss: 0.656, data_loss: 0.612, reg_loss: 0.044, lr: 0.0349662575614532\n",
      "epoch: 8700, acc: 0.735, loss: 0.669, data_loss: 0.625, reg_loss: 0.044, lr: 0.034844419666190465\n",
      "epoch: 8800, acc: 0.721, loss: 0.699, data_loss: 0.654, reg_loss: 0.045, lr: 0.034723427896801974\n",
      "epoch: 8900, acc: 0.705, loss: 0.671, data_loss: 0.626, reg_loss: 0.045, lr: 0.03460327346967023\n",
      "epoch: 9000, acc: 0.723, loss: 0.666, data_loss: 0.622, reg_loss: 0.044, lr: 0.034483947722335255\n",
      "epoch: 9100, acc: 0.717, loss: 0.695, data_loss: 0.651, reg_loss: 0.044, lr: 0.034365442111412764\n",
      "epoch: 9200, acc: 0.713, loss: 0.673, data_loss: 0.629, reg_loss: 0.044, lr: 0.03424774821055516\n",
      "epoch: 9300, acc: 0.718, loss: 0.713, data_loss: 0.670, reg_loss: 0.044, lr: 0.03413085770845422\n",
      "epoch: 9400, acc: 0.707, loss: 0.670, data_loss: 0.627, reg_loss: 0.043, lr: 0.034014762406884586\n",
      "epoch: 9500, acc: 0.712, loss: 0.668, data_loss: 0.624, reg_loss: 0.044, lr: 0.03389945421878708\n",
      "epoch: 9600, acc: 0.706, loss: 0.692, data_loss: 0.649, reg_loss: 0.043, lr: 0.033784925166390756\n",
      "epoch: 9700, acc: 0.719, loss: 0.692, data_loss: 0.649, reg_loss: 0.043, lr: 0.03367116737937304\n",
      "epoch: 9800, acc: 0.719, loss: 0.648, data_loss: 0.605, reg_loss: 0.043, lr: 0.033558173093056816\n",
      "epoch: 9900, acc: 0.720, loss: 0.675, data_loss: 0.632, reg_loss: 0.043, lr: 0.0334459346466437\n",
      "epoch: 10000, acc: 0.714, loss: 0.661, data_loss: 0.619, reg_loss: 0.043, lr: 0.03333444448148271\n",
      "validation, acc: 0.723, loss: 0.643\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 500, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# dropout layer\n",
    "dropout1 = Layer_Dropout(rate = 0.1) # ie. dropout rate of 90%\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # forward pass through dropout\n",
    "    dropout1.forward(activation1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(dropout1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # regularizatin penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "    \n",
    "    # overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "# validate the model\n",
    "\n",
    "# creating dataset\n",
    "X_test, y_test = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# calculating accuracy of output of activation 2 and targets\n",
    "# calculate values along first axis because of batches\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e4113",
   "metadata": {},
   "source": [
    "The validation set performs better than the training here because the dropout layer isn't used, allowing the model to use all the neurons. Because of the regularization, a larger model can now be helpful in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e30ea15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.327, loss: 1.099, data_loss: 1.099, reg_loss: 0.000, lr: 0.05\n",
      "epoch: 100, acc: 0.743, loss: 0.693, data_loss: 0.566, reg_loss: 0.126, lr: 0.04975371909050202\n",
      "epoch: 200, acc: 0.797, loss: 0.668, data_loss: 0.518, reg_loss: 0.151, lr: 0.049507401356502806\n",
      "epoch: 300, acc: 0.803, loss: 0.654, data_loss: 0.492, reg_loss: 0.162, lr: 0.0492635105177595\n",
      "epoch: 400, acc: 0.823, loss: 0.588, data_loss: 0.421, reg_loss: 0.167, lr: 0.04902201088288642\n",
      "epoch: 500, acc: 0.810, loss: 0.590, data_loss: 0.415, reg_loss: 0.175, lr: 0.048782867456949125\n",
      "epoch: 600, acc: 0.837, loss: 0.556, data_loss: 0.385, reg_loss: 0.171, lr: 0.04854604592455945\n",
      "epoch: 700, acc: 0.830, loss: 0.607, data_loss: 0.440, reg_loss: 0.168, lr: 0.048311512633460556\n",
      "epoch: 800, acc: 0.820, loss: 0.600, data_loss: 0.433, reg_loss: 0.167, lr: 0.04807923457858551\n",
      "epoch: 900, acc: 0.867, loss: 0.542, data_loss: 0.373, reg_loss: 0.170, lr: 0.04784917938657352\n",
      "epoch: 1000, acc: 0.850, loss: 0.580, data_loss: 0.410, reg_loss: 0.171, lr: 0.04762131530072861\n",
      "epoch: 1100, acc: 0.847, loss: 0.519, data_loss: 0.350, reg_loss: 0.169, lr: 0.04739561116640599\n",
      "epoch: 1200, acc: 0.877, loss: 0.532, data_loss: 0.367, reg_loss: 0.166, lr: 0.04717203641681212\n",
      "epoch: 1300, acc: 0.863, loss: 0.544, data_loss: 0.380, reg_loss: 0.164, lr: 0.04695056105920466\n",
      "epoch: 1400, acc: 0.857, loss: 0.569, data_loss: 0.405, reg_loss: 0.164, lr: 0.04673115566147951\n",
      "epoch: 1500, acc: 0.847, loss: 0.527, data_loss: 0.360, reg_loss: 0.167, lr: 0.046513791339132055\n",
      "epoch: 1600, acc: 0.840, loss: 0.548, data_loss: 0.379, reg_loss: 0.168, lr: 0.04629843974258068\n",
      "epoch: 1700, acc: 0.877, loss: 0.523, data_loss: 0.362, reg_loss: 0.162, lr: 0.046085073044840774\n",
      "epoch: 1800, acc: 0.830, loss: 0.517, data_loss: 0.352, reg_loss: 0.165, lr: 0.04587366392953806\n",
      "epoch: 1900, acc: 0.850, loss: 0.585, data_loss: 0.421, reg_loss: 0.163, lr: 0.04566418557925019\n",
      "epoch: 2000, acc: 0.857, loss: 0.550, data_loss: 0.383, reg_loss: 0.167, lr: 0.045456611664166556\n",
      "epoch: 2100, acc: 0.860, loss: 0.491, data_loss: 0.328, reg_loss: 0.163, lr: 0.045250916331055706\n",
      "epoch: 2200, acc: 0.837, loss: 0.532, data_loss: 0.371, reg_loss: 0.161, lr: 0.0450470741925312\n",
      "epoch: 2300, acc: 0.847, loss: 0.536, data_loss: 0.377, reg_loss: 0.159, lr: 0.04484506031660612\n",
      "epoch: 2400, acc: 0.867, loss: 0.539, data_loss: 0.381, reg_loss: 0.158, lr: 0.04464485021652753\n",
      "epoch: 2500, acc: 0.840, loss: 0.574, data_loss: 0.417, reg_loss: 0.157, lr: 0.044446419840881816\n",
      "epoch: 2600, acc: 0.880, loss: 0.541, data_loss: 0.383, reg_loss: 0.158, lr: 0.04424974556396301\n",
      "epoch: 2700, acc: 0.837, loss: 0.579, data_loss: 0.418, reg_loss: 0.161, lr: 0.04405480417639544\n",
      "epoch: 2800, acc: 0.847, loss: 0.504, data_loss: 0.347, reg_loss: 0.158, lr: 0.04386157287600334\n",
      "epoch: 2900, acc: 0.863, loss: 0.536, data_loss: 0.376, reg_loss: 0.160, lr: 0.04367002925891961\n",
      "epoch: 3000, acc: 0.830, loss: 0.585, data_loss: 0.425, reg_loss: 0.160, lr: 0.043480151310926564\n",
      "epoch: 3100, acc: 0.880, loss: 0.524, data_loss: 0.364, reg_loss: 0.160, lr: 0.04329191739902161\n",
      "epoch: 3200, acc: 0.833, loss: 0.585, data_loss: 0.428, reg_loss: 0.157, lr: 0.043105306263201\n",
      "epoch: 3300, acc: 0.843, loss: 0.578, data_loss: 0.421, reg_loss: 0.157, lr: 0.0429202970084553\n",
      "epoch: 3400, acc: 0.850, loss: 0.504, data_loss: 0.348, reg_loss: 0.156, lr: 0.04273686909696996\n",
      "epoch: 3500, acc: 0.867, loss: 0.485, data_loss: 0.330, reg_loss: 0.155, lr: 0.04255500234052514\n",
      "epoch: 3600, acc: 0.843, loss: 0.579, data_loss: 0.426, reg_loss: 0.153, lr: 0.042374676893088686\n",
      "epoch: 3700, acc: 0.840, loss: 0.530, data_loss: 0.377, reg_loss: 0.153, lr: 0.042195873243596776\n",
      "epoch: 3800, acc: 0.833, loss: 0.594, data_loss: 0.442, reg_loss: 0.152, lr: 0.04201857220891634\n",
      "epoch: 3900, acc: 0.870, loss: 0.478, data_loss: 0.325, reg_loss: 0.152, lr: 0.041842754926984395\n",
      "epoch: 4000, acc: 0.863, loss: 0.527, data_loss: 0.370, reg_loss: 0.156, lr: 0.04166840285011875\n",
      "epoch: 4100, acc: 0.887, loss: 0.479, data_loss: 0.323, reg_loss: 0.156, lr: 0.041495497738495375\n",
      "epoch: 4200, acc: 0.860, loss: 0.513, data_loss: 0.356, reg_loss: 0.157, lr: 0.041324021653787346\n",
      "epoch: 4300, acc: 0.860, loss: 0.520, data_loss: 0.365, reg_loss: 0.155, lr: 0.041153956952961035\n",
      "epoch: 4400, acc: 0.883, loss: 0.488, data_loss: 0.332, reg_loss: 0.156, lr: 0.040985286282224684\n",
      "epoch: 4500, acc: 0.853, loss: 0.479, data_loss: 0.324, reg_loss: 0.155, lr: 0.04081799257112535\n",
      "epoch: 4600, acc: 0.837, loss: 0.577, data_loss: 0.426, reg_loss: 0.151, lr: 0.04065205902678971\n",
      "epoch: 4700, acc: 0.850, loss: 0.575, data_loss: 0.423, reg_loss: 0.152, lr: 0.04048746912830479\n",
      "epoch: 4800, acc: 0.873, loss: 0.562, data_loss: 0.413, reg_loss: 0.150, lr: 0.04032420662123473\n",
      "epoch: 4900, acc: 0.877, loss: 0.539, data_loss: 0.388, reg_loss: 0.151, lr: 0.04016225551226957\n",
      "epoch: 5000, acc: 0.813, loss: 0.531, data_loss: 0.381, reg_loss: 0.151, lr: 0.04000160006400256\n",
      "epoch: 5100, acc: 0.860, loss: 0.574, data_loss: 0.417, reg_loss: 0.156, lr: 0.039842224789832265\n",
      "epoch: 5200, acc: 0.870, loss: 0.515, data_loss: 0.359, reg_loss: 0.156, lr: 0.03968411444898608\n",
      "epoch: 5300, acc: 0.857, loss: 0.488, data_loss: 0.336, reg_loss: 0.152, lr: 0.03952725404166173\n",
      "epoch: 5400, acc: 0.880, loss: 0.479, data_loss: 0.326, reg_loss: 0.153, lr: 0.03937162880428363\n",
      "epoch: 5500, acc: 0.850, loss: 0.519, data_loss: 0.368, reg_loss: 0.151, lr: 0.03921722420487078\n",
      "epoch: 5600, acc: 0.853, loss: 0.528, data_loss: 0.373, reg_loss: 0.155, lr: 0.03906402593851323\n",
      "epoch: 5700, acc: 0.867, loss: 0.506, data_loss: 0.351, reg_loss: 0.155, lr: 0.038912019922954205\n",
      "epoch: 5800, acc: 0.867, loss: 0.489, data_loss: 0.334, reg_loss: 0.154, lr: 0.038761192294274965\n",
      "epoch: 5900, acc: 0.873, loss: 0.477, data_loss: 0.323, reg_loss: 0.154, lr: 0.038611529402679645\n",
      "epoch: 6000, acc: 0.847, loss: 0.540, data_loss: 0.386, reg_loss: 0.154, lr: 0.03846301780837725\n",
      "epoch: 6100, acc: 0.887, loss: 0.550, data_loss: 0.396, reg_loss: 0.154, lr: 0.03831564427755853\n",
      "epoch: 6200, acc: 0.870, loss: 0.489, data_loss: 0.334, reg_loss: 0.154, lr: 0.03816939577846483\n",
      "epoch: 6300, acc: 0.843, loss: 0.585, data_loss: 0.428, reg_loss: 0.156, lr: 0.038024259477546674\n",
      "epoch: 6400, acc: 0.853, loss: 0.459, data_loss: 0.300, reg_loss: 0.158, lr: 0.03788022273570969\n",
      "epoch: 6500, acc: 0.883, loss: 0.470, data_loss: 0.315, reg_loss: 0.155, lr: 0.03773727310464546\n",
      "epoch: 6600, acc: 0.883, loss: 0.513, data_loss: 0.363, reg_loss: 0.150, lr: 0.03759539832324524\n",
      "epoch: 6700, acc: 0.850, loss: 0.546, data_loss: 0.395, reg_loss: 0.150, lr: 0.03745458631409416\n",
      "epoch: 6800, acc: 0.867, loss: 0.518, data_loss: 0.370, reg_loss: 0.148, lr: 0.03731482518004403\n",
      "epoch: 6900, acc: 0.837, loss: 0.603, data_loss: 0.455, reg_loss: 0.149, lr: 0.03717610320086248\n",
      "epoch: 7000, acc: 0.863, loss: 0.501, data_loss: 0.354, reg_loss: 0.147, lr: 0.03703840882995667\n",
      "epoch: 7100, acc: 0.880, loss: 0.495, data_loss: 0.346, reg_loss: 0.150, lr: 0.036901730691169414\n",
      "epoch: 7200, acc: 0.873, loss: 0.501, data_loss: 0.351, reg_loss: 0.151, lr: 0.03676605757564617\n",
      "epoch: 7300, acc: 0.870, loss: 0.477, data_loss: 0.328, reg_loss: 0.149, lr: 0.03663137843877066\n",
      "epoch: 7400, acc: 0.867, loss: 0.498, data_loss: 0.348, reg_loss: 0.151, lr: 0.03649768239716778\n",
      "epoch: 7500, acc: 0.897, loss: 0.461, data_loss: 0.314, reg_loss: 0.147, lr: 0.03636495872577185\n",
      "epoch: 7600, acc: 0.833, loss: 0.544, data_loss: 0.396, reg_loss: 0.148, lr: 0.03623319685495851\n",
      "epoch: 7700, acc: 0.843, loss: 0.491, data_loss: 0.342, reg_loss: 0.148, lr: 0.03610238636773891\n",
      "epoch: 7800, acc: 0.880, loss: 0.457, data_loss: 0.311, reg_loss: 0.146, lr: 0.03597251699701428\n",
      "epoch: 7900, acc: 0.867, loss: 0.515, data_loss: 0.369, reg_loss: 0.146, lr: 0.035843578622889706\n",
      "epoch: 8000, acc: 0.850, loss: 0.505, data_loss: 0.355, reg_loss: 0.150, lr: 0.03571556127004536\n",
      "epoch: 8100, acc: 0.870, loss: 0.476, data_loss: 0.327, reg_loss: 0.149, lr: 0.03558845510516389\n",
      "epoch: 8200, acc: 0.827, loss: 0.499, data_loss: 0.349, reg_loss: 0.150, lr: 0.03546225043441257\n",
      "epoch: 8300, acc: 0.857, loss: 0.493, data_loss: 0.343, reg_loss: 0.150, lr: 0.035336937700978836\n",
      "epoch: 8400, acc: 0.853, loss: 0.472, data_loss: 0.328, reg_loss: 0.144, lr: 0.03521250748265784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8500, acc: 0.867, loss: 0.500, data_loss: 0.351, reg_loss: 0.149, lr: 0.035088950489490865\n",
      "epoch: 8600, acc: 0.870, loss: 0.487, data_loss: 0.340, reg_loss: 0.148, lr: 0.0349662575614532\n",
      "epoch: 8700, acc: 0.877, loss: 0.547, data_loss: 0.397, reg_loss: 0.149, lr: 0.034844419666190465\n",
      "epoch: 8800, acc: 0.840, loss: 0.599, data_loss: 0.451, reg_loss: 0.148, lr: 0.034723427896801974\n",
      "epoch: 8900, acc: 0.863, loss: 0.508, data_loss: 0.359, reg_loss: 0.148, lr: 0.03460327346967023\n",
      "epoch: 9000, acc: 0.867, loss: 0.517, data_loss: 0.364, reg_loss: 0.153, lr: 0.034483947722335255\n",
      "epoch: 9100, acc: 0.860, loss: 0.517, data_loss: 0.365, reg_loss: 0.152, lr: 0.034365442111412764\n",
      "epoch: 9200, acc: 0.870, loss: 0.505, data_loss: 0.355, reg_loss: 0.149, lr: 0.03424774821055516\n",
      "epoch: 9300, acc: 0.860, loss: 0.521, data_loss: 0.364, reg_loss: 0.156, lr: 0.03413085770845422\n",
      "epoch: 9400, acc: 0.877, loss: 0.484, data_loss: 0.333, reg_loss: 0.152, lr: 0.034014762406884586\n",
      "epoch: 9500, acc: 0.873, loss: 0.484, data_loss: 0.337, reg_loss: 0.147, lr: 0.03389945421878708\n",
      "epoch: 9600, acc: 0.867, loss: 0.501, data_loss: 0.355, reg_loss: 0.146, lr: 0.033784925166390756\n",
      "epoch: 9700, acc: 0.850, loss: 0.524, data_loss: 0.371, reg_loss: 0.153, lr: 0.03367116737937304\n",
      "epoch: 9800, acc: 0.883, loss: 0.499, data_loss: 0.353, reg_loss: 0.146, lr: 0.033558173093056816\n",
      "epoch: 9900, acc: 0.870, loss: 0.544, data_loss: 0.400, reg_loss: 0.145, lr: 0.0334459346466437\n",
      "epoch: 10000, acc: 0.887, loss: 0.455, data_loss: 0.313, reg_loss: 0.142, lr: 0.03333444448148271\n",
      "validation, acc: 0.867, loss: 0.437\n"
     ]
    }
   ],
   "source": [
    "# creating dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# dense layer wtih 2 input features and 64 ouput values\n",
    "dense1 = Layer_Dense(2, 512, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# dropout layer\n",
    "dropout1 = Layer_Dropout(0.1) # ie. dropout rate of 90%\n",
    "\n",
    "# second dense layer with 64 inputs features, an 3 output values\n",
    "dense2 = Layer_Dense(512, 3)\n",
    "\n",
    "# create softmax classifier's combined loss and acivation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
    "\n",
    "# training in loop\n",
    "for epoch in range(10001):\n",
    "    # forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # forward pass through activation\n",
    "    activation1.forward(dense1.output)\n",
    "    \n",
    "    # forward pass through dropout\n",
    "    dropout1.forward(activation1.output)\n",
    "\n",
    "    # forward pass through seecond dense layer\n",
    "    dense2.forward(dropout1.output)\n",
    "\n",
    "    # forward pass through activation/loss function\n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # regularizatin penalty\n",
    "    regularization_loss = \\\n",
    "        loss_activation.loss.regularization_loss(dense1) + \\\n",
    "        loss_activation.loss.regularization_loss(dense2)\n",
    "    \n",
    "    # overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # calculating accuracy of output of activation 2 and targets\n",
    "    # calculate values along first axis because of batches\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis = 1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    # update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    \n",
    "# validate the model\n",
    "\n",
    "# creating dataset\n",
    "X_test, y_test = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# forward pass of our training data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# forward pass through activation\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# forward pass through seecond dense layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# forward pass through activation/loss function\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "\n",
    "# calculating accuracy of output of activation 2 and targets\n",
    "# calculate values along first axis because of batches\n",
    "predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_test = np.argmax(y, axis = 1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e5805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
