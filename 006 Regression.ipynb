{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "837bec18",
   "metadata": {},
   "source": [
    "### Code Until Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "304a66cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # Set regularization strength\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # Gradients on regularization\n",
    "        # L1 on weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        # L2 on weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "                             self.weights\n",
    "        # L1 on biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "        # L2 on biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "                            self.biases\n",
    "\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# Dropout\n",
    "class Layer_Dropout:\n",
    "\n",
    "    # Init\n",
    "    def __init__(self, rate):\n",
    "        # Store rate, we invert it as for example for dropout\n",
    "        # of 0.1 we need success rate of 0.9\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input values\n",
    "        self.inputs = inputs\n",
    "        # Generate and save scaled mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate,\n",
    "                           size=inputs.shape) / self.rate\n",
    "        # Apply mask to output values\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable,\n",
    "        # let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "                                            keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "                                            keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in \\\n",
    "                enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,\n",
    "                                         single_dvalues)\n",
    "\n",
    "\n",
    "# Sigmoid activation\n",
    "class Activation_Sigmoid:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Save input and calculate/save output\n",
    "        # of the sigmoid function\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Derivative - calculates from output of the sigmoid function\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output\n",
    "\n",
    "\n",
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            # filled with zeros\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for weights\n",
    "                # The array doesn't exist for biases yet either.\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * \\\n",
    "                             layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * \\\n",
    "                           layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adagrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# RMSprop optimizer\n",
    "class Optimizer_RMSprop:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.rho * layer.weight_cache + \\\n",
    "            (1 - self.rho) * layer.dweights**2\n",
    "        layer.bias_cache = self.rho * layer.bias_cache + \\\n",
    "            (1 - self.rho) * layer.dbiases**2\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         layer.dweights / \\\n",
    "                         (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                        layer.dbiases / \\\n",
    "                        (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "                 beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update momentum  with current gradients\n",
    "        layer.weight_momentums = self.beta_1 * \\\n",
    "                                 layer.weight_momentums + \\\n",
    "                                 (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * \\\n",
    "                               layer.bias_momentums + \\\n",
    "                               (1 - self.beta_1) * layer.dbiases\n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        weight_momentums_corrected = layer.weight_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / \\\n",
    "            (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "            (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "            (1 - self.beta_2) * layer.dbiases**2\n",
    "        # Get corrected cache\n",
    "        weight_cache_corrected = layer.weight_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / \\\n",
    "            (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "                         weight_momentums_corrected / \\\n",
    "                         (np.sqrt(weight_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "                         bias_momentums_corrected / \\\n",
    "                         (np.sqrt(bias_cache_corrected) +\n",
    "                             self.epsilon)\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "    # Regularization loss calculation\n",
    "    def regularization_loss(self, layer):\n",
    "\n",
    "        # 0 by default\n",
    "        regularization_loss = 0\n",
    "\n",
    "        # L1 regularization - weights\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.weights))\n",
    "\n",
    "        # L2 regularization - weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * \\\n",
    "                                   np.sum(layer.weights *\n",
    "                                          layer.weights)\n",
    "\n",
    "        # L1 regularization - biases\n",
    "        # calculate only when factor greater than 0\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * \\\n",
    "                                   np.sum(np.abs(layer.biases))\n",
    "\n",
    "        # L2 regularization - biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * \\\n",
    "                                   np.sum(layer.biases *\n",
    "                                          layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        # Return loss\n",
    "        return data_loss\n",
    "\n",
    "\n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values -\n",
    "        # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[\n",
    "                range(samples),\n",
    "                y_true\n",
    "            ]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(\n",
    "                y_pred_clipped * y_true,\n",
    "                axis=1\n",
    "            )\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "\n",
    "        # If labels are one-hot encoded,\n",
    "        # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d6447",
   "metadata": {},
   "source": [
    "A classisification algorithm is concerned with *what* something is. A regression is concerned with a specfic value with respect to some input, eg. the temperature tomorrow or the price of a car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48a005",
   "metadata": {},
   "source": [
    "### Linear Activation\n",
    "A linear activation is used because the model is now trying to predict a scalar value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a26e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Linear:\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb025105",
   "metadata": {},
   "source": [
    "### Error Loss\n",
    "The two main error functions used in regression is \n",
    "\n",
    "* Mean Squared Error: sum up the squares of the difference between predicted and real values and divide by the number of outputs\n",
    "* Mean Absolute Error: sum up the absolute values of the differences between predicted and real values, and divide by the number of outputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c06033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanSquaredError(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis = -1)\n",
    "        # axis of -1, meaning innermost arrays\n",
    "        # for calculating multiple samples separately\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #the first sample used to count the number of outputs\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ded8143",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_MeanAbsoluteError(Loss):\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \n",
    "        sample_losses = np.mean(np.abs(y_true - y_pred), axis = -1)\n",
    "        # axis of -1, meaning innermost arrays\n",
    "        # for calculating multiple samples separately\n",
    "        \n",
    "        return sample_losses\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        \n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        #the first sample used to count the number of outputs\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # gradient on values\n",
    "        self.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "        # normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3b51f",
   "metadata": {},
   "source": [
    "### Accuracy\n",
    "In a regression model, it makes no sense to measure accuracy by checking if the predictions equals the true values, but an accuracy metric can be accomplished by adding \"cushion\" values through either sides, where the model will be deemed precise enough\n",
    "\n",
    "    accuracy_prediction = np.std(y) / 250\n",
    "    # dividing the standard deviation by the our choice of \"precision\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8271a518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.500 (data_loss: 0.500, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100, acc: 0.003, loss: 0.335 (data_loss: 0.335, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 200, acc: 0.004, loss: 0.166 (data_loss: 0.166, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 300, acc: 0.004, loss: 0.155 (data_loss: 0.155, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 400, acc: 0.004, loss: 0.154 (data_loss: 0.154, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 500, acc: 0.004, loss: 0.152 (data_loss: 0.152, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 600, acc: 0.004, loss: 0.151 (data_loss: 0.151, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 700, acc: 0.004, loss: 0.150 (data_loss: 0.150, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 800, acc: 0.004, loss: 0.149 (data_loss: 0.149, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 900, acc: 0.003, loss: 0.148 (data_loss: 0.148, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1000, acc: 0.005, loss: 0.147 (data_loss: 0.147, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1100, acc: 0.003, loss: 0.146 (data_loss: 0.146, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1200, acc: 0.003, loss: 0.146 (data_loss: 0.146, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1300, acc: 0.003, loss: 0.146 (data_loss: 0.146, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1400, acc: 0.004, loss: 0.145 (data_loss: 0.145, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1500, acc: 0.004, loss: 0.145 (data_loss: 0.145, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1600, acc: 0.003, loss: 0.144 (data_loss: 0.144, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1700, acc: 0.003, loss: 0.142 (data_loss: 0.142, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1800, acc: 0.003, loss: 0.138 (data_loss: 0.138, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 1900, acc: 0.003, loss: 0.132 (data_loss: 0.132, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2000, acc: 0.003, loss: 0.125 (data_loss: 0.125, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2100, acc: 0.003, loss: 0.116 (data_loss: 0.116, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2200, acc: 0.004, loss: 0.107 (data_loss: 0.107, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2300, acc: 0.004, loss: 0.097 (data_loss: 0.097, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2400, acc: 0.004, loss: 0.089 (data_loss: 0.089, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2500, acc: 0.005, loss: 0.080 (data_loss: 0.080, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2600, acc: 0.005, loss: 0.073 (data_loss: 0.073, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2700, acc: 0.004, loss: 0.067 (data_loss: 0.067, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2800, acc: 0.006, loss: 0.062 (data_loss: 0.062, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2900, acc: 0.006, loss: 0.057 (data_loss: 0.057, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3000, acc: 0.005, loss: 0.053 (data_loss: 0.053, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3100, acc: 0.011, loss: 0.049 (data_loss: 0.049, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3200, acc: 0.011, loss: 0.046 (data_loss: 0.046, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3300, acc: 0.026, loss: 0.043 (data_loss: 0.043, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3400, acc: 0.043, loss: 0.040 (data_loss: 0.040, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3500, acc: 0.053, loss: 0.038 (data_loss: 0.038, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3600, acc: 0.062, loss: 0.035 (data_loss: 0.035, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3700, acc: 0.070, loss: 0.033 (data_loss: 0.033, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3800, acc: 0.076, loss: 0.031 (data_loss: 0.031, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3900, acc: 0.085, loss: 0.030 (data_loss: 0.030, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4000, acc: 0.093, loss: 0.028 (data_loss: 0.028, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4100, acc: 0.097, loss: 0.027 (data_loss: 0.027, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4200, acc: 0.107, loss: 0.025 (data_loss: 0.025, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4300, acc: 0.111, loss: 0.024 (data_loss: 0.024, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4400, acc: 0.119, loss: 0.023 (data_loss: 0.023, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4500, acc: 0.122, loss: 0.021 (data_loss: 0.021, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4600, acc: 0.129, loss: 0.020 (data_loss: 0.020, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4700, acc: 0.134, loss: 0.019 (data_loss: 0.019, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4800, acc: 0.133, loss: 0.018 (data_loss: 0.018, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4900, acc: 0.144, loss: 0.017 (data_loss: 0.017, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5000, acc: 0.149, loss: 0.016 (data_loss: 0.016, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5100, acc: 0.154, loss: 0.016 (data_loss: 0.016, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5200, acc: 0.160, loss: 0.015 (data_loss: 0.015, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5300, acc: 0.166, loss: 0.014 (data_loss: 0.014, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5400, acc: 0.170, loss: 0.013 (data_loss: 0.013, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5500, acc: 0.176, loss: 0.013 (data_loss: 0.013, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5600, acc: 0.184, loss: 0.012 (data_loss: 0.012, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5700, acc: 0.189, loss: 0.011 (data_loss: 0.011, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5800, acc: 0.198, loss: 0.011 (data_loss: 0.011, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5900, acc: 0.203, loss: 0.010 (data_loss: 0.010, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6000, acc: 0.207, loss: 0.010 (data_loss: 0.010, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6100, acc: 0.212, loss: 0.009 (data_loss: 0.009, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6200, acc: 0.214, loss: 0.009 (data_loss: 0.009, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6300, acc: 0.216, loss: 0.008 (data_loss: 0.008, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6400, acc: 0.223, loss: 0.008 (data_loss: 0.008, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6500, acc: 0.230, loss: 0.008 (data_loss: 0.008, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6600, acc: 0.114, loss: 0.007 (data_loss: 0.007, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6700, acc: 0.238, loss: 0.007 (data_loss: 0.007, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6800, acc: 0.239, loss: 0.007 (data_loss: 0.007, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6900, acc: 0.242, loss: 0.006 (data_loss: 0.006, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7000, acc: 0.248, loss: 0.006 (data_loss: 0.006, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7100, acc: 0.253, loss: 0.006 (data_loss: 0.006, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7200, acc: 0.255, loss: 0.005 (data_loss: 0.005, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7300, acc: 0.260, loss: 0.005 (data_loss: 0.005, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7400, acc: 0.263, loss: 0.005 (data_loss: 0.005, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7500, acc: 0.283, loss: 0.005 (data_loss: 0.005, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7600, acc: 0.271, loss: 0.004 (data_loss: 0.004, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7700, acc: 0.274, loss: 0.004 (data_loss: 0.004, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7800, acc: 0.277, loss: 0.004 (data_loss: 0.004, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7900, acc: 0.282, loss: 0.004 (data_loss: 0.004, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8000, acc: 0.283, loss: 0.004 (data_loss: 0.004, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8100, acc: 0.279, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8200, acc: 0.288, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8300, acc: 0.292, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8400, acc: 0.293, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8500, acc: 0.296, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8600, acc: 0.300, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8700, acc: 0.300, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8800, acc: 0.299, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8900, acc: 0.307, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9000, acc: 0.305, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9100, acc: 0.309, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9200, acc: 0.339, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9300, acc: 0.311, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9400, acc: 0.328, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9500, acc: 0.312, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9600, acc: 0.316, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9700, acc: 0.316, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9800, acc: 0.318, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9900, acc: 0.323, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 10000, acc: 0.322, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# creating first dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# creating ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# creating seecond dense layer with 64 input features and 1 output value\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "# creating Llinear Activation for dense2\n",
    "activation2 = Activation_Linear()\n",
    "\n",
    "# creating loss\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = Optimizer_Adam()\n",
    "\n",
    "# the precision is defined as a fraction of the standard deviation\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # the absolute difference between the predictions and ground\n",
    "    # truth values, and if differences are lower than given precision\n",
    "    predictions = activation2.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) < \n",
    "                       accuracy_precision)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, '+\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b4fcae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation2.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9232aa",
   "metadata": {},
   "source": [
    "A neural network can approximate any non-linear function, but it needs at least 2 non-linear activation functions, hence another layer will be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2769f070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.002, loss: 0.500 (data_loss: 0.500, reg_loss: 0.000), lr: 0.005\n",
      "epoch: 100, acc: 0.006, loss: 0.078 (data_loss: 0.078, reg_loss: 0.000), lr: 0.004549590536851684\n",
      "epoch: 200, acc: 0.021, loss: 0.028 (data_loss: 0.028, reg_loss: 0.000), lr: 0.004170141784820684\n",
      "epoch: 300, acc: 0.011, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.003849114703618168\n",
      "epoch: 400, acc: 0.060, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0035739814152966403\n",
      "epoch: 500, acc: 0.549, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00333555703802535\n",
      "epoch: 600, acc: 0.040, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0031269543464665416\n",
      "epoch: 700, acc: 0.713, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002942907592701589\n",
      "epoch: 800, acc: 0.029, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0027793218454697055\n",
      "epoch: 900, acc: 0.756, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0026329647182727752\n",
      "epoch: 1000, acc: 0.766, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002501250625312656\n",
      "epoch: 1100, acc: 0.373, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0023820867079561697\n",
      "epoch: 1200, acc: 0.781, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002273760800363802\n",
      "epoch: 1300, acc: 0.790, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.002174858634188778\n",
      "epoch: 1400, acc: 0.777, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020842017507294707\n",
      "epoch: 1500, acc: 0.804, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020008003201280513\n",
      "epoch: 1600, acc: 0.129, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001923816852635629\n",
      "epoch: 1700, acc: 0.823, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001852537977028529\n",
      "epoch: 1800, acc: 0.823, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017863522686673815\n",
      "epoch: 1900, acc: 0.016, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017247326664367024\n",
      "epoch: 2000, acc: 0.837, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016672224074691564\n",
      "epoch: 2100, acc: 0.839, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016134236850596968\n",
      "epoch: 2200, acc: 0.868, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015629884338855893\n",
      "epoch: 2300, acc: 0.851, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015156107911488332\n",
      "epoch: 2400, acc: 0.855, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014710208884966167\n",
      "epoch: 2500, acc: 0.816, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014289797084881396\n",
      "epoch: 2600, acc: 0.864, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.001389274798555154\n",
      "epoch: 2700, acc: 0.870, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013517166801838335\n",
      "epoch: 2800, acc: 0.836, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013161358252171624\n",
      "epoch: 2900, acc: 0.882, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012823800974608873\n",
      "epoch: 3000, acc: 0.888, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012503125781445363\n",
      "epoch: 3100, acc: 0.902, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012198097096852891\n",
      "epoch: 3200, acc: 0.924, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011907597046915933\n",
      "epoch: 3300, acc: 0.910, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011630611770179114\n",
      "epoch: 3400, acc: 0.140, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011366219595362584\n",
      "epoch: 3500, acc: 0.917, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011113580795732384\n",
      "epoch: 3600, acc: 0.920, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010871928680147858\n",
      "epoch: 3700, acc: 0.902, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010640561821664183\n",
      "epoch: 3800, acc: 0.923, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010418837257762034\n",
      "epoch: 3900, acc: 0.952, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010206164523372118\n",
      "epoch: 4000, acc: 0.933, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010002000400080014\n",
      "epoch: 4100, acc: 0.926, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009805844283192783\n",
      "epoch: 4200, acc: 0.870, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009617234083477593\n",
      "epoch: 4300, acc: 0.928, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009435742592942063\n",
      "epoch: 4400, acc: 0.129, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009260974254491572\n",
      "epoch: 4500, acc: 0.934, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009092562284051646\n",
      "epoch: 4600, acc: 0.937, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000893016610108948\n",
      "epoch: 4700, acc: 0.958, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008773469029654326\n",
      "epoch: 4800, acc: 0.936, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000862217623728229\n",
      "epoch: 4900, acc: 0.252, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008476012883539582\n",
      "epoch: 5000, acc: 0.941, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008334722453742291\n",
      "epoch: 5100, acc: 0.906, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008198065256599442\n",
      "epoch: 5200, acc: 0.938, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008065817067268914\n",
      "epoch: 5300, acc: 0.940, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007937767899666614\n",
      "epoch: 5400, acc: 0.943, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007813720893889669\n",
      "epoch: 5500, acc: 0.945, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007693491306354824\n",
      "epoch: 5600, acc: 0.964, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007576905591756327\n",
      "epoch: 5700, acc: 0.950, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007463800567248844\n",
      "epoch: 5800, acc: 0.098, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007354022650389764\n",
      "epoch: 5900, acc: 0.965, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007247427163357008\n",
      "epoch: 6000, acc: 0.941, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000714387769681383\n",
      "epoch: 6100, acc: 0.943, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007043245527539089\n",
      "epoch: 6200, acc: 0.960, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006945409084595084\n",
      "epoch: 6300, acc: 0.198, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006850253459377996\n",
      "epoch: 6400, acc: 0.971, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006757669955399379\n",
      "epoch: 6500, acc: 0.971, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006667555674089878\n",
      "epoch: 6600, acc: 0.959, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006579813133307014\n",
      "epoch: 6700, acc: 0.969, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006494349915573451\n",
      "epoch: 6800, acc: 0.675, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006411078343377356\n",
      "epoch: 6900, acc: 0.974, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00063299151791366\n",
      "epoch: 7000, acc: 0.975, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006250781347668457\n",
      "epoch: 7100, acc: 0.971, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006173601679219657\n",
      "epoch: 7200, acc: 0.975, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006098304671301379\n",
      "epoch: 7300, acc: 0.973, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006024822267743102\n",
      "epoch: 7400, acc: 0.977, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005953089653530181\n",
      "epoch: 7500, acc: 0.081, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000588304506412519\n",
      "epoch: 7600, acc: 0.978, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005814629608093965\n",
      "epoch: 7700, acc: 0.976, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005747787101965744\n",
      "epoch: 7800, acc: 0.975, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005682463916354131\n",
      "epoch: 7900, acc: 0.977, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005618608832453085\n",
      "epoch: 8000, acc: 0.960, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00055561729081009\n",
      "epoch: 8100, acc: 0.976, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005495109352676119\n",
      "epoch: 8200, acc: 0.493, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005435373410153278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8300, acc: 0.976, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005376922249704269\n",
      "epoch: 8400, acc: 0.978, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005319714863283328\n",
      "epoch: 8500, acc: 0.921, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005263711969681019\n",
      "epoch: 8600, acc: 0.978, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005208875924575476\n",
      "epoch: 8700, acc: 0.414, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005155170636148056\n",
      "epoch: 8800, acc: 0.978, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005102561485865905\n",
      "epoch: 8900, acc: 0.875, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005051015254066068\n",
      "epoch: 9000, acc: 0.979, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005000500050005\n",
      "epoch: 9100, acc: 0.899, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004950985246063966\n",
      "epoch: 9200, acc: 0.979, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004902441415825081\n",
      "epoch: 9300, acc: 0.860, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004854840275754928\n",
      "epoch: 9400, acc: 0.980, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004808154630252909\n",
      "epoch: 9500, acc: 0.978, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047623583198399844\n",
      "epoch: 9600, acc: 0.979, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047174261722804036\n",
      "epoch: 9700, acc: 0.244, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046733339564445275\n",
      "epoch: 9800, acc: 0.979, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046300583387350687\n",
      "epoch: 9900, acc: 0.295, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045875768419121016\n",
      "epoch: 10000, acc: 0.981, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045458678061641964\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X, y = sine_data()\n",
    "\n",
    "# creating first dense layer with 1 input feature and 64 output values\n",
    "dense1 = Layer_Dense(1, 64)\n",
    "\n",
    "# creating ReLU activation for dense1\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# creating seecond dense layer with 64 input features and 64 output value\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "\n",
    "# creating ReLU activation for dense2\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "# creating third dense layer with 64 input features and 1 output value\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "\n",
    "# creating Llinear Activation for dense3\n",
    "activation3 = Activation_Linear()\n",
    "\n",
    "# creating loss\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "\n",
    "# create optimizer\n",
    "optimizer = Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
    "\n",
    "# the precision is defined as a fraction of the standard deviation\n",
    "accuracy_precision = np.std(y) / 250\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "    \n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation3.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2) + \\\n",
    "        loss_function.regularization_loss(dense3)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # the absolute difference between the predictions and ground\n",
    "    # truth values, and if differences are lower than given precision\n",
    "    predictions = activation3.output\n",
    "    accuracy = np.mean(np.absolute(predictions - y) < \n",
    "                       accuracy_precision)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, '+\n",
    "              f'loss: {loss:.3f} (' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation3.output, y)\n",
    "    activation3.backward(loss_function.dinputs)\n",
    "    dense3.backward(activation3.dinputs)\n",
    "    activation2.backward(dense3.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.update_params(dense3)\n",
    "    optimizer.post_update_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac553186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_test, y_test = sine_data()\n",
    "\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "plt.plot(X_test, y_test)\n",
    "plt.plot(X_test, activation3.output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62df25bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
