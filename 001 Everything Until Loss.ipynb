{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4732a2fb",
   "metadata": {},
   "source": [
    "### Single Neuron\n",
    "Every neuron has a unique connection to every neuron in the previous layer, as well as a unique bias\n",
    "The following are examples of two neurons being calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51e4bb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1.2, 5.1, 2.1]\n",
    "weights = [3.1, 2.1, 8.7]\n",
    "bias = 3\n",
    "\n",
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c05fa1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5]\n",
    "bias = 2\n",
    "\n",
    "output = inputs[0] * weights[0] + inputs[1] * weights[1] + inputs[2] * weights[2] + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0311a",
   "metadata": {},
   "source": [
    "### A Single Full Layer\n",
    "To model 3 output neurons with 4 output neurons, we need 3 unique weight sets, each with 4 values, and 3 unique biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71d8adcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1.0]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "outputs = [\n",
    "    inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "    inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "    inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3,\n",
    "]\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fad46",
   "metadata": {},
   "source": [
    "The whole crux of deep learning is to figure out these weights and biases to get the output values we want\n",
    "The following is a simplification of the code for scalability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae80a2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]\n",
    "          ]\n",
    "\n",
    "biases = [2,\n",
    "          3,\n",
    "          0.5,\n",
    "         ]\n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input * weight\n",
    "    neuron_output += neuron_bias\n",
    "    \n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35014b7b",
   "metadata": {},
   "source": [
    "### Shape\n",
    "Is what is the size of the dimension at each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "473c315b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [1,5,6,2]\n",
    "shape = (4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0963786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[1,5,6,2],\n",
    "         [3,2,1,3]]\n",
    "shape = (2, 4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9e407c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "array = [[[1,5,6,2],\n",
    "         [3,2,1,3]],\n",
    "         [[1,5,6,2],\n",
    "         [3,2,1,3]],\n",
    "         [[1,5,6,2],\n",
    "         [3,2,1,3]]\n",
    "        ]\n",
    "shape = (3, 2, 4, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1fc121",
   "metadata": {},
   "source": [
    "A 2D array, that is a matrix\n",
    "These arrays need to be homologous, with each level being of the same shape as every other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f4fea",
   "metadata": {},
   "source": [
    "## Dot Product\n",
    "How do we multiply the weights with the inputs efficienty?\n",
    "A dot product is the sum of the products of all corresponding elements of 2 matrices of the same shape\n",
    "Gives us a single scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9875bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d66994c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias = 2\n",
    "\n",
    "output = np.dot(inputs, weights) + bias #Here the order doesn't matter, as both arrays are of single dimension\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f3d70e",
   "metadata": {},
   "source": [
    "### Dot Product of a Layer of Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "06bf93bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  , 1.21 , 2.385])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]\n",
    "          ]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "outputs = np.dot(weights, inputs) + biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d89977",
   "metadata": {},
   "source": [
    "Now that the weights is a matrix of vectors, the order matters for the dot product\n",
    "Because we have 3 sets of weights, we want things indexed by the weight sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712dd370",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Because most of the calculations taking place are independent of each other, it makes sense to calculate multiple sets of inputs, as a single batch together\n",
    "Hence it makes sense to do Neural Networks on GPU with their hundreds to thousands of cores, as opposed to CPUs with their dozen cores at most\n",
    "\n",
    "Another reason we want to batch is because higher batch size results in a greater generalization to the data\n",
    "If it tries to adjust the weights and biases for every input set, it may jump around very chaotically\n",
    "\n",
    "Too large of batch sizes may result in overfitting\n",
    "32 is the usual go to amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6c2d2d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-0506eb6c5187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8],\n",
    "         ]\n",
    "# size = (3, 4)\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]\n",
    "          ]\n",
    "#size = (3, 4)\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "outputs = np.dot(inputs, weights) + biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84679082",
   "metadata": {},
   "source": [
    "The shapes not being aligned here is because we can't dot multiply a (3,4) matrix with (3,4) matrix\n",
    "We need the weights matrix to be (4,3)\n",
    "This operation is known as **Transpose**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981443b4",
   "metadata": {},
   "source": [
    "#### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5606fb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  1.21 ,  2.385],\n",
       "       [ 8.9  , -1.81 ,  0.2  ],\n",
       "       [ 1.41 ,  1.051,  0.026]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8],\n",
    "         ]\n",
    "# size = (3, 4)\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]\n",
    "          ]\n",
    "#size = (3, 4)\n",
    "weights = np.array(weights).T\n",
    "#size = (4, 3)\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "outputs = np.dot(inputs, weights) + biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f3618",
   "metadata": {},
   "source": [
    "### Second Layer\n",
    "We're going to need another set of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "878513fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5031 , -1.04185, -2.03875],\n",
       "       [ 0.2434 , -2.7332 , -5.7633 ],\n",
       "       [-0.99314,  1.41254, -0.35655]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2.0, 5.0, -1.0, 2.0],\n",
    "          [-1.5, 2.7, 3.3, -0.8],\n",
    "         ]\n",
    "\n",
    "weights1 = [[0.2, 0.8, -0.5, 1.0],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]\n",
    "          ]\n",
    "weights1 = np.array(weights1).T\n",
    "\n",
    "biases1 = [2, 3, 0.5]\n",
    "\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "            [-0.5, 0.12, -0.33],\n",
    "            [-0.44, 0.73, -0.13]\n",
    "          ]\n",
    "weights2 = np.array(weights2).T\n",
    "\n",
    "biases2 = [-1, 2, -0.5]\n",
    "\n",
    "layer1_outputs = np.dot(inputs, weights1) + biases1\n",
    "layer2_outputs = np.dot(layer1_outputs, weights2) + biases2\n",
    "\n",
    "layer2_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a9389",
   "metadata": {},
   "source": [
    "### Scalable Number of Layers\n",
    "When you save a model, you're saving the weights and biases. Loading model just means setting the weights and biases as the same as the saved model\n",
    "\n",
    "Initializing a neural network is setting the weights and biases as random numbers. Generally between -1 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c67d7073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.148296  , -0.08397602],\n",
       "       [ 0.14100315, -0.01340469],\n",
       "       [ 0.20124979, -0.07290616]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#Standart name of the input set in machine learning is X\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8],\n",
    "    ]\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        # the np.random.randn provides a gaussian distribution around 0\n",
    "            # accepts the shape as the parameters\n",
    "        # we need to know 2 things regarding the shape from the programmer when this layer is created\n",
    "            # n_inputs: the number of features coming in per input set\n",
    "            # n_neurons: how many neurons we have in this layer\n",
    "        # 0.1 is to bound the weights even tighter around 0\n",
    "        # the weights are now shaped so the transpose is no longer necessary\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        # np.zeros accepts the shape as a tuple for the first parameter\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "layer1 = Layer_Dense(4, 5)\n",
    "# since the input is X, we need to set the n_inputs accordingly\n",
    "layer2 = Layer_Dense(5,2)\n",
    "# since the input is the output from the previous layer, the input is the size of the previous layer\n",
    "# the output can of any size we want\n",
    "\n",
    "\n",
    "layer1.forward(X)\n",
    "layer2.forward(layer1.output)\n",
    "layer2.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a73c2",
   "metadata": {},
   "source": [
    "The size of layer2 output is 3, because that's the size of the batch of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947f352",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "Comes after the weights and functions, and determines the output\n",
    "Ensures the output is within a reasonable range\n",
    "\n",
    "## Step Function\n",
    "A function that outputs 1 if x is more than 0, and 0 if x is less than 0\n",
    "\n",
    "## Sigmoid Function\n",
    "* Slightly more reliable of a function, with a more granular output\n",
    "* Fits whatever the output was between 0 and 1\n",
    "* Very useful when calculating the error\n",
    "* Vanishing gradianet problem\n",
    "\n",
    "## Rectified Linear Unit Activation Function\n",
    "* If x is more than 0, return x, else return 0\n",
    "* Much faster than Sigmoid\n",
    "* Doesn't have the vanishing gradient problem\n",
    "* The most popular activation function for hidden layers\n",
    "\n",
    "# Why Activation Function?\n",
    "* Just using weights and biases means we're using a linear activation function (return x)\n",
    "* A linear activation function can't fit a non-linear function\n",
    "    * Such as a linear function trying to fit into a sine function\n",
    "* The Rectified Linear is ALMOST linear, the behaviour at 0 makes it non-linear enough to be able to fit non-linear functions\n",
    "* Honestly just rewatch [this](https://www.youtube.com/watch?v=gmjzbpSVY1A&list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&index=5) video if you need a refresher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "688295f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.65504505e-04,\n",
       "        4.56846210e-05],\n",
       "       [0.00000000e+00, 5.93469958e-05, 0.00000000e+00, 2.03573116e-04,\n",
       "        6.10024377e-04],\n",
       "       ...,\n",
       "       [1.13291524e-01, 0.00000000e+00, 0.00000000e+00, 8.11079666e-02,\n",
       "        0.00000000e+00],\n",
       "       [1.34588361e-01, 0.00000000e+00, 3.09493970e-02, 5.66337556e-02,\n",
       "        0.00000000e+00],\n",
       "       [1.07817926e-01, 0.00000000e+00, 0.00000000e+00, 8.72561932e-02,\n",
       "        0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "X, y = spiral_data(100, 3) #100 feature sets with 3 classes\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "layer1 = Layer_Dense(2, 5) # because the spiral data gives a pair of x and y observations, the n_input is 2\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "activation1.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abbb551",
   "metadata": {},
   "source": [
    "We see a lot of 0 values in the batch, but not really a problem because the weights and biases will be altered later on as the model is optimized\n",
    "\n",
    "If it's not enough, we can change the initialization of the biases as a non-zero value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bcda03",
   "metadata": {},
   "source": [
    "### Softmax Activation Function\n",
    "Let's say our network gave us the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fa23cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7072f",
   "metadata": {},
   "source": [
    "* If the model is meant to classify images of dogs or cats or neither, we want the output layers to be a probability distribution, which cannot be the case if all the neurons are not boundend and independent of one another \n",
    "* We COULD just sum it all up, and divide each value by the total, but if ReLU is used, it's going to clip out any negatives, regardless of if it is -1 or -20 or -9000, and it would be impossible to learn how wrong it is\n",
    "    * You've lost all meaning once you've clipped\n",
    "* Just using a linear activation function still does not get rid of the negative problem\n",
    "* We can make all numbers positive, without losing the meaning through **exponentiation functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7ff1f838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[121.51041751873483, 3.353484652549023, 10.859062664920513]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import e\n",
    "\n",
    "exp_values = [e**x for x in layer_outputs]\n",
    "\n",
    "exp_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a2363",
   "metadata": {},
   "source": [
    "Next step would be to normalize the outputs, i.e. turn it into a probability distribution by dividing all the values by the total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6522da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_base = sum(exp_values)\n",
    "norm_values = [x/norm_base for x in exp_values]\n",
    "\n",
    "print(norm_values)\n",
    "sum(norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680b6719",
   "metadata": {},
   "source": [
    "###### The same, but done in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6be4baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89528266 0.02470831 0.08000903]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import e\n",
    "import numpy as np\n",
    "\n",
    "exp_values = np.exp(layer_outputs)\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "\n",
    "print(norm_values)\n",
    "np.sum(norm_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced2b713",
   "metadata": {},
   "source": [
    "##### In Summary\n",
    "1. Took the set of the values of the output layers, e.g \\[dog, cat, human\\]\n",
    "2. Exponentiated every value with base e\n",
    "3. Normalized it all, by dividing each value by the sum\n",
    "4. Output\n",
    "\n",
    "The combination of exponentiation and the normalization is the **Softmax Function**\n",
    "\n",
    "Next up, we convert this code to support batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "155a8ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 135.72296484],\n",
       "       [7333.35859605],\n",
       "       [   7.98280655]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_outputs = np.array([[4.8, 1.21, 2.385],\n",
    "                          [8.9, -1.81, 0.2],\n",
    "                          [1.41, 1.051, 0.026]])\n",
    "\n",
    "exp_values = np.exp(layer_outputs) # nothing changes with this step\n",
    "\n",
    "#for the normalization, we want to iterate over the 3 items in the batch and divide them by the sum individually\n",
    "norm_bases = np.sum(exp_values, axis = 1, keepdims = True) \n",
    "# axis = 0 means the sum of the columns, axis = 1 means the sum of the rows\n",
    "# notice the keepdims\n",
    "\n",
    "norm_bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a5c9a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8.95282664e-01, 2.47083068e-02, 8.00090293e-02],\n",
       "       [9.99811129e-01, 2.23163963e-05, 1.66554348e-04],\n",
       "       [5.13097164e-01, 3.58333899e-01, 1.28568936e-01]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_values = exp_values / norm_bases\n",
    "\n",
    "print(np.sum(norm_values, axis = 1))\n",
    "norm_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8ddbad",
   "metadata": {},
   "source": [
    "* One problem with this approach is because we're exponentiating, it does not take a too high output, for e^output to cause an overflow\n",
    "* We can solve this problem by subtracting all the values by the largest value in the batch first\n",
    "    * This would cut the range of the exp to between 0 and 1\n",
    "    * This still returns identical normalized values, while protecting us from an overflow\n",
    "\n",
    "### All so far put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "00c30b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333334, 0.33333334, 0.33333334],\n",
       "       [0.33331734, 0.3333183 , 0.33336434],\n",
       "       [0.3332888 , 0.33329153, 0.33341965],\n",
       "       [0.33325943, 0.33326396, 0.33347666],\n",
       "       [0.33323312, 0.33323926, 0.33352762]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "X, y = spiral_data(100, 3) #100 feature sets with 3 classes\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "activation2.output[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee1094",
   "metadata": {},
   "source": [
    "# Training\n",
    "* We now want to know not just what's right and what's wrong, but also how right and how wrong\n",
    "    * We do so through a **Loss Function**\n",
    "\n",
    "### Loss Function\n",
    "* Since the neural network does not output a discrete classification, but a probability distribution, it does not make sense to throw away the probabilities and only try to optimize for the accuracy of the classes\n",
    "* One simple way to implement a loss function is the **Mean Absolute Error**\n",
    "    * The sum of the absolute errors of the model at regular intervals\n",
    "* In general, the Loss Function choice for classification is the **Categorical Cross-Entropy**\n",
    "    * Take the negative sum of the target value, multiplied by the log of the predicted value for each of the values in the distribution\n",
    "    * Very convenient for back propagation and optimization\n",
    "    * The formula can be simplified by **One-Hot Encoding**\n",
    "\n",
    "#### One-Hot Encoding\n",
    "* Have a vector that's as long as how many classes you have, filled with 0s, except for at the target index, at which will be a 1\n",
    "* eg:\n",
    "    * Classes = 3\n",
    "    * Label = 0\n",
    "    * One-Hot = \\[1, 0, 0\\]\n",
    "    * Prediction = \\[0.7, 0.1, 0.2\\]\n",
    "    * L = - Sigma_j y_j * log(y^hat_j) = -(1 * log(0.7) + 0 * log(0.1) + 0 * log(0.2)) = -(-0.3566 + 0 + 0) = 0.3566\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "50006bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.35667494 0.69314718 0.10536052]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38506088005216804"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                            [0.1, 0.5, 0.4],\n",
    "                            [0.02, 0.9, 0.08]])\n",
    "\n",
    "class_targets = [0, 1, 1]\n",
    "# because item in the batch predicts highest likely for index 0, second item for index 1, etc etc\n",
    "\n",
    "logs = -np.log(softmax_outputs[[0,1,2], [class_targets]])\n",
    "# numpy arrays can receive a list of indices you're interested in\n",
    "# for the first dimension, we're picking all 0 to 2 of them\n",
    "# for the second dimension, we're only picking the predicted class targets\n",
    "# because we're multiplying the distribution of wrong choices by 0, we can just ignore them altogether\n",
    "\n",
    "print(logs)\n",
    "np.mean(logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0700b",
   "metadata": {},
   "source": [
    "* The problem with this approach is if our model gives the prediction 0 for the right class\n",
    "    * This infinitely wrong output will mess up the mean of the loss for the entire batch\n",
    "    * One solution is the clip the probabilities by a small insignificant amount, such as between 1e-7 and 1 - 1e-7\n",
    "        * The upper end is also clipped so as to be not biased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e6826",
   "metadata": {},
   "source": [
    "### Put Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f29a124a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33331734 0.3333183  0.33336434]\n",
      " [0.3332888  0.33329153 0.33341965]\n",
      " [0.33325943 0.33326396 0.33347666]\n",
      " [0.33323312 0.33323926 0.33352762]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.098445"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis = 1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "\n",
    "class Loss:\n",
    "    def calculate(self, outputs, y):\n",
    "        # y is the intended target values\n",
    "        sample_losses = self.forward(outputs, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    #inheriting from the base Loss class\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred will come from the neural network\n",
    "        # y_true will come from the training set\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # means scalar class values have been passed\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # one hot encoded values have been passed\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis = 1)\n",
    "            \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "        \n",
    "        \n",
    "X, y = spiral_data(100, 3) #100 feature sets with 3 classes\n",
    "\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n",
    "\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218e668c",
   "metadata": {},
   "source": [
    "Our goal is now to decrease this loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b3172",
   "metadata": {},
   "source": [
    "| ||\n",
    "\n",
    "|| |_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
